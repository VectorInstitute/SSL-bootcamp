{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tight-gregory",
   "metadata": {},
   "source": [
    "## Latent Outlier Exposure for Anomaly Detection with Contaminated Data\n",
    "\n",
    "In anomaly detection, our goal is to identify data points that exhibit systematic deviations from the majority of data in an unlabeled dataset. Usually, it is assumed that we have clean training data without anomalies, but in practice, this may not be the case. To overcome this challenge, [Latent OE-AD](https://proceedings.mlr.press/v162/qiu22b.html) propose a strategy for training our anomaly detector when dealing with unlabeled anomalies. This approach is compatible with a wide range of models. The main idea is to jointly infer binary labels (normal vs. anomalous) for each data point while updating the model parameters. Taking inspiration from the concept of outlier exposure, where synthetically created, labeled anomalies are used, we employ a dual loss method. This means using two losses that share parameters, one for normal data and another for anomalous data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-million",
   "metadata": {},
   "source": [
    "## Loading libraries\n",
    "\n",
    "The configuration is loaded from the \"config_files\" directory, and you can find more detailed information there. For this task, we are utilizing the [thyroid dataset](https://odds.cs.stonybrook.edu/thyroid-disease-dataset/), which should be present in the \"DATA\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "macro-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "# type: ignore\n",
    "import argparse\n",
    "import os\n",
    "from config.base import Grid, Config\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from evaluation.Experiments import runExperiment\n",
    "from evaluation.Kvariants_Eval import KVariantEval\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from utils import Patience\n",
    "from models.Losses import *\n",
    "from config.Dataset_Class import *\n",
    "from copy import deepcopy\n",
    "from utils import read_config_file\n",
    "from models.TabNets import TabNets\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "from utils import compute_pre_recall_f1\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from loader.LoadData import load_data\n",
    "from utils import Logger\n",
    "\n",
    "# choose the configuration file / this file should be in config_files folder\n",
    "# Consists of all the details related to the training\n",
    "config_file = \"config_thyroid.yml\"\n",
    "config_file = \"config_files/\" + config_file\n",
    "# Choose the dataset name / data file loaded from DATA folder\n",
    "dataset_name = \"thyroid\"\n",
    "# Set the 'contamination' rate to control the percentage of anomalies in the dataset.\n",
    "contamination = float(0.1)\n",
    "query_num = int(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-vegetation",
   "metadata": {},
   "source": [
    "## Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "handled-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the configuration file and create a grid of model configurations using the Grid class.\n",
    "model_configurations = Grid(config_file, dataset_name)\n",
    "\n",
    "# Select the first model configuration from the grid. This configuration consists of detailed settings for the model.\n",
    "model_configuration = Config(**model_configurations[0])\n",
    "\n",
    "# Get the dataset name from the model configuration.\n",
    "dataset = model_configuration.dataset\n",
    "\n",
    "# Create the result folder path for saving results and the model. The path is based on the model configuration details.\n",
    "result_folder = model_configuration.result_folder + model_configuration.exp_name\n",
    "\n",
    "# Combine the result folder path with the contamination rate and training method to create the experiment path.\n",
    "exp_path = os.path.join(result_folder, f\"{contamination}_{model_configuration.train_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incoming-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KVariantEval class controls the variety of experiments to validate the model.\n",
    "risk_assesser = KVariantEval(dataset, exp_path, model_configurations, contamination, query_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "revised-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TabNeutralAD(nn.Module):\n",
    "    \"\"\"\n",
    "    Tabular Neutral Autoencoder for Anomaly Detection.\n",
    "\n",
    "    This class implements a Tabular Neutral Autoencoder for anomaly detection. It consists of an encoder network\n",
    "    and multiple transformation networks (trans) that help disentangle different aspects of the input data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The autoencoder model for disentangling the input data.\n",
    "        x_dim (int): The dimensionality of the input data.\n",
    "        config (dict): A dictionary containing configuration settings for the model.\n",
    "\n",
    "    Attributes:\n",
    "        enc (nn.Module): The encoder network.\n",
    "        trans (nn.Module): The list of transformation networks.\n",
    "        num_trans (int): The number of transformation networks.\n",
    "        trans_type (str): The type of transformation used, either 'forward' or 'residual'.\n",
    "        device (str): The device (CPU or GPU) to perform computations on.\n",
    "        z_dim (int): The dimensionality of the latent space representation.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, x_dim, config):\n",
    "        super(TabNeutralAD, self).__init__()\n",
    "\n",
    "        # Extract encoder and transformation networks from the given model.\n",
    "        self.enc, self.trans = model._make_nets(x_dim, config)\n",
    "\n",
    "        # Get the number of transformation networks.\n",
    "        self.num_trans = config[\"num_trans\"]\n",
    "\n",
    "        # Get the type of transformation used, either 'forward' or 'residual'.\n",
    "        self.trans_type = config[\"trans_type\"]\n",
    "\n",
    "        # Get the device (CPU or GPU) to perform computations on.\n",
    "        self.device = config[\"device\"]\n",
    "\n",
    "        # Set the dimensionality of the latent space representation.\n",
    "        try:\n",
    "            self.z_dim = config[\"latent_dim\"]\n",
    "        except:\n",
    "            if 32 <= x_dim <= 300:\n",
    "                self.z_dim = 32\n",
    "            elif x_dim < 32:\n",
    "                self.z_dim = 2 * x_dim\n",
    "            else:\n",
    "                self.z_dim = 64\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Tabular Neutral Autoencoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input data.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The latent space representation of the input data.\n",
    "        \"\"\"\n",
    "        # Convert the input data to the appropriate device (CPU or GPU).\n",
    "        x = x.type(torch.FloatTensor).to(self.device)\n",
    "\n",
    "        # Initialize a tensor to store the transformed versions of the input data.\n",
    "        x_T = torch.empty(x.shape[0], self.num_trans, x.shape[-1]).to(x)\n",
    "\n",
    "        # Apply the transformation networks to the input data.\n",
    "        for i in range(self.num_trans):\n",
    "            mask = self.trans[i](x)\n",
    "            if self.trans_type == \"forward\":\n",
    "                x_T[:, i] = mask\n",
    "            elif self.trans_type == \"residual\":\n",
    "                x_T[:, i] = mask + x\n",
    "\n",
    "        # Concatenate the original input data with the transformed versions.\n",
    "        x_cat = torch.cat([x.unsqueeze(1), x_T], 1)\n",
    "\n",
    "        # Encode the concatenated data to obtain the latent space representation.\n",
    "        zs = self.enc(x_cat.reshape(-1, x.shape[-1]))\n",
    "        zs = zs.reshape(x.shape[0], self.num_trans + 1, self.z_dim)\n",
    "\n",
    "        return zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "surrounded-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DCL(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Clustering Loss (DCL) Module.\n",
    "\n",
    "    This class implements the Deep Clustering Loss (DCL) module, which is used for clustering in unsupervised learning.\n",
    "    The DCL calculates two types of loss: neighbor loss (loss_n) and anti-neighbor loss (loss_a) based on the similarity\n",
    "    matrix between the input embeddings (z).\n",
    "\n",
    "    Args:\n",
    "        temperature (float, optional): The temperature parameter for the DCL. Default is 0.1.\n",
    "\n",
    "    Attributes:\n",
    "        temp (float): The temperature parameter for the DCL.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super(DCL, self).__init__()\n",
    "        self.temp = temperature\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass of the Deep Clustering Loss (DCL) module.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): The input embeddings.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two losses: neighbor loss (loss_n) and anti-neighbor loss (loss_a).\n",
    "\n",
    "        \"\"\"\n",
    "        # Normalize the input embeddings (z) along the last dimension (L2 normalization).\n",
    "        z = F.normalize(z, p=2, dim=-1)\n",
    "\n",
    "        # Split the input embeddings into the original embedding (z_ori) and transformed embeddings (z_trans).\n",
    "        z_ori = z[:, 0]  # n, z\n",
    "        z_trans = z[:, 1:]  # n, k-1, z\n",
    "\n",
    "        # Get the batch size, number of transformations (k), and dimension of embeddings (z_dim).\n",
    "        batch_size, num_trans, z_dim = z.shape\n",
    "\n",
    "        # Calculate the similarity matrix between embeddings using exponential of the dot product with temperature.\n",
    "        sim_matrix = torch.exp(torch.matmul(z, z.permute(0, 2, 1) / self.temp))  # n, k, k\n",
    "\n",
    "        # Create a mask to remove the similarity of embeddings with themselves.\n",
    "        mask = (torch.ones_like(sim_matrix).to(z) - torch.eye(num_trans).unsqueeze(0).to(z)).bool()\n",
    "\n",
    "        # Apply the mask to the similarity matrix to get similarity values between different embeddings.\n",
    "        sim_matrix = sim_matrix.masked_select(mask).view(batch_size, num_trans, -1)\n",
    "\n",
    "        # Calculate the sum of similarities for each transformation (trans_matrix) to use in loss_n.\n",
    "        trans_matrix = sim_matrix[:, 1:].sum(-1)  # n, k-1\n",
    "\n",
    "        # Calculate the positive similarity between original and transformed embeddings to use in loss_a.\n",
    "        pos_sim = torch.exp(torch.sum(z_trans * z_ori.unsqueeze(1), -1) / self.temp)  # n, k-1\n",
    "\n",
    "        # Calculate the scale factor for loss tensor normalization.\n",
    "        K = num_trans - 1\n",
    "        scale = 1 / np.abs(np.log(1.0 / K))\n",
    "\n",
    "        # Calculate neighbor loss (loss_n) and anti-neighbor loss (loss_a).\n",
    "        loss_tensor = (torch.log(trans_matrix) - torch.log(pos_sim)) * scale\n",
    "        loss_n = loss_tensor.mean(1)\n",
    "        loss_a = -torch.log(1 - pos_sim / trans_matrix) * scale\n",
    "        loss_a = loss_a.mean(1)\n",
    "\n",
    "        return loss_n, loss_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "later-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "\n",
    "class NeutralAD_trainer:\n",
    "    \"\"\"\n",
    "    Trainer for the NeutralAD model.\n",
    "\n",
    "    This class implements the trainer for the NeutralAD model, which is used for anomaly detection.\n",
    "    It includes methods for training the model and detecting outliers in the data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The NeutralAD model.\n",
    "        loss_function (nn.Module): The loss function used during training.\n",
    "        config (dict): A dictionary containing the configuration parameters.\n",
    "\n",
    "    Attributes:\n",
    "        loss_fun (nn.Module): The loss function used during training.\n",
    "        device (torch.device): The device on which the model and data are located.\n",
    "        model (nn.Module): The NeutralAD model.\n",
    "        train_method (str): The training method used for the model.\n",
    "        max_epochs (int): The maximum number of training epochs.\n",
    "        warmup (int): The number of warm-up epochs during training.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_function, config):\n",
    "        self.loss_fun = loss_function\n",
    "        self.device = torch.device(config[\"device\"])\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_method = config[\"train_method\"]\n",
    "        self.max_epochs = config[\"training_epochs\"]\n",
    "        self.warmup = 2\n",
    "\n",
    "    def _train(self, epoch, train_loader, optimizer):\n",
    "        \"\"\"\n",
    "        Perform a single training epoch.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): The current epoch number.\n",
    "            train_loader (DataLoader): The training data loader.\n",
    "            optimizer (torch.optim): The optimizer used for training.\n",
    "\n",
    "        Returns:\n",
    "            float: The average loss for the current epoch.\n",
    "\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        loss_all = 0\n",
    "\n",
    "        for data in train_loader:\n",
    "            samples = data[\"sample\"]\n",
    "            labels = data[\"label\"]\n",
    "\n",
    "            z = self.model(samples)\n",
    "            loss_n, loss_a = self.loss_fun(z)\n",
    "\n",
    "            if epoch <= self.warmup:\n",
    "                if self.train_method == \"gt\":\n",
    "                    loss = torch.cat([loss_n[labels == 0], loss_a[labels == 1]], 0)\n",
    "                    loss_mean = loss.mean()\n",
    "                else:\n",
    "                    loss = loss_n\n",
    "                    loss_mean = loss.mean()\n",
    "            else:\n",
    "                score = loss_n - loss_a\n",
    "\n",
    "                if self.train_method == \"blind\":\n",
    "                    loss = loss_n\n",
    "                    loss_mean = loss.mean()\n",
    "                elif self.train_method == \"loe_hard\":\n",
    "                    _, idx_n = torch.topk(\n",
    "                        score,\n",
    "                        int(score.shape[0] * (1 - self.contamination)),\n",
    "                        largest=False,\n",
    "                        sorted=False,\n",
    "                    )\n",
    "                    _, idx_a = torch.topk(\n",
    "                        score,\n",
    "                        int(score.shape[0] * self.contamination),\n",
    "                        largest=True,\n",
    "                        sorted=False,\n",
    "                    )\n",
    "                    loss = torch.cat([loss_n[idx_n], loss_a[idx_a]], 0)\n",
    "                    loss_mean = loss.mean()\n",
    "                elif self.train_method == \"loe_soft\":\n",
    "                    _, idx_n = torch.topk(\n",
    "                        score,\n",
    "                        int(score.shape[0] * (1 - self.contamination)),\n",
    "                        largest=False,\n",
    "                        sorted=False,\n",
    "                    )\n",
    "                    _, idx_a = torch.topk(\n",
    "                        score,\n",
    "                        int(score.shape[0] * self.contamination),\n",
    "                        largest=True,\n",
    "                        sorted=False,\n",
    "                    )\n",
    "                    loss = torch.cat([loss_n[idx_n], 0.5 * loss_n[idx_a] + 0.5 * loss_a[idx_a]], 0)\n",
    "                    loss_mean = loss.mean()\n",
    "                elif self.train_method == \"refine\":\n",
    "                    _, idx_n = torch.topk(\n",
    "                        loss_n,\n",
    "                        int(loss_n.shape[0] * (1 - self.contamination)),\n",
    "                        largest=False,\n",
    "                        sorted=False,\n",
    "                    )\n",
    "                    loss = loss_n[idx_n]\n",
    "                    loss_mean = loss.mean()\n",
    "                elif self.train_method == \"gt\":\n",
    "                    loss = torch.cat([loss_n[labels == 0], loss_a[labels == 1]], 0)\n",
    "                    loss_mean = loss.mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_mean.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_all += loss.sum()\n",
    "\n",
    "        return loss_all.item() / len(train_loader.dataset)\n",
    "\n",
    "    def detect_outliers(self, loader):\n",
    "        \"\"\"\n",
    "        Detect outliers in the data using the trained model.\n",
    "\n",
    "        Args:\n",
    "            loader (DataLoader): The data loader for which to detect outliers.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the area under the ROC curve (AUC), average precision (AP),\n",
    "                   F1 score, anomaly scores, inlier loss, and outlier loss.\n",
    "\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "\n",
    "        loss_in = 0\n",
    "        loss_out = 0\n",
    "        target_all = []\n",
    "        score_all = []\n",
    "        for data in loader:\n",
    "            with torch.no_grad():\n",
    "                samples = data[\"sample\"]\n",
    "                labels = data[\"label\"]\n",
    "\n",
    "                z = model(samples)\n",
    "                loss_n, loss_a = self.loss_fun(z)\n",
    "                score = loss_n\n",
    "                loss_in += loss_n[labels == 0].sum()\n",
    "                loss_out += loss_n[labels == 1].sum()\n",
    "                target_all.append(labels)\n",
    "                score_all.append(score)\n",
    "\n",
    "        score_all = torch.cat(score_all).cpu().numpy()\n",
    "        target_all = np.concatenate(target_all)\n",
    "        auc = roc_auc_score(target_all, score_all)\n",
    "        ap = average_precision_score(target_all, score_all)\n",
    "        f1 = compute_pre_recall_f1(target_all, score_all)\n",
    "        return (\n",
    "            auc,\n",
    "            ap,\n",
    "            f1,\n",
    "            score_all,\n",
    "            loss_in.item() / (target_all == 0).sum(),\n",
    "            loss_out.item() / (target_all == 1).sum(),\n",
    "        )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader,\n",
    "        contamination,\n",
    "        query_num=0,\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        validation_loader=None,\n",
    "        test_loader=None,\n",
    "        early_stopping=None,\n",
    "        logger=None,\n",
    "        log_every=2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train the NeutralAD model.\n",
    "\n",
    "        Args:\n",
    "            train_loader (DataLoader): The data loader for training.\n",
    "            contamination (float): The contamination rate in the data.\n",
    "            query_num (int, optional): The query number. Default is 0.\n",
    "            optimizer (torch.optim, optional): The optimizer used for training. Default is None.\n",
    "            scheduler (torch.optim.lr_scheduler, optional): The learning rate scheduler. Default is None.\n",
    "            validation_loader (DataLoader, optional): The data loader for validation. Default is None.\n",
    "            test_loader (DataLoader, optional): The data loader for testing. Default is None.\n",
    "            early_stopping (object, optional): The early stopping criteria. Default is None.\n",
    "            logger (Logger, optional): The logger for logging training progress. Default is None.\n",
    "            log_every (int, optional): The frequency of logging training progress. Default is 2.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the validation loss, validation AUC, test AUC, test AP, test F1 score,\n",
    "                   test anomaly scores.\n",
    "\n",
    "        \"\"\"\n",
    "        self.contamination = contamination\n",
    "        early_stopper = early_stopping() if early_stopping is not None else None\n",
    "\n",
    "        val_auc, val_f1 = -1, -1\n",
    "        test_auc, test_f1, test_score = None, None, None\n",
    "\n",
    "        for epoch in range(1, self.max_epochs + 1):\n",
    "            train_loss = self._train(epoch, train_loader, optimizer)\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            if test_loader is not None:\n",
    "                (\n",
    "                    test_auc,\n",
    "                    test_ap,\n",
    "                    test_f1,\n",
    "                    test_score,\n",
    "                    testin_loss,\n",
    "                    testout_loss,\n",
    "                ) = self.detect_outliers(test_loader)\n",
    "\n",
    "            if validation_loader is not None:\n",
    "                (\n",
    "                    val_auc,\n",
    "                    val_ap,\n",
    "                    val_f1,\n",
    "                    _,\n",
    "                    valin_loss,\n",
    "                    valout_loss,\n",
    "                ) = self.detect_outliers(validation_loader)\n",
    "                if epoch > self.warmup:\n",
    "                    if early_stopper is not None and early_stopper.stop(\n",
    "                        epoch,\n",
    "                        valin_loss,\n",
    "                        val_auc,\n",
    "                        testin_loss,\n",
    "                        test_auc,\n",
    "                        test_ap,\n",
    "                        test_f1,\n",
    "                        test_score,\n",
    "                        train_loss,\n",
    "                    ):\n",
    "                        break\n",
    "\n",
    "            if epoch % log_every == 0 or epoch == 1:\n",
    "                msg = f\"Epoch: {epoch}, TR loss: {train_loss}, VAL loss: {valin_loss,valout_loss}, VL auc: {val_auc} VL ap: {val_ap} VL f1: {val_f1} \"\n",
    "\n",
    "                if logger is not None:\n",
    "                    logger.log(msg)\n",
    "                    print(msg)\n",
    "                else:\n",
    "                    print(msg)\n",
    "\n",
    "        if early_stopper is not None:\n",
    "            (\n",
    "                train_loss,\n",
    "                val_loss,\n",
    "                val_auc,\n",
    "                test_loss,\n",
    "                test_auc,\n",
    "                test_ap,\n",
    "                test_f1,\n",
    "                test_score,\n",
    "                best_epoch,\n",
    "            ) = early_stopper.get_best_vl_metrics()\n",
    "            msg = (\n",
    "                f\"Stopping at epoch {best_epoch}, TR loss: {train_loss}, VAL loss: {val_loss}, VAL auc: {val_auc},\"\n",
    "                f\"TS loss: {test_loss}, TS auc: {test_auc} TS ap: {test_ap} TS f1: {test_f1}\"\n",
    "            )\n",
    "            if logger is not None:\n",
    "                logger.log(msg)\n",
    "                print(msg)\n",
    "            else:\n",
    "                print(msg)\n",
    "\n",
    "        return val_loss, val_auc, test_auc, test_ap, test_f1, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "italic-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "class runExperiment:\n",
    "    \"\"\"\n",
    "    Class for running an experiment using a model configuration.\n",
    "\n",
    "    This class is used to run experiments with a given model configuration. It includes methods for running\n",
    "    the test phase of the experiment.\n",
    "\n",
    "    Args:\n",
    "        model_configuration (dict): A dictionary containing the configuration parameters for the model.\n",
    "        exp_path (str): The path to save the experiment results.\n",
    "\n",
    "    Attributes:\n",
    "        model_config (Config): The model configuration.\n",
    "        exp_path (str): The path to save the experiment results.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_configuration, exp_path):\n",
    "        self.model_config = Config.from_dict(model_configuration)\n",
    "        self.exp_path = exp_path\n",
    "\n",
    "    def run_test(self, train_data, val_data, test_data, logger, contamination, query_num):\n",
    "        \"\"\"\n",
    "        Run the test phase of the experiment.\n",
    "\n",
    "        Args:\n",
    "            train_data (Dataset): The training dataset.\n",
    "            val_data (Dataset): The validation dataset.\n",
    "            test_data (Dataset): The test dataset.\n",
    "            logger (Logger): The logger for logging experiment progress.\n",
    "            contamination (float): The contamination rate in the data.\n",
    "            query_num (int): The query number.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the validation AUC, test AUC, test average precision (AP), test F1 score,\n",
    "                   and test anomaly scores.\n",
    "\n",
    "        \"\"\"\n",
    "        optim_class = self.model_config.optimizer\n",
    "        sched_class = self.model_config.scheduler\n",
    "        stopper_class = self.model_config.early_stopper\n",
    "        network = self.model_config.network\n",
    "\n",
    "        try:\n",
    "            x_dim = self.model_config[\"x_dim\"]\n",
    "        except:\n",
    "            x_dim = train_data.dim_features\n",
    "        try:\n",
    "            batch_size = self.model_config[\"batch_size\"]\n",
    "        except:\n",
    "            batch_size = int(np.ceil(len(train_data) / 4))\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "        if len(val_data) == 0:\n",
    "            val_loader = None\n",
    "        else:\n",
    "            val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        if len(test_data) == 0:\n",
    "            test_loader = None\n",
    "        else:\n",
    "            test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        model = TabNeutralAD(network(), x_dim, config=self.model_config)\n",
    "        optimizer = optim_class(\n",
    "            model.parameters(),\n",
    "            lr=self.model_config[\"learning_rate\"],\n",
    "            weight_decay=self.model_config[\"l2\"],\n",
    "        )\n",
    "\n",
    "        if sched_class is not None:\n",
    "            scheduler = sched_class(optimizer)\n",
    "        else:\n",
    "            scheduler = None\n",
    "\n",
    "        trainer = NeutralAD_trainer(\n",
    "            model,\n",
    "            loss_function=DCL(self.model_config[\"loss_temp\"]),\n",
    "            config=self.model_config,\n",
    "        )\n",
    "\n",
    "        val_loss, val_auc, test_auc, test_ap, test_f1, test_score = trainer.train(\n",
    "            train_loader=train_loader,\n",
    "            contamination=contamination,\n",
    "            query_num=query_num,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            validation_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            early_stopping=stopper_class,\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "        return val_auc, test_auc, test_ap, test_f1, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "optical-investment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Cls: 0\n",
      "Epoch: 1, TR loss: 0.8143219128579295, VAL loss: (0.6871203805779296, 0.7904305153704704), VL auc: 0.8509597028912568 VL ap: 0.3186165491967505 VL f1: 0.3404255319148936 \n",
      "Epoch: 2, TR loss: 0.6131055834709985, VAL loss: (0.5245263419636181, 0.6860609663293716), VL auc: 0.8813763261717168 VL ap: 0.3367383721304317 VL f1: 0.3191489361702128 \n",
      "Epoch: 4, TR loss: 0.3427906755250398, VAL loss: (0.3043727988844141, 0.5000987357281624), VL auc: 0.8860967454560179 VL ap: 0.3472220554909719 VL f1: 0.3617021276595745 \n",
      "Epoch: 6, TR loss: 0.20723840505307758, VAL loss: (0.17582530881996838, 0.37497317537348324), VL auc: 0.9072692143047216 VL ap: 0.39960394200709337 VL f1: 0.43617021276595747 \n",
      "Epoch: 8, TR loss: 0.14762498255284814, VAL loss: (0.12608419545906402, 0.2950583113000748), VL auc: 0.9399187810211377 VL ap: 0.4512600674605257 VL f1: 0.44680851063829785 \n",
      "Epoch: 10, TR loss: 0.13444091079868759, VAL loss: (0.11551638463711596, 0.25192666561045546), VL auc: 0.9490993023497971 VL ap: 0.4928825495278068 VL f1: 0.47872340425531923 \n",
      "Epoch: 12, TR loss: 0.13087268442834527, VAL loss: (0.11159820307720739, 0.2294767258015085), VL auc: 0.9537040250830123 VL ap: 0.5302117327545265 VL f1: 0.5212765957446809 \n",
      "Epoch: 14, TR loss: 0.12893106140345845, VAL loss: (0.10955313536834302, 0.2186020790262425), VL auc: 0.9571633519604781 VL ap: 0.5500403210355665 VL f1: 0.5531914893617021 \n",
      "Epoch: 16, TR loss: 0.127876509537139, VAL loss: (0.10816899974298191, 0.20971018202761385), VL auc: 0.9584475836775306 VL ap: 0.5738850214265079 VL f1: 0.5638297872340425 \n",
      "Epoch: 18, TR loss: 0.12691660249460812, VAL loss: (0.1072823802954221, 0.20691062034444607), VL auc: 0.9616523781426076 VL ap: 0.6042541376147288 VL f1: 0.5957446808510638 \n",
      "Epoch: 20, TR loss: 0.12671645319000857, VAL loss: (0.10660344300677169, 0.20181302821382563), VL auc: 0.96118380711071 VL ap: 0.6088868782454694 VL f1: 0.5957446808510638 \n",
      "Epoch: 22, TR loss: 0.12619581703339147, VAL loss: (0.106104101416467, 0.20166131283374542), VL auc: 0.9631101546862888 VL ap: 0.6265893581748392 VL f1: 0.6063829787234043 \n",
      "Epoch: 24, TR loss: 0.12544564925393112, VAL loss: (0.10557498071036824, 0.20220439992052444), VL auc: 0.9649381601934446 VL ap: 0.6496726253941931 VL f1: 0.6276595744680851 \n",
      "Epoch: 26, TR loss: 0.1255618640154644, VAL loss: (0.1051838197028786, 0.199865016531437), VL auc: 0.9651059201925192 VL ap: 0.6509854662081516 VL f1: 0.6382978723404256 \n",
      "Epoch: 28, TR loss: 0.12516245419525132, VAL loss: (0.10492859384558524, 0.20393412163917055), VL auc: 0.9692478567213911 VL ap: 0.6833486663172419 VL f1: 0.6382978723404256 \n",
      "Epoch: 30, TR loss: 0.12492795581042737, VAL loss: (0.10478194684292584, 0.2009340651491855), VL auc: 0.9690800967223167 VL ap: 0.6814408916019206 VL f1: 0.6382978723404256 \n",
      "Epoch: 32, TR loss: 0.12435128860361601, VAL loss: (0.10471322006217289, 0.20567475988509806), VL auc: 0.9707519118855068 VL ap: 0.700894586124879 VL f1: 0.6382978723404256 \n",
      "Epoch: 34, TR loss: 0.12468557782576939, VAL loss: (0.10447277174882749, 0.20333911002950467), VL auc: 0.9721518401536451 VL ap: 0.702685305271562 VL f1: 0.648936170212766 \n",
      "Epoch: 36, TR loss: 0.12454413340713717, VAL loss: (0.10430882481403361, 0.20608867482936127), VL auc: 0.9726666898059768 VL ap: 0.7160644288387474 VL f1: 0.6702127659574468 \n",
      "Epoch: 38, TR loss: 0.12435323796433324, VAL loss: (0.10423544323139698, 0.20704837555580952), VL auc: 0.9724352967038054 VL ap: 0.7166773435477338 VL f1: 0.6702127659574468 \n",
      "Epoch: 40, TR loss: 0.12435602383233144, VAL loss: (0.10411802772078066, 0.20745086669921875), VL auc: 0.9733782235951548 VL ap: 0.7265774507863065 VL f1: 0.6808510638297872 \n",
      "Epoch: 42, TR loss: 0.12386001235879757, VAL loss: (0.10397425989147889, 0.20992354129223115), VL auc: 0.9724989298069024 VL ap: 0.7350467001920704 VL f1: 0.6914893617021277 \n",
      "Epoch: 44, TR loss: 0.12411797776037843, VAL loss: (0.104006835207335, 0.2078553869369182), VL auc: 0.9727650318743999 VL ap: 0.717722315873156 VL f1: 0.6702127659574468 \n",
      "Epoch: 46, TR loss: 0.12437677220042523, VAL loss: (0.1036450384491614, 0.20501025179599194), VL auc: 0.974778151863293 VL ap: 0.7406640284114895 VL f1: 0.6914893617021277 \n",
      "Epoch: 48, TR loss: 0.1236804172331016, VAL loss: (0.10361087834335916, 0.2131959631087932), VL auc: 0.9755706732382308 VL ap: 0.7581121488231428 VL f1: 0.7127659574468085 \n",
      "Epoch: 50, TR loss: 0.12398709424616831, VAL loss: (0.10366413060448104, 0.21178854272720662), VL auc: 0.975235153240082 VL ap: 0.740416633913694 VL f1: 0.7021276595744681 \n",
      "Epoch: 52, TR loss: 0.12331446770412689, VAL loss: (0.10366033042754214, 0.22394226967020237), VL auc: 0.980013420799926 VL ap: 0.7869765130452833 VL f1: 0.7340425531914893 \n",
      "Epoch: 54, TR loss: 0.12371601360543243, VAL loss: (0.10360925206676523, 0.21944364588311377), VL auc: 0.9781507063274443 VL ap: 0.760271604175154 VL f1: 0.7127659574468085 \n",
      "Epoch: 56, TR loss: 0.12435251348927741, VAL loss: (0.10339601899437958, 0.2115523764427672), VL auc: 0.9754839008249164 VL ap: 0.7501445418599327 VL f1: 0.7127659574468085 \n",
      "Epoch: 58, TR loss: 0.12363051808002937, VAL loss: (0.10349418470041463, 0.21466630570431974), VL auc: 0.9757615725475224 VL ap: 0.740957206699508 VL f1: 0.7127659574468085 \n",
      "Epoch: 60, TR loss: 0.12312654484969637, VAL loss: (0.10337415553098661, 0.2204929919953042), VL auc: 0.9792787477005311 VL ap: 0.7792642654331491 VL f1: 0.723404255319149 \n",
      "Epoch: 62, TR loss: 0.12344332716946968, VAL loss: (0.1032289522636708, 0.22058245476256025), VL auc: 0.9785093656358105 VL ap: 0.7823649768113969 VL f1: 0.723404255319149 \n",
      "Epoch: 64, TR loss: 0.12331887430498195, VAL loss: (0.10324209523369528, 0.2204437255859375), VL auc: 0.9779887311559241 VL ap: 0.7752502281738042 VL f1: 0.7446808510638298 \n",
      "Epoch: 66, TR loss: 0.12401256290534753, VAL loss: (0.10299966385858483, 0.21764736987174826), VL auc: 0.9786134925317876 VL ap: 0.7994661698925878 VL f1: 0.7446808510638298 \n",
      "Epoch: 68, TR loss: 0.1238187919687653, VAL loss: (0.10296140487197951, 0.21947629401024352), VL auc: 0.9809794870014925 VL ap: 0.7957670083848498 VL f1: 0.7446808510638298 \n",
      "Epoch: 70, TR loss: 0.12344991466430724, VAL loss: (0.10298308579412215, 0.22640339871670337), VL auc: 0.981349715964967 VL ap: 0.7966953365566731 VL f1: 0.7446808510638298 \n",
      "Epoch: 72, TR loss: 0.12342762772032168, VAL loss: (0.10288608171422563, 0.22496312729855802), VL auc: 0.9814422732058359 VL ap: 0.8029256497256251 VL f1: 0.7446808510638298 \n",
      "Epoch: 74, TR loss: 0.12404024233376927, VAL loss: (0.10283087128332738, 0.21788791899985455), VL auc: 0.9797502111462059 VL ap: 0.8009781962180388 VL f1: 0.7446808510638298 \n",
      "Epoch: 76, TR loss: 0.12325690554685588, VAL loss: (0.10286871540345466, 0.2259871097321206), VL auc: 0.9802043201092177 VL ap: 0.8022940193432319 VL f1: 0.7553191489361702 \n",
      "Epoch: 78, TR loss: 0.12285872061230421, VAL loss: (0.10270534098440048, 0.2232728105910281), VL auc: 0.9799613573519375 VL ap: 0.8056746377503046 VL f1: 0.7553191489361702 \n",
      "Epoch: 80, TR loss: 0.12309053022372507, VAL loss: (0.10284803845818889, 0.22878788887186252), VL auc: 0.980967917346384 VL ap: 0.8095569304648232 VL f1: 0.7553191489361702 \n",
      "Epoch: 82, TR loss: 0.12308911114887344, VAL loss: (0.10287963468915681, 0.2293575570938435), VL auc: 0.9819281987203963 VL ap: 0.8082877123663709 VL f1: 0.7553191489361702 \n",
      "Epoch: 84, TR loss: 0.12390997873120488, VAL loss: (0.1027976985597429, 0.22458344317497092), VL auc: 0.9825703145789224 VL ap: 0.821638472214494 VL f1: 0.776595744680851 \n",
      "Epoch: 86, TR loss: 0.12325571800526952, VAL loss: (0.10271662535260331, 0.21943830936513048), VL auc: 0.9792671780454225 VL ap: 0.7970581170369639 VL f1: 0.7446808510638298 \n",
      "Epoch: 88, TR loss: 0.12282770262357363, VAL loss: (0.1028350697320852, 0.23382671843183803), VL auc: 0.9832818483681002 VL ap: 0.8237249144218807 VL f1: 0.776595744680851 \n",
      "Epoch: 90, TR loss: 0.12280126301844331, VAL loss: (0.10272938664546799, 0.23373173652811252), VL auc: 0.9821480221674592 VL ap: 0.830753108468852 VL f1: 0.7978723404255319 \n",
      "Epoch: 92, TR loss: 0.1234951383391841, VAL loss: (0.10285460994320113, 0.2241686557201629), VL auc: 0.9830851642312544 VL ap: 0.818741331168385 VL f1: 0.7872340425531915 \n",
      "Epoch: 94, TR loss: 0.12365372368826098, VAL loss: (0.10272295621505828, 0.22363784465383976), VL auc: 0.9832992028507631 VL ap: 0.8318340337214687 VL f1: 0.7872340425531915 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96, TR loss: 0.12273623204616373, VAL loss: (0.10257888138456277, 0.2255839692785385), VL auc: 0.9816620966528988 VL ap: 0.8261484623718134 VL f1: 0.776595744680851 \n",
      "Epoch: 98, TR loss: 0.1234413404646774, VAL loss: (0.10260520881125952, 0.22714012227159866), VL auc: 0.981826964238196 VL ap: 0.8331569322487256 VL f1: 0.776595744680851 \n",
      "Epoch: 100, TR loss: 0.12297064827402257, VAL loss: (0.1025914186495293, 0.23071504146494765), VL auc: 0.9826975807851168 VL ap: 0.8372612630744808 VL f1: 0.776595744680851 \n",
      "Epoch: 102, TR loss: 0.12286692883999863, VAL loss: (0.1025834615104804, 0.23606371372304064), VL auc: 0.9837909131928777 VL ap: 0.8468218817743446 VL f1: 0.7978723404255319 \n",
      "Epoch: 104, TR loss: 0.1228806863972444, VAL loss: (0.10258436591940252, 0.2397415485787899), VL auc: 0.98489292284197 VL ap: 0.8593665132648863 VL f1: 0.8191489361702128 \n",
      "Epoch: 106, TR loss: 0.12263014499778206, VAL loss: (0.10239731092178155, 0.23281113644863696), VL auc: 0.9845545104300442 VL ap: 0.8547107067479738 VL f1: 0.8085106382978723 \n",
      "Epoch: 108, TR loss: 0.12309791688176853, VAL loss: (0.1024325911670745, 0.2293692243860123), VL auc: 0.982801707681094 VL ap: 0.8579130601375639 VL f1: 0.7978723404255319 \n",
      "Epoch: 110, TR loss: 0.12251730614063265, VAL loss: (0.10251034544238971, 0.24066537491818693), VL auc: 0.9845660800851526 VL ap: 0.8745954821056067 VL f1: 0.8191489361702128 \n",
      "Epoch: 112, TR loss: 0.12259580338637267, VAL loss: (0.10248681421575499, 0.23917451817938623), VL auc: 0.9850115118068331 VL ap: 0.8600797941488828 VL f1: 0.8191489361702128 \n",
      "Epoch: 114, TR loss: 0.1229868780090362, VAL loss: (0.1024155899388042, 0.23298318335350524), VL auc: 0.9826744414748998 VL ap: 0.8637926863617965 VL f1: 0.7978723404255319 \n",
      "Epoch: 116, TR loss: 0.12256990153592297, VAL loss: (0.10252332246582296, 0.23883007942362033), VL auc: 0.9837648814688835 VL ap: 0.8764604872641848 VL f1: 0.8085106382978723 \n",
      "Epoch: 118, TR loss: 0.12343971973182055, VAL loss: (0.10245575731120854, 0.23041447172773646), VL auc: 0.9815290456191502 VL ap: 0.8541801079335907 VL f1: 0.7978723404255319 \n",
      "Epoch: 120, TR loss: 0.1233881550330014, VAL loss: (0.10248769373268846, 0.22698564732328375), VL auc: 0.979478324251154 VL ap: 0.8418316830218547 VL f1: 0.7659574468085105 \n",
      "Epoch: 122, TR loss: 0.12269103077773189, VAL loss: (0.10240379943349817, 0.2355920507552776), VL auc: 0.9851156387028103 VL ap: 0.8617419197539724 VL f1: 0.7978723404255319 \n",
      "Epoch: 124, TR loss: 0.12370835060123363, VAL loss: (0.10231119293826893, 0.22280189838815243), VL auc: 0.9805745490726921 VL ap: 0.853642300303961 VL f1: 0.776595744680851 \n",
      "Epoch: 126, TR loss: 0.12306807896580932, VAL loss: (0.10235452989056552, 0.2284140688307742), VL auc: 0.9822723959598765 VL ap: 0.8524982684781995 VL f1: 0.776595744680851 \n",
      "Epoch: 128, TR loss: 0.12256439701926165, VAL loss: (0.10235721822534326, 0.23270422347048494), VL auc: 0.9831545821619057 VL ap: 0.8510289046806679 VL f1: 0.8085106382978723 \n",
      "Epoch: 130, TR loss: 0.12277602589252937, VAL loss: (0.10227555590780485, 0.23104278077470494), VL auc: 0.9848611062904216 VL ap: 0.8510913978699051 VL f1: 0.8191489361702128 \n",
      "Epoch: 132, TR loss: 0.12243728525663088, VAL loss: (0.10227549782649793, 0.23239273720599235), VL auc: 0.9828537711290827 VL ap: 0.8577536411015793 VL f1: 0.8085106382978723 \n",
      "Epoch: 134, TR loss: 0.12350613990369172, VAL loss: (0.1022314887905069, 0.22200967910441946), VL auc: 0.983617368366249 VL ap: 0.8559583173691054 VL f1: 0.7872340425531915 \n",
      "Epoch: 136, TR loss: 0.12349212840673565, VAL loss: (0.10220862135023705, 0.22765873848123752), VL auc: 0.9820554649265906 VL ap: 0.8651614454023402 VL f1: 0.7978723404255319 \n",
      "Epoch: 138, TR loss: 0.12274704689029613, VAL loss: (0.10226906739608822, 0.23185301841573513), VL auc: 0.9827901380259855 VL ap: 0.875189555536057 VL f1: 0.8085106382978723 \n",
      "Epoch: 140, TR loss: 0.12262891264330565, VAL loss: (0.10229293881323494, 0.23459749018892329), VL auc: 0.9848842456006387 VL ap: 0.8827217902331264 VL f1: 0.8297872340425532 \n",
      "Epoch: 142, TR loss: 0.12315951219914419, VAL loss: (0.1023872379636946, 0.22717066014066656), VL auc: 0.9823967697522937 VL ap: 0.862908262458019 VL f1: 0.7978723404255319 \n",
      "Epoch: 144, TR loss: 0.12255898212838044, VAL loss: (0.10211613101762167, 0.22706186010482463), VL auc: 0.9829926069903858 VL ap: 0.8738295018239105 VL f1: 0.8191489361702128 \n",
      "Epoch: 146, TR loss: 0.12343507412888521, VAL loss: (0.1022386825752362, 0.2224515752589449), VL auc: 0.9824864345793851 VL ap: 0.8585290490676042 VL f1: 0.7872340425531915 \n",
      "Epoch: 148, TR loss: 0.12296332883531418, VAL loss: (0.10219676446629453, 0.23115149964677525), VL auc: 0.9821190980296879 VL ap: 0.8635237985562574 VL f1: 0.8085106382978723 \n",
      "Epoch: 150, TR loss: 0.1227141467601834, VAL loss: (0.10223811835682606, 0.2348478195515085), VL auc: 0.9829868221628313 VL ap: 0.864466150212428 VL f1: 0.8085106382978723 \n",
      "Epoch: 152, TR loss: 0.12196107361168472, VAL loss: (0.10249367610730187, 0.24273892666431182), VL auc: 0.9832934180232088 VL ap: 0.8622711984893172 VL f1: 0.8085106382978723 \n",
      "Epoch: 154, TR loss: 0.12261890443119416, VAL loss: (0.10233990169854966, 0.23866836060868932), VL auc: 0.9842479145696668 VL ap: 0.8783097289459061 VL f1: 0.8191489361702128 \n",
      "Epoch: 156, TR loss: 0.12187059638667094, VAL loss: (0.10233386124262932, 0.2430904469591506), VL auc: 0.9860469959390511 VL ap: 0.8679036895338396 VL f1: 0.8191489361702128 \n",
      "Epoch: 158, TR loss: 0.12322530499055463, VAL loss: (0.1023789821207815, 0.22963970265489944), VL auc: 0.9852833987018849 VL ap: 0.8837163603922832 VL f1: 0.8297872340425532 \n",
      "Epoch: 160, TR loss: 0.1223131535511885, VAL loss: (0.10224600081990892, 0.23718371289841672), VL auc: 0.9851214235303647 VL ap: 0.8677945533372615 VL f1: 0.8297872340425532 \n",
      "Epoch: 162, TR loss: 0.12255249172813801, VAL loss: (0.10222684228595279, 0.23558480688866149), VL auc: 0.9846181435331414 VL ap: 0.8908718471007949 VL f1: 0.8297872340425532 \n",
      "Epoch: 164, TR loss: 0.122686773553177, VAL loss: (0.10229482230704527, 0.2374376946307243), VL auc: 0.9861048442145941 VL ap: 0.877476659938475 VL f1: 0.8297872340425532 \n",
      "Epoch: 166, TR loss: 0.12260487799660809, VAL loss: (0.1021961753558957, 0.236514964002244), VL auc: 0.985404880080525 VL ap: 0.8730628011850126 VL f1: 0.8297872340425532 \n",
      "Epoch: 168, TR loss: 0.12340754407676365, VAL loss: (0.10203728979213397, 0.22488610287930103), VL auc: 0.9827785683708768 VL ap: 0.8623893002021612 VL f1: 0.8085106382978723 \n",
      "Epoch: 170, TR loss: 0.12301189853931489, VAL loss: (0.10210161069089009, 0.23332985411299037), VL auc: 0.985607349044925 VL ap: 0.8795542890045979 VL f1: 0.8297872340425532 \n",
      "Epoch: 172, TR loss: 0.12254940710753946, VAL loss: (0.10204908029744002, 0.2297730547316531), VL auc: 0.9822723959598765 VL ap: 0.8572010736247104 VL f1: 0.8085106382978723 \n",
      "Epoch: 174, TR loss: 0.12314925004732241, VAL loss: (0.10249175112684374, 0.24638086684206698), VL auc: 0.9863130980065484 VL ap: 0.8723133752795136 VL f1: 0.8404255319148938 \n",
      "Epoch: 176, TR loss: 0.12260862734174238, VAL loss: (0.10223996866131814, 0.2376026397055768), VL auc: 0.9848668911179759 VL ap: 0.8754429316545709 VL f1: 0.8297872340425532 \n",
      "Epoch: 178, TR loss: 0.12313486510961591, VAL loss: (0.10213804426499201, 0.23098645311720828), VL auc: 0.9833570511263059 VL ap: 0.8829387149388932 VL f1: 0.8297872340425532 \n",
      "Epoch: 180, TR loss: 0.12289186721331223, VAL loss: (0.10221920874275676, 0.23328486909257604), VL auc: 0.9864056552474172 VL ap: 0.8876379305256781 VL f1: 0.8404255319148938 \n",
      "Epoch: 182, TR loss: 0.12228297953855391, VAL loss: (0.102173191753012, 0.23945017063871343), VL auc: 0.9849594483588445 VL ap: 0.8755349687067985 VL f1: 0.8404255319148938 \n",
      "Epoch: 184, TR loss: 0.12330059064584098, VAL loss: (0.10217880904512473, 0.23229112016393783), VL auc: 0.9855552855969365 VL ap: 0.8796232469857512 VL f1: 0.8297872340425532 \n",
      "Epoch: 186, TR loss: 0.12322330334813235, VAL loss: (0.10216296114566341, 0.22448754817881483), VL auc: 0.9826975807851168 VL ap: 0.8770277871114052 VL f1: 0.7978723404255319 \n",
      "Epoch: 188, TR loss: 0.12322192161735576, VAL loss: (0.10214084046505403, 0.22392768048225564), VL auc: 0.982043895271482 VL ap: 0.8709273309938618 VL f1: 0.7978723404255319 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, TR loss: 0.12250018761663302, VAL loss: (0.10237310761145238, 0.23237715376184342), VL auc: 0.9847685490495528 VL ap: 0.8788917439843048 VL f1: 0.7978723404255319 \n",
      "Epoch: 192, TR loss: 0.1225065435782053, VAL loss: (0.10220680423506322, 0.2399122156995408), VL auc: 0.985526361459165 VL ap: 0.8740975750447225 VL f1: 0.8297872340425532 \n",
      "Epoch: 194, TR loss: 0.12280640903198421, VAL loss: (0.10227024561688587, 0.23904974917148022), VL auc: 0.9838342993995349 VL ap: 0.8690827423264876 VL f1: 0.8191489361702128 \n",
      "Epoch: 196, TR loss: 0.12264856309559319, VAL loss: (0.10230399085621007, 0.23772363459810297), VL auc: 0.9836520773315749 VL ap: 0.8598610208267399 VL f1: 0.776595744680851 \n",
      "Epoch: 198, TR loss: 0.12335820508481705, VAL loss: (0.10209389417439844, 0.22857911536034117), VL auc: 0.9830967338863629 VL ap: 0.8828498268720525 VL f1: 0.8297872340425532 \n",
      "Epoch: 200, TR loss: 0.12227562275577046, VAL loss: (0.10223516450750238, 0.23631925785795171), VL auc: 0.9848611062904216 VL ap: 0.8638935990492193 VL f1: 0.8297872340425532 \n",
      "Epoch: 202, TR loss: 0.12300525876277227, VAL loss: (0.10228874036447713, 0.23871714003542635), VL auc: 0.9862321104207884 VL ap: 0.8809619223963098 VL f1: 0.8404255319148938 \n",
      "Epoch: 204, TR loss: 0.12251602897326619, VAL loss: (0.1021723205334081, 0.239695650465945), VL auc: 0.9851561324956903 VL ap: 0.8596830792189913 VL f1: 0.8191489361702128 \n",
      "Epoch: 206, TR loss: 0.12260221162965003, VAL loss: (0.10210600827555737, 0.23915347647159657), VL auc: 0.9845660800851527 VL ap: 0.868422268402712 VL f1: 0.8297872340425532 \n",
      "Epoch: 208, TR loss: 0.12266753388571494, VAL loss: (0.1020867086869987, 0.23507942037379487), VL auc: 0.9856594124929134 VL ap: 0.8732075117567968 VL f1: 0.8297872340425532 \n",
      "Epoch: 210, TR loss: 0.12256028170219194, VAL loss: (0.1021557258742863, 0.2390772636900557), VL auc: 0.9850057269792788 VL ap: 0.8767746197694131 VL f1: 0.8404255319148938 \n",
      "Epoch: 212, TR loss: 0.12284385767043716, VAL loss: (0.10223104073471061, 0.23357472521193484), VL auc: 0.9859862552497312 VL ap: 0.878488053561587 VL f1: 0.8510638297872339 \n",
      "Epoch: 214, TR loss: 0.12295586748912063, VAL loss: (0.10226637906131049, 0.244238407053846), VL auc: 0.9851850566334618 VL ap: 0.88083911738745 VL f1: 0.8404255319148938 \n",
      "Epoch: 216, TR loss: 0.12281317577838198, VAL loss: (0.10211405668523145, 0.23247266323008436), VL auc: 0.9835421656080433 VL ap: 0.8811266811584046 VL f1: 0.8297872340425532 \n",
      "Epoch: 218, TR loss: 0.1241616404529185, VAL loss: (0.10204401892640787, 0.21949096436196186), VL auc: 0.983021531128157 VL ap: 0.8706033446620532 VL f1: 0.8085106382978723 \n",
      "Epoch: 220, TR loss: 0.12294225183936001, VAL loss: (0.10206260494462428, 0.22763901568473655), VL auc: 0.982778568370877 VL ap: 0.8812937492567475 VL f1: 0.8191489361702128 \n",
      "Epoch: 222, TR loss: 0.12214548612306121, VAL loss: (0.1022828243685002, 0.2453100732032289), VL auc: 0.9853528166325363 VL ap: 0.8710304706149468 VL f1: 0.8404255319148938 \n",
      "Epoch: 224, TR loss: 0.12309533267177558, VAL loss: (0.10199822596456125, 0.23090650680217337), VL auc: 0.98650399731584 VL ap: 0.8845624945039571 VL f1: 0.8297872340425532 \n",
      "Epoch: 226, TR loss: 0.12263423043959175, VAL loss: (0.10221527580854489, 0.23664653047602227), VL auc: 0.9846644221535756 VL ap: 0.8737087721672091 VL f1: 0.8191489361702128 \n",
      "Epoch: 228, TR loss: 0.1231783634882258, VAL loss: (0.10206831350736219, 0.23468614131846327), VL auc: 0.9856189187000337 VL ap: 0.8745013528425747 VL f1: 0.8297872340425532 \n",
      "Epoch: 230, TR loss: 0.1224835246903489, VAL loss: (0.10212315885575976, 0.23477065309565118), VL auc: 0.9853875255978619 VL ap: 0.8653243220278605 VL f1: 0.8085106382978723 \n",
      "Epoch: 232, TR loss: 0.12285069163617, VAL loss: (0.10222246129594463, 0.2441316361122943), VL auc: 0.9845545104300442 VL ap: 0.878544916397802 VL f1: 0.8404255319148938 \n",
      "Epoch: 234, TR loss: 0.12242210862453347, VAL loss: (0.10220993232830768, 0.24343125363613696), VL auc: 0.984930524221073 VL ap: 0.8850976819366276 VL f1: 0.8510638297872339 \n",
      "Epoch: 236, TR loss: 0.12250510956572366, VAL loss: (0.10226875209756492, 0.24006756315840053), VL auc: 0.9877014566195783 VL ap: 0.8787274065479858 VL f1: 0.8404255319148938 \n",
      "Epoch: 238, TR loss: 0.12228538449698666, VAL loss: (0.10209236746575924, 0.23945004889305602), VL auc: 0.9848379669802044 VL ap: 0.8724985709297087 VL f1: 0.8404255319148938 \n",
      "Epoch: 240, TR loss: 0.12262532014328653, VAL loss: (0.10225832235430686, 0.24434824192777593), VL auc: 0.9882076290305786 VL ap: 0.8787843286204533 VL f1: 0.8404255319148938 \n",
      "Epoch: 242, TR loss: 0.12246557712788332, VAL loss: (0.10226893463881526, 0.2438966467025432), VL auc: 0.9864519338678515 VL ap: 0.8756107655848981 VL f1: 0.8404255319148938 \n",
      "Epoch: 244, TR loss: 0.12285701025366724, VAL loss: (0.10213180467316221, 0.23368218604554522), VL auc: 0.9791341270116738 VL ap: 0.8714674686416958 VL f1: 0.8191489361702128 \n",
      "Epoch: 246, TR loss: 0.12221954689268616, VAL loss: (0.10218132313598167, 0.24717925457244225), VL auc: 0.9860990593870397 VL ap: 0.8875160658412297 VL f1: 0.8510638297872339 \n",
      "Epoch: 248, TR loss: 0.12344989972667722, VAL loss: (0.10200337030888901, 0.2275167424628075), VL auc: 0.9868019159348859 VL ap: 0.8923619908458864 VL f1: 0.8297872340425532 \n",
      "Epoch: 250, TR loss: 0.12241791115049866, VAL loss: (0.10216191568213873, 0.24007953481471284), VL auc: 0.9859949324910625 VL ap: 0.885263815604586 VL f1: 0.8510638297872339 \n",
      "Epoch: 252, TR loss: 0.12248743088059838, VAL loss: (0.10208957126569722, 0.24012981577122466), VL auc: 0.9842594842247754 VL ap: 0.8860650545241292 VL f1: 0.8404255319148938 \n",
      "Epoch: 254, TR loss: 0.12281759731686705, VAL loss: (0.10210889574624456, 0.23487718054588805), VL auc: 0.98479168835977 VL ap: 0.8935603732900181 VL f1: 0.8297872340425532 \n",
      "Epoch: 256, TR loss: 0.12221145069721687, VAL loss: (0.10218988598008853, 0.2415474222061482), VL auc: 0.9857172607684566 VL ap: 0.8839660292237419 VL f1: 0.8297872340425532 \n",
      "Epoch: 258, TR loss: 0.12283470090323666, VAL loss: (0.10202953178899453, 0.2263666721100503), VL auc: 0.982182731132785 VL ap: 0.862120501812558 VL f1: 0.7978723404255319 \n",
      "Epoch: 260, TR loss: 0.12225942289601689, VAL loss: (0.10215251480774623, 0.23583525799690408), VL auc: 0.9831256580241343 VL ap: 0.8660040314176896 VL f1: 0.7978723404255319 \n",
      "Epoch: 262, TR loss: 0.12230719343681168, VAL loss: (0.10213644288038676, 0.23745971030377327), VL auc: 0.9840685849154837 VL ap: 0.8681665126051961 VL f1: 0.8085106382978723 \n",
      "Epoch: 264, TR loss: 0.12217297883110774, VAL loss: (0.10210195088140209, 0.23969723315949135), VL auc: 0.9855957793898165 VL ap: 0.8792119395966581 VL f1: 0.8297872340425532 \n",
      "Epoch: 266, TR loss: 0.12255439627596519, VAL loss: (0.10219028425190746, 0.2401499849684695), VL auc: 0.9856767669755766 VL ap: 0.8789651787219521 VL f1: 0.8404255319148938 \n",
      "Epoch: 268, TR loss: 0.1229146022861983, VAL loss: (0.1022458348733177, 0.2321581739060422), VL auc: 0.9857143683546794 VL ap: 0.8905026841542231 VL f1: 0.8297872340425532 \n",
      "Epoch: 270, TR loss: 0.12224361141464375, VAL loss: (0.10219856498680924, 0.24048819440476438), VL auc: 0.9865618455913829 VL ap: 0.8923289449059435 VL f1: 0.8297872340425532 \n",
      "Epoch: 272, TR loss: 0.12236494231445791, VAL loss: (0.1022709840792168, 0.24779936607847822), VL auc: 0.9861453380074741 VL ap: 0.8938603722130841 VL f1: 0.8404255319148938 \n",
      "Epoch: 274, TR loss: 0.12281386290936276, VAL loss: (0.10224085647558116, 0.2425124797415226), VL auc: 0.987377506276538 VL ap: 0.8928949231694957 VL f1: 0.8404255319148938 \n",
      "Epoch: 276, TR loss: 0.12224518733461057, VAL loss: (0.10188004709962531, 0.23045574350559966), VL auc: 0.98291740423218 VL ap: 0.8725635895819944 VL f1: 0.8297872340425532 \n",
      "Epoch: 278, TR loss: 0.12249618433178842, VAL loss: (0.10198802024920134, 0.23716512639471826), VL auc: 0.9839933821572778 VL ap: 0.887552014990143 VL f1: 0.8404255319148938 \n",
      "Epoch: 280, TR loss: 0.12281332515468214, VAL loss: (0.10196638911103606, 0.23047120520409117), VL auc: 0.9847627642219986 VL ap: 0.8833894433365166 VL f1: 0.8404255319148938 \n",
      "Epoch: 282, TR loss: 0.12246619703952903, VAL loss: (0.10195854813460101, 0.22878293788179438), VL auc: 0.9856825518031308 VL ap: 0.886977269607632 VL f1: 0.8297872340425532 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 284, TR loss: 0.12225836232428568, VAL loss: (0.10203978728833181, 0.23412805922487948), VL auc: 0.9854511587009591 VL ap: 0.8895409036639086 VL f1: 0.8404255319148938 \n",
      "Epoch: 286, TR loss: 0.12279854436978019, VAL loss: (0.10188319178752889, 0.2296563412280793), VL auc: 0.9854656207698449 VL ap: 0.8872398323839246 VL f1: 0.8297872340425532 \n",
      "Epoch: 288, TR loss: 0.12247027501252371, VAL loss: (0.10190210140159818, 0.23139340826805602), VL auc: 0.9835884442284776 VL ap: 0.8844940480327504 VL f1: 0.8297872340425532 \n",
      "Epoch: 290, TR loss: 0.12265765264345861, VAL loss: (0.1019221311551582, 0.2266729841841028), VL auc: 0.9828335242326427 VL ap: 0.8843417947998169 VL f1: 0.8191489361702128 \n",
      "Epoch: 292, TR loss: 0.12267560020592419, VAL loss: (0.10204102359043637, 0.23274468361063205), VL auc: 0.9856449504240281 VL ap: 0.8943129671109299 VL f1: 0.8404255319148938 \n",
      "Epoch: 294, TR loss: 0.12266741438467481, VAL loss: (0.10215317029678155, 0.2436289888747195), VL auc: 0.9908136938437866 VL ap: 0.8931629308619992 VL f1: 0.8404255319148938 \n",
      "Epoch: 296, TR loss: 0.12266809404684058, VAL loss: (0.10196862939001751, 0.2403412473962662), VL auc: 0.9848842456006387 VL ap: 0.8954043195604412 VL f1: 0.8404255319148938 \n",
      "Epoch: 298, TR loss: 0.1233928603864568, VAL loss: (0.10182961593055415, 0.22727641653507313), VL auc: 0.9824256938900652 VL ap: 0.8887627474119539 VL f1: 0.8297872340425532 \n",
      "Epoch: 300, TR loss: 0.12170018790343551, VAL loss: (0.10203461805201536, 0.24361527219731757), VL auc: 0.9848842456006386 VL ap: 0.8884066649646302 VL f1: 0.8404255319148938 \n",
      "Stopping at epoch 267, TR loss: 0.12167412173905562, VAL loss: 0.10216213971003688, VAL auc: 0.9844156745687411,TS loss: 0.10216213971003688, TS auc: 0.9844156745687411 TS ap: 0.8816149485145598 TS f1: 0.8404255319148938\n",
      "Final training run 1: 0.9844156745687411, (0.9844156745687411, 0.8816149485145598, 0.8404255319148938)\n",
      "Normal Cls: 0\n",
      "Epoch: 1, TR loss: 0.8498297946685328, VAL loss: (0.7636388848176658, 0.7904286486037234), VL auc: 0.6180278365901912 VL ap: 0.20829447269350407 VL f1: 0.23404255319148937 \n",
      "Epoch: 2, TR loss: 0.673808108575777, VAL loss: (0.5642935507578847, 0.6766211327086104), VL auc: 0.8813011234135111 VL ap: 0.3063683078597995 VL f1: 0.32978723404255317 \n",
      "Epoch: 4, TR loss: 0.36478887512236907, VAL loss: (0.29722458308905997, 0.5303125178560297), VL auc: 0.9311027038283989 VL ap: 0.47093871415550653 VL f1: 0.47872340425531923 \n",
      "Epoch: 6, TR loss: 0.19716481093501437, VAL loss: (0.16023203780820927, 0.35899673624241607), VL auc: 0.9475605382203557 VL ap: 0.5330330830314917 VL f1: 0.5212765957446809 \n",
      "Epoch: 8, TR loss: 0.1456433117418854, VAL loss: (0.12022235615218445, 0.28110514295862077), VL auc: 0.9453912278874967 VL ap: 0.5319802192498565 VL f1: 0.5106382978723404 \n",
      "Epoch: 10, TR loss: 0.13402707869669298, VAL loss: (0.11247281628889257, 0.24486572184461228), VL auc: 0.9451945437506508 VL ap: 0.5448241642572554 VL f1: 0.5425531914893617 \n",
      "Epoch: 12, TR loss: 0.13037282651508197, VAL loss: (0.10945644658696982, 0.22770889769209193), VL auc: 0.9455705575416797 VL ap: 0.5673917605788048 VL f1: 0.5425531914893617 \n",
      "Epoch: 14, TR loss: 0.12901950711078988, VAL loss: (0.10770378506916123, 0.2199554240449946), VL auc: 0.949932317517615 VL ap: 0.6050666886925902 VL f1: 0.5638297872340425 \n",
      "Epoch: 16, TR loss: 0.12787586721904828, VAL loss: (0.10694296973240382, 0.21280136514217296), VL auc: 0.9490703782120254 VL ap: 0.6046247475039787 VL f1: 0.5638297872340425 \n",
      "Epoch: 18, TR loss: 0.1268184025148755, VAL loss: (0.10611111265994597, 0.2131669876423288), VL auc: 0.957817037474113 VL ap: 0.6492776632056493 VL f1: 0.5957446808510638 \n",
      "Epoch: 20, TR loss: 0.12625977996512483, VAL loss: (0.10567142886918417, 0.21156639748431266), VL auc: 0.9591359781564912 VL ap: 0.6524073495272744 VL f1: 0.6063829787234043 \n",
      "Epoch: 22, TR loss: 0.1260058551924636, VAL loss: (0.10513528032494732, 0.21060829974235373), VL auc: 0.9621845822776023 VL ap: 0.6842224426454203 VL f1: 0.6170212765957447 \n",
      "Epoch: 24, TR loss: 0.125394099492742, VAL loss: (0.10479200320635365, 0.2119598185762446), VL auc: 0.9654935036386566 VL ap: 0.7009020629365978 VL f1: 0.6276595744680851 \n",
      "Epoch: 26, TR loss: 0.12566385815322137, VAL loss: (0.10453017267472982, 0.20823549717030626), VL auc: 0.963867967095901 VL ap: 0.7005758248405021 VL f1: 0.6276595744680851 \n",
      "Epoch: 28, TR loss: 0.1250986032957431, VAL loss: (0.10430896586863615, 0.21181751819367103), VL auc: 0.9677380167297214 VL ap: 0.7288785104571663 VL f1: 0.648936170212766 \n",
      "Epoch: 30, TR loss: 0.12516233469421117, VAL loss: (0.10404502781530384, 0.207436906530502), VL auc: 0.9693577684449227 VL ap: 0.7320002410754809 VL f1: 0.648936170212766 \n",
      "Epoch: 32, TR loss: 0.12459052221715537, VAL loss: (0.10397222704573647, 0.2115388626747943), VL auc: 0.9693404139622598 VL ap: 0.7411170939359205 VL f1: 0.6595744680851063 \n",
      "Epoch: 34, TR loss: 0.12433030870225695, VAL loss: (0.10379799142228707, 0.21076581833210398), VL auc: 0.9709428111947983 VL ap: 0.7464127658042234 VL f1: 0.6595744680851063 \n",
      "Epoch: 36, TR loss: 0.12412785900263476, VAL loss: (0.10372303334703388, 0.21618688867447225), VL auc: 0.9741128966945496 VL ap: 0.7841720249589045 VL f1: 0.7127659574468085 \n",
      "Epoch: 38, TR loss: 0.12400353310800218, VAL loss: (0.10353371317844277, 0.21429830916384432), VL auc: 0.9737658070412921 VL ap: 0.7817407130395746 VL f1: 0.7021276595744681 \n",
      "Epoch: 40, TR loss: 0.1236962809961798, VAL loss: (0.10344387799128688, 0.22073583399995844), VL auc: 0.976750778059306 VL ap: 0.8148871016087392 VL f1: 0.7553191489361702 \n",
      "Epoch: 42, TR loss: 0.12398060384592588, VAL loss: (0.10333432834909427, 0.21911864585064827), VL auc: 0.9770053104716949 VL ap: 0.813748834408192 VL f1: 0.7340425531914893 \n",
      "Epoch: 44, TR loss: 0.12373210143296087, VAL loss: (0.10315420162165664, 0.2181406426937022), VL auc: 0.977265627711638 VL ap: 0.8303567475905017 VL f1: 0.7553191489361702 \n",
      "Epoch: 46, TR loss: 0.12378239643322855, VAL loss: (0.10298397360838517, 0.2174780622441718), VL auc: 0.978677125634885 VL ap: 0.836095384671873 VL f1: 0.7553191489361702 \n",
      "Epoch: 48, TR loss: 0.12403810625267682, VAL loss: (0.10296267436340233, 0.21824540483190658), VL auc: 0.9769763863339235 VL ap: 0.8396372608613736 VL f1: 0.7553191489361702 \n",
      "Epoch: 50, TR loss: 0.12380454893854397, VAL loss: (0.10298393212173736, 0.21781383676731841), VL auc: 0.9806439670033437 VL ap: 0.8420648414254068 VL f1: 0.7553191489361702 \n",
      "Epoch: 52, TR loss: 0.12351752237776478, VAL loss: (0.10297042406921221, 0.22462771801238365), VL auc: 0.9806034732104637 VL ap: 0.8512838356535876 VL f1: 0.7659574468085105 \n",
      "Epoch: 54, TR loss: 0.1241752411650491, VAL loss: (0.10294914971621805, 0.2229336677713597), VL auc: 0.980510915969595 VL ap: 0.8572847455374272 VL f1: 0.7659574468085105 \n",
      "Epoch: 56, TR loss: 0.12395453768154598, VAL loss: (0.10280875890004758, 0.21861311729918134), VL auc: 0.9796605463191143 VL ap: 0.852597149216223 VL f1: 0.776595744680851 \n",
      "Epoch: 58, TR loss: 0.12417816147171745, VAL loss: (0.10273698699934578, 0.21887060936461103), VL auc: 0.9812398042414356 VL ap: 0.8637341274691587 VL f1: 0.776595744680851 \n",
      "Epoch: 60, TR loss: 0.12434207208589544, VAL loss: (0.10262084927748182, 0.21393678543415476), VL auc: 0.9806613214860067 VL ap: 0.8507661402852179 VL f1: 0.7659574468085105 \n",
      "Epoch: 62, TR loss: 0.12341950911840736, VAL loss: (0.10272975172796867, 0.22721375810339095), VL auc: 0.9813844249302929 VL ap: 0.8678686010177955 VL f1: 0.7872340425531915 \n",
      "Epoch: 64, TR loss: 0.12362359448851643, VAL loss: (0.10262191133566562, 0.21940738596814743), VL auc: 0.9805919035553551 VL ap: 0.8644139577229288 VL f1: 0.776595744680851 \n",
      "Epoch: 66, TR loss: 0.12361009833979594, VAL loss: (0.10259410698430703, 0.21613733819190492), VL auc: 0.9798572304559601 VL ap: 0.8501131507913189 VL f1: 0.7659574468085105 \n",
      "Epoch: 68, TR loss: 0.12333401359300432, VAL loss: (0.10268487976970331, 0.22645321298152843), VL auc: 0.9845169090509411 VL ap: 0.879020958138318 VL f1: 0.7872340425531915 \n",
      "Epoch: 70, TR loss: 0.12353791971155317, VAL loss: (0.10247002042072373, 0.221135768484562), VL auc: 0.9812918676894242 VL ap: 0.8745784812615693 VL f1: 0.7872340425531915 \n",
      "Epoch: 72, TR loss: 0.12364080263829616, VAL loss: (0.10259272133027036, 0.22132394668903757), VL auc: 0.9829578980250598 VL ap: 0.8718294544595336 VL f1: 0.7872340425531915 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74, TR loss: 0.12321904612357747, VAL loss: (0.1025795783602459, 0.23127093213669797), VL auc: 0.9835479504355975 VL ap: 0.8836266524263604 VL f1: 0.7978723404255319 \n",
      "Epoch: 76, TR loss: 0.12325175953331498, VAL loss: (0.10245208989154261, 0.22211040334498627), VL auc: 0.983021531128157 VL ap: 0.871530916128365 VL f1: 0.7872340425531915 \n",
      "Epoch: 78, TR loss: 0.12332398297444781, VAL loss: (0.10245544201268522, 0.22822410502332321), VL auc: 0.981928198720396 VL ap: 0.8800468280696396 VL f1: 0.7872340425531915 \n",
      "Epoch: 80, TR loss: 0.1226813511934808, VAL loss: (0.10240681966145833, 0.23024644242956283), VL auc: 0.9834669628498374 VL ap: 0.8895914830501871 VL f1: 0.7978723404255319 \n",
      "Epoch: 82, TR loss: 0.12307380754692089, VAL loss: (0.10246351531434798, 0.22877342142957321), VL auc: 0.9832240000925572 VL ap: 0.8861224750003667 VL f1: 0.7978723404255319 \n",
      "Epoch: 84, TR loss: 0.12304671068606982, VAL loss: (0.10235875323131202, 0.2254658151180186), VL auc: 0.983397544919186 VL ap: 0.8831334551012976 VL f1: 0.7978723404255319 \n",
      "Epoch: 86, TR loss: 0.12325813790133229, VAL loss: (0.10236529152700602, 0.22405608156894116), VL auc: 0.9820034014786019 VL ap: 0.8781148464111712 VL f1: 0.7978723404255319 \n",
      "Epoch: 88, TR loss: 0.12387432260835399, VAL loss: (0.10238089880391008, 0.21273856467389046), VL auc: 0.9809332083810581 VL ap: 0.8775897829474777 VL f1: 0.7978723404255319 \n",
      "Epoch: 90, TR loss: 0.12288974606984979, VAL loss: (0.10243140464894729, 0.2308197224393804), VL auc: 0.9837214952622262 VL ap: 0.8976984717207814 VL f1: 0.8297872340425532 \n",
      "Epoch: 92, TR loss: 0.1228319523793135, VAL loss: (0.10236705056087293, 0.23539748090378781), VL auc: 0.9850982842201473 VL ap: 0.8996659724637536 VL f1: 0.8191489361702128 \n",
      "Epoch: 94, TR loss: 0.12328552604596871, VAL loss: (0.1022318289810189, 0.227813923612554), VL auc: 0.9836491849177977 VL ap: 0.8926730829745075 VL f1: 0.8297872340425532 \n",
      "Epoch: 96, TR loss: 0.12382838192723629, VAL loss: (0.10218548839542126, 0.22229509150728266), VL auc: 0.9834033297467404 VL ap: 0.8942011535400856 VL f1: 0.8297872340425532 \n",
      "Epoch: 98, TR loss: 0.12296453131453056, VAL loss: (0.10219778503783052, 0.22987258180658868), VL auc: 0.9834322538845118 VL ap: 0.8915555954023148 VL f1: 0.8297872340425532 \n",
      "Epoch: 100, TR loss: 0.12265789164553888, VAL loss: (0.10231069509849527, 0.2321004258825424), VL auc: 0.9850404359446046 VL ap: 0.8938267337439193 VL f1: 0.8404255319148938 \n",
      "Epoch: 102, TR loss: 0.12287062590342786, VAL loss: (0.10228486551157219, 0.23809039339106133), VL auc: 0.9841842814665696 VL ap: 0.8986038672585929 VL f1: 0.8404255319148938 \n",
      "Epoch: 104, TR loss: 0.12279094111610148, VAL loss: (0.10247104099225972, 0.239602433874252), VL auc: 0.9855032221489477 VL ap: 0.9057721646713262 VL f1: 0.8297872340425532 \n",
      "Epoch: 106, TR loss: 0.12337872191964559, VAL loss: (0.10220567579824293, 0.22979058610632064), VL auc: 0.9854106649080792 VL ap: 0.9009836474072439 VL f1: 0.8191489361702128 \n",
      "Epoch: 108, TR loss: 0.12345051216950792, VAL loss: (0.10230055576177186, 0.22371762864133146), VL auc: 0.981008411139264 VL ap: 0.8946864493922506 VL f1: 0.8297872340425532 \n",
      "Epoch: 110, TR loss: 0.12321656647699461, VAL loss: (0.1022057670688681, 0.23021902936570188), VL auc: 0.9847396249117814 VL ap: 0.8989848267416326 VL f1: 0.8297872340425532 \n",
      "Epoch: 112, TR loss: 0.12235302955451924, VAL loss: (0.1021848412037155, 0.23433389054968), VL auc: 0.9860180718012798 VL ap: 0.9036100583312457 VL f1: 0.8297872340425532 \n",
      "Epoch: 114, TR loss: 0.12312254903366679, VAL loss: (0.10209367014650031, 0.2265372986489154), VL auc: 0.9850172966343874 VL ap: 0.8951451701918663 VL f1: 0.8297872340425532 \n",
      "Epoch: 116, TR loss: 0.12288272538374174, VAL loss: (0.10220157691743985, 0.22728808382724194), VL auc: 0.9853701711151992 VL ap: 0.8939283894881841 VL f1: 0.8191489361702128 \n",
      "Epoch: 118, TR loss: 0.12255923606809074, VAL loss: (0.10223854981796322, 0.2334953470433012), VL auc: 0.9863535917994285 VL ap: 0.9001132728383017 VL f1: 0.8297872340425532 \n",
      "Epoch: 120, TR loss: 0.12264149759659508, VAL loss: (0.10202881821865228, 0.23093030807819773), VL auc: 0.9835971214698092 VL ap: 0.8933381246961851 VL f1: 0.8297872340425532 \n",
      "Epoch: 122, TR loss: 0.12340836564641458, VAL loss: (0.10213617736584081, 0.22069478542246718), VL auc: 0.9803778649358463 VL ap: 0.882190300362206 VL f1: 0.8297872340425532 \n",
      "Epoch: 124, TR loss: 0.12278998510778037, VAL loss: (0.1021815305692207, 0.2399811846144656), VL auc: 0.9846730993949071 VL ap: 0.9045367338116038 VL f1: 0.8404255319148938 \n",
      "Epoch: 126, TR loss: 0.12341680540737426, VAL loss: (0.10233448354234638, 0.2328562837965945), VL auc: 0.9851156387028102 VL ap: 0.8959048296305042 VL f1: 0.8404255319148938 \n",
      "Epoch: 128, TR loss: 0.12270380992021154, VAL loss: (0.1022515849227034, 0.2357213243525079), VL auc: 0.9854395890458506 VL ap: 0.901362702299214 VL f1: 0.8297872340425532 \n",
      "Epoch: 130, TR loss: 0.1230684075936697, VAL loss: (0.10222765542424976, 0.22805148997205368), VL auc: 0.9857635393888909 VL ap: 0.8943427218245156 VL f1: 0.8297872340425532 \n",
      "Epoch: 132, TR loss: 0.12301717152271094, VAL loss: (0.10210746860556008, 0.2302605446348799), VL auc: 0.9832500318165516 VL ap: 0.8907663258709758 VL f1: 0.8404255319148938 \n",
      "Epoch: 134, TR loss: 0.12236207428949461, VAL loss: (0.1020941596889444, 0.2341352016367811), VL auc: 0.984033875950158 VL ap: 0.8965095002686582 VL f1: 0.8404255319148938 \n",
      "Epoch: 136, TR loss: 0.12307074533276738, VAL loss: (0.10211249678727399, 0.23189830780029297), VL auc: 0.985665197320468 VL ap: 0.900233637734595 VL f1: 0.8404255319148938 \n",
      "Epoch: 138, TR loss: 0.12249081425379726, VAL loss: (0.10217431189250271, 0.24385671412691157), VL auc: 0.9863998704198629 VL ap: 0.905121331765988 VL f1: 0.8404255319148938 \n",
      "Epoch: 140, TR loss: 0.12314337955872567, VAL loss: (0.10207336658106478, 0.23019025680866648), VL auc: 0.9844417062927354 VL ap: 0.8970667499416172 VL f1: 0.8404255319148938 \n",
      "Epoch: 142, TR loss: 0.12280384722843628, VAL loss: (0.10219455737663131, 0.2338276315242686), VL auc: 0.9856941214582393 VL ap: 0.9021425320217155 VL f1: 0.8404255319148938 \n",
      "Epoch: 144, TR loss: 0.12290330943790535, VAL loss: (0.10210110455378688, 0.22629283336882897), VL auc: 0.9847425173255586 VL ap: 0.8975321483898768 VL f1: 0.8404255319148938 \n",
      "Epoch: 146, TR loss: 0.12329808859281312, VAL loss: (0.10203158122939607, 0.22638341213794463), VL auc: 0.9852197655987875 VL ap: 0.90038544044885 VL f1: 0.8404255319148938 \n",
      "Epoch: 148, TR loss: 0.1228902763557154, VAL loss: (0.10219276515344616, 0.23894303910275724), VL auc: 0.9860122869737253 VL ap: 0.9094140124168183 VL f1: 0.8404255319148938 \n",
      "Epoch: 150, TR loss: 0.12283314738971488, VAL loss: (0.10202825400024215, 0.23433251076556266), VL auc: 0.9858908055950852 VL ap: 0.9093030458448527 VL f1: 0.8510638297872339 \n",
      "Epoch: 152, TR loss: 0.12248339772049376, VAL loss: (0.10222416224850463, 0.2406104067538647), VL auc: 0.9900674510892831 VL ap: 0.915191483680407 VL f1: 0.8404255319148938 \n",
      "Epoch: 154, TR loss: 0.12271094263854472, VAL loss: (0.10198347331260196, 0.2378702569515147), VL auc: 0.9867643145557831 VL ap: 0.912364236314739 VL f1: 0.8404255319148938 \n",
      "Epoch: 156, TR loss: 0.12289601240564198, VAL loss: (0.10199813469393608, 0.23654649612751413), VL auc: 0.98809482489327 VL ap: 0.913236207581103 VL f1: 0.8510638297872339 \n",
      "Epoch: 158, TR loss: 0.12319337580639302, VAL loss: (0.10218164673183455, 0.23607172864548703), VL auc: 0.9865907697291545 VL ap: 0.9193849332508638 VL f1: 0.8510638297872339 \n",
      "Epoch: 160, TR loss: 0.12291016581008321, VAL loss: (0.1019394476819518, 0.22261441007573554), VL auc: 0.9854569435285134 VL ap: 0.9050572184928436 VL f1: 0.8404255319148938 \n",
      "Epoch: 162, TR loss: 0.12299560905378121, VAL loss: (0.1021018430161178, 0.2290962300402053), VL auc: 0.9867035738664631 VL ap: 0.9084369451912406 VL f1: 0.8297872340425532 \n",
      "Epoch: 164, TR loss: 0.12245864606755537, VAL loss: (0.10199506468199854, 0.23631097915324759), VL auc: 0.9868366249002118 VL ap: 0.9137712874271228 VL f1: 0.8510638297872339 \n",
      "Epoch: 166, TR loss: 0.12266486751875688, VAL loss: (0.10192166650470279, 0.23564890597728974), VL auc: 0.9855899945622622 VL ap: 0.9123380518579483 VL f1: 0.8510638297872339 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 168, TR loss: 0.12338440568786711, VAL loss: (0.10259160119077963, 0.23823474315886803), VL auc: 0.9871866069672464 VL ap: 0.912923593780202 VL f1: 0.8510638297872339 \n",
      "Epoch: 170, TR loss: 0.12281520729606431, VAL loss: (0.10228383664270664, 0.24371076137461561), VL auc: 0.9895930952298313 VL ap: 0.9213210815089051 VL f1: 0.8617021276595744 \n",
      "Epoch: 172, TR loss: 0.12315270810867138, VAL loss: (0.10195191856828184, 0.23311779346871883), VL auc: 0.9868742262793146 VL ap: 0.9156224037108538 VL f1: 0.8510638297872339 \n",
      "Epoch: 174, TR loss: 0.12272821053884453, VAL loss: (0.10210476367612323, 0.23271286741216132), VL auc: 0.9867527449006747 VL ap: 0.9135921572203629 VL f1: 0.8510638297872339 \n",
      "Epoch: 176, TR loss: 0.12292729180289785, VAL loss: (0.10221870260565355, 0.24784664397544048), VL auc: 0.9877419504124582 VL ap: 0.9211816224410738 VL f1: 0.8617021276595744 \n",
      "Epoch: 178, TR loss: 0.12348767699299054, VAL loss: (0.10198874211687313, 0.22697712512726478), VL auc: 0.9861337683523654 VL ap: 0.9140521726089388 VL f1: 0.8510638297872339 \n",
      "Epoch: 180, TR loss: 0.12307989463115288, VAL loss: (0.1019525657599876, 0.23318394194258021), VL auc: 0.9862147559381256 VL ap: 0.9136121981694759 VL f1: 0.8404255319148938 \n",
      "Epoch: 182, TR loss: 0.12251632772586653, VAL loss: (0.10212044562899333, 0.2466477536140604), VL auc: 0.985844526974651 VL ap: 0.9204095730455343 VL f1: 0.8617021276595744 \n",
      "Epoch: 184, TR loss: 0.12255210334975755, VAL loss: (0.10212968055679462, 0.24234150825662815), VL auc: 0.9881353186861501 VL ap: 0.9209394236846687 VL f1: 0.8510638297872339 \n",
      "Epoch: 186, TR loss: 0.12298548134062959, VAL loss: (0.10198578826754945, 0.23328943455472906), VL auc: 0.9865155669709487 VL ap: 0.9125274506998434 VL f1: 0.8510638297872339 \n",
      "Epoch: 188, TR loss: 0.12279799914628457, VAL loss: (0.10212633673298158, 0.23408102481923204), VL auc: 0.9889741186815221 VL ap: 0.9149231231256519 VL f1: 0.8617021276595744 \n",
      "Epoch: 190, TR loss: 0.12285665921936184, VAL loss: (0.10198785430261012, 0.23845976971565408), VL auc: 0.9861945090416855 VL ap: 0.9096683816141599 VL f1: 0.8510638297872339 \n",
      "Epoch: 192, TR loss: 0.12229120270387833, VAL loss: (0.10221361634263272, 0.24516856416742852), VL auc: 0.991039302118404 VL ap: 0.9223701714346579 VL f1: 0.8510638297872339 \n",
      "Epoch: 194, TR loss: 0.12224709188243775, VAL loss: (0.10195275659856749, 0.2428811661740567), VL auc: 0.9872213159325721 VL ap: 0.917273025457942 VL f1: 0.8617021276595744 \n",
      "Epoch: 196, TR loss: 0.12289097842432621, VAL loss: (0.10187076238784666, 0.2326365125940201), VL auc: 0.9852949683569934 VL ap: 0.9096243749983679 VL f1: 0.8510638297872339 \n",
      "Epoch: 198, TR loss: 0.12355287227920032, VAL loss: (0.10200079813672512, 0.2258517691429625), VL auc: 0.9868684414517602 VL ap: 0.9130800259499918 VL f1: 0.8510638297872339 \n",
      "Epoch: 200, TR loss: 0.12250620748152992, VAL loss: (0.10188006369428443, 0.23358990283722572), VL auc: 0.9857751090439995 VL ap: 0.9116426483778559 VL f1: 0.8510638297872339 \n",
      "Epoch: 202, TR loss: 0.12213877912718352, VAL loss: (0.10198009629947068, 0.2400638093339636), VL auc: 0.987875001446207 VL ap: 0.914380989568765 VL f1: 0.8510638297872339 \n",
      "Epoch: 204, TR loss: 0.12306443418408514, VAL loss: (0.10189750468102145, 0.22592049456657248), VL auc: 0.9882625848923444 VL ap: 0.8977233157132921 VL f1: 0.8404255319148938 \n",
      "Epoch: 206, TR loss: 0.12330048608243085, VAL loss: (0.10200459831366401, 0.22745875094799287), VL auc: 0.9864577186954058 VL ap: 0.9110232244029355 VL f1: 0.8404255319148938 \n",
      "Epoch: 208, TR loss: 0.12333841272504435, VAL loss: (0.10205341980080036, 0.23256259268902718), VL auc: 0.9878923559288698 VL ap: 0.9195347899011721 VL f1: 0.8617021276595744 \n",
      "Epoch: 210, TR loss: 0.12257302350059655, VAL loss: (0.10196415712938417, 0.23587991836223196), VL auc: 0.9857635393888908 VL ap: 0.9143599166273289 VL f1: 0.8510638297872339 \n",
      "Epoch: 212, TR loss: 0.12278766230631272, VAL loss: (0.10192101931299705, 0.2380871874220828), VL auc: 0.986480858005623 VL ap: 0.9086771349812282 VL f1: 0.8510638297872339 \n",
      "Epoch: 214, TR loss: 0.12232880071863146, VAL loss: (0.10208878301938894, 0.24000660916592212), VL auc: 0.9866081242118173 VL ap: 0.912137234754076 VL f1: 0.8510638297872339 \n",
      "Epoch: 216, TR loss: 0.12258059687901524, VAL loss: (0.10211743369836274, 0.24401262973217255), VL auc: 0.991120289704164 VL ap: 0.9195768218163046 VL f1: 0.8510638297872339 \n",
      "Epoch: 218, TR loss: 0.12212691118013491, VAL loss: (0.10214866484682997, 0.24646872662483377), VL auc: 0.9874527090347437 VL ap: 0.914288613255636 VL f1: 0.8510638297872339 \n",
      "Epoch: 220, TR loss: 0.12255708504936827, VAL loss: (0.10199289907898314, 0.24300784253059549), VL auc: 0.9869928152441776 VL ap: 0.9130942223800076 VL f1: 0.8510638297872339 \n",
      "Epoch: 222, TR loss: 0.12283493243650193, VAL loss: (0.10198890806346435, 0.23886325511526554), VL auc: 0.9869725683477375 VL ap: 0.9202787922999482 VL f1: 0.8510638297872339 \n",
      "Epoch: 224, TR loss: 0.12282843456744448, VAL loss: (0.10199644203870564, 0.23396877532309673), VL auc: 0.990026957296403 VL ap: 0.9134597980209433 VL f1: 0.8510638297872339 \n",
      "Epoch: 226, TR loss: 0.12266485258112686, VAL loss: (0.10252527233826977, 0.2384571927659055), VL auc: 0.9812629435516527 VL ap: 0.9096467213504623 VL f1: 0.8510638297872339 \n",
      "Epoch: 228, TR loss: 0.12310199485476321, VAL loss: (0.10194844198719583, 0.23556991333657123), VL auc: 0.9877014566195782 VL ap: 0.910935663211988 VL f1: 0.8510638297872339 \n",
      "Epoch: 230, TR loss: 0.12265898956134515, VAL loss: (0.10181419119490043, 0.23764900451010845), VL auc: 0.9858503118022052 VL ap: 0.916255333877461 VL f1: 0.8510638297872339 \n",
      "Epoch: 232, TR loss: 0.12237433808373868, VAL loss: (0.10213706518010383, 0.24080300838389296), VL auc: 0.9888005738548934 VL ap: 0.9120252847281677 VL f1: 0.8617021276595744 \n",
      "Epoch: 234, TR loss: 0.12278314367323254, VAL loss: (0.10202637880376139, 0.2423651472051093), VL auc: 0.9878981407564241 VL ap: 0.91905806571722 VL f1: 0.8617021276595744 \n",
      "Epoch: 236, TR loss: 0.1230384277702253, VAL loss: (0.10181561003825533, 0.23116385683100274), VL auc: 0.9858358497333195 VL ap: 0.9102632199730115 VL f1: 0.8510638297872339 \n",
      "Epoch: 238, TR loss: 0.12303773317042951, VAL loss: (0.1019902356361941, 0.23486636547332115), VL auc: 0.9881700276514758 VL ap: 0.9166134045327634 VL f1: 0.8617021276595744 \n",
      "Epoch: 240, TR loss: 0.12267436038263277, VAL loss: (0.10222781307351142, 0.24415815637466756), VL auc: 0.9881295338585957 VL ap: 0.922173223233456 VL f1: 0.8617021276595744 \n",
      "Epoch: 242, TR loss: 0.12294981774896369, VAL loss: (0.10182736735424314, 0.23168371078815866), VL auc: 0.9884303448914188 VL ap: 0.9123365427563879 VL f1: 0.8510638297872339 \n",
      "Epoch: 244, TR loss: 0.122941377988004, VAL loss: (0.10182655421594616, 0.23163982147866108), VL auc: 0.9860701352492682 VL ap: 0.9090486422393678 VL f1: 0.8510638297872339 \n",
      "Epoch: 246, TR loss: 0.12291230189117566, VAL loss: (0.10205577624239566, 0.2373036729528549), VL auc: 0.9863073131789941 VL ap: 0.9161515098485787 VL f1: 0.8510638297872339 \n",
      "Epoch: 248, TR loss: 0.12222302736048014, VAL loss: (0.10201568354595739, 0.24612004706200133), VL auc: 0.9870709104161605 VL ap: 0.9240672599452118 VL f1: 0.8510638297872339 \n",
      "Epoch: 250, TR loss: 0.12251626050653144, VAL loss: (0.10185362840230339, 0.23808803964168468), VL auc: 0.9869899228304005 VL ap: 0.9160709858149971 VL f1: 0.8510638297872339 \n",
      "Epoch: 252, TR loss: 0.12262095835532152, VAL loss: (0.10183987142989141, 0.23344910398442695), VL auc: 0.9866081242118172 VL ap: 0.9055655198014176 VL f1: 0.8510638297872339 \n",
      "Epoch: 254, TR loss: 0.12223131027632464, VAL loss: (0.10178298493842186, 0.23443627864756483), VL auc: 0.983640507676466 VL ap: 0.8945702719875394 VL f1: 0.8297872340425532 \n",
      "Epoch: 256, TR loss: 0.1228143259758933, VAL loss: (0.10196185047176624, 0.23729543483003657), VL auc: 0.9855726400795993 VL ap: 0.9078259183009995 VL f1: 0.8510638297872339 \n",
      "Epoch: 258, TR loss: 0.1228600201861157, VAL loss: (0.10199311480955173, 0.2372668854733731), VL auc: 0.9865907697291544 VL ap: 0.8991449709484046 VL f1: 0.8404255319148938 \n",
      "Epoch: 260, TR loss: 0.12325771964769182, VAL loss: (0.1021191180562636, 0.24068966317684093), VL auc: 0.9874180000694179 VL ap: 0.9093749894657162 VL f1: 0.8404255319148938 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 262, TR loss: 0.12315193135191049, VAL loss: (0.10195669783010892, 0.23122791533774517), VL auc: 0.9870824800712692 VL ap: 0.9050456746712777 VL f1: 0.8404255319148938 \n",
      "Epoch: 264, TR loss: 0.12265608419230681, VAL loss: (0.10191016640593138, 0.23704583594139586), VL auc: 0.9869957076579547 VL ap: 0.90989706305739 VL f1: 0.8510638297872339 \n",
      "Epoch: 266, TR loss: 0.12276673468665872, VAL loss: (0.10185442494594124, 0.23424343352622173), VL auc: 0.9877072414471325 VL ap: 0.9159075707026904 VL f1: 0.8510638297872339 \n",
      "Epoch: 268, TR loss: 0.12272783709809411, VAL loss: (0.10196571702734163, 0.24054009863670836), VL auc: 0.989905475917763 VL ap: 0.9240122479377177 VL f1: 0.8617021276595744 \n",
      "Epoch: 270, TR loss: 0.12295007915748898, VAL loss: (0.1019193515497553, 0.23200246121021026), VL auc: 0.9864750731780686 VL ap: 0.9206616162386794 VL f1: 0.8510638297872339 \n",
      "Epoch: 272, TR loss: 0.12279235272213809, VAL loss: (0.10200312968633174, 0.2414462312738946), VL auc: 0.988372496615876 VL ap: 0.9236709512466473 VL f1: 0.8617021276595744 \n",
      "Epoch: 274, TR loss: 0.12328870776116235, VAL loss: (0.10199557911643131, 0.2417424587493247), VL auc: 0.9862089711105713 VL ap: 0.9239325052370451 VL f1: 0.8723404255319149 \n",
      "Epoch: 276, TR loss: 0.1221318704733006, VAL loss: (0.1020142149186251, 0.2463602309531354), VL auc: 0.987143220760589 VL ap: 0.9240166392843391 VL f1: 0.8617021276595744 \n",
      "Epoch: 278, TR loss: 0.12264851081388813, VAL loss: (0.10184363012018251, 0.240684042585657), VL auc: 0.9865792000740458 VL ap: 0.9183020872337013 VL f1: 0.8617021276595744 \n",
      "Epoch: 280, TR loss: 0.12314922764087739, VAL loss: (0.10189873268579645, 0.232334055799119), VL auc: 0.9853123228396563 VL ap: 0.916481438314884 VL f1: 0.8617021276595744 \n",
      "Epoch: 282, TR loss: 0.12312013660641902, VAL loss: (0.10232950514460984, 0.24148780741590134), VL auc: 0.9878605393773212 VL ap: 0.9230208887678991 VL f1: 0.8723404255319149 \n",
      "Epoch: 284, TR loss: 0.12242908449775147, VAL loss: (0.10186405814556145, 0.23360528337194564), VL auc: 0.9873543669663207 VL ap: 0.9159342182413363 VL f1: 0.8617021276595744 \n",
      "Epoch: 286, TR loss: 0.1227030705075257, VAL loss: (0.10193831094780197, 0.24735454802817486), VL auc: 0.9861279835248111 VL ap: 0.9225639773184015 VL f1: 0.8617021276595744 \n",
      "Epoch: 288, TR loss: 0.12232129455954785, VAL loss: (0.10194552132719038, 0.24593815904982547), VL auc: 0.9871634676570292 VL ap: 0.9220026333975171 VL f1: 0.8723404255319149 \n",
      "Epoch: 290, TR loss: 0.12324840603537614, VAL loss: (0.10177341811743815, 0.22574244154260514), VL auc: 0.9878518621359899 VL ap: 0.9083446289655941 VL f1: 0.8510638297872339 \n",
      "Epoch: 292, TR loss: 0.12280566215048336, VAL loss: (0.10195938616488666, 0.23833845016804148), VL auc: 0.9891245241979336 VL ap: 0.9203992389599724 VL f1: 0.8617021276595744 \n",
      "Epoch: 294, TR loss: 0.12309312937134805, VAL loss: (0.10199333054012032, 0.24615703745091216), VL auc: 0.9866196938669257 VL ap: 0.9208603116593531 VL f1: 0.8723404255319149 \n",
      "Epoch: 296, TR loss: 0.12388348684436949, VAL loss: (0.10197076180371466, 0.2270328034745886), VL auc: 0.9862263255932342 VL ap: 0.920849541868171 VL f1: 0.8510638297872339 \n",
      "Epoch: 298, TR loss: 0.12254703202436674, VAL loss: (0.10200180211360199, 0.2496392473261407), VL auc: 0.9875712979996066 VL ap: 0.9259960227265113 VL f1: 0.8617021276595744 \n",
      "Epoch: 300, TR loss: 0.12241339251741847, VAL loss: (0.10206477054763968, 0.247903215124252), VL auc: 0.9884939779945161 VL ap: 0.9250704911912144 VL f1: 0.8617021276595744 \n",
      "Stopping at epoch 245, TR loss: 0.12196810923542278, VAL loss: 0.10188099299519525, VAL auc: 0.9862726042136685,TS loss: 0.10188099299519525, TS auc: 0.9862726042136685 TS ap: 0.9102501029119184 TS f1: 0.8510638297872339\n",
      "Final training run 2: 0.9862726042136685, (0.9862726042136685, 0.9102501029119184, 0.8510638297872339)\n",
      "Normal Cls: 0\n",
      "Epoch: 1, TR loss: 0.8201765675668441, VAL loss: (0.6935760348694943, 0.8183279646203873), VL auc: 0.8705008503696504 VL ap: 0.5670886922788511 VL f1: 0.5425531914893617 \n",
      "Epoch: 2, TR loss: 0.6136836100021414, VAL loss: (0.5026470472658374, 0.6983597938050615), VL auc: 0.930923374174216 VL ap: 0.43780929950197367 VL f1: 0.5212765957446809 \n",
      "Epoch: 4, TR loss: 0.2817774477159049, VAL loss: (0.22872435262243407, 0.554479680162795), VL auc: 0.9514479423368389 VL ap: 0.5226204049137185 VL f1: 0.5851063829787234 \n",
      "Epoch: 6, TR loss: 0.16602986056140634, VAL loss: (0.1356085092234443, 0.40834568916483127), VL auc: 0.9598301574630062 VL ap: 0.6149731801399672 VL f1: 0.6063829787234043 \n",
      "Epoch: 8, TR loss: 0.14199038900934594, VAL loss: (0.1198220929741665, 0.3401455777756711), VL auc: 0.9616118843497276 VL ap: 0.6722157982022159 VL f1: 0.6276595744680851 \n",
      "Epoch: 10, TR loss: 0.13466210722398128, VAL loss: (0.11342294349691154, 0.29363358274419255), VL auc: 0.9602235257366977 VL ap: 0.6657874406874988 VL f1: 0.6276595744680851 \n",
      "Epoch: 12, TR loss: 0.1312837530687867, VAL loss: (0.11028184003969974, 0.2667809141443131), VL auc: 0.9607557298716926 VL ap: 0.6726101306540141 VL f1: 0.6276595744680851 \n",
      "Epoch: 14, TR loss: 0.1293349749191217, VAL loss: (0.10828478897700432, 0.24379375133108586), VL auc: 0.9603218678051207 VL ap: 0.6697054175171859 VL f1: 0.6276595744680851 \n",
      "Epoch: 16, TR loss: 0.12791122458929882, VAL loss: (0.10711619308164678, 0.23172597682222407), VL auc: 0.9622077215878195 VL ap: 0.6766091934575105 VL f1: 0.6276595744680851 \n",
      "Epoch: 18, TR loss: 0.12706330495900636, VAL loss: (0.10635168543323477, 0.2241553042797332), VL auc: 0.9673909270764639 VL ap: 0.7053878703299612 VL f1: 0.6595744680851063 \n",
      "Epoch: 20, TR loss: 0.1263694669823406, VAL loss: (0.10561396156464536, 0.22171850407377203), VL auc: 0.9688776277579165 VL ap: 0.7232725596240803 VL f1: 0.6595744680851063 \n",
      "Epoch: 22, TR loss: 0.12546605405653452, VAL loss: (0.10512561393600887, 0.22264064626490815), VL auc: 0.9709717353325698 VL ap: 0.7473496734767338 VL f1: 0.6595744680851063 \n",
      "Epoch: 24, TR loss: 0.12508741501086026, VAL loss: (0.1047392985689828, 0.21721756711919257), VL auc: 0.9724352967038052 VL ap: 0.7562361932623635 VL f1: 0.6914893617021277 \n",
      "Epoch: 26, TR loss: 0.124947680950865, VAL loss: (0.10445915583101804, 0.2123000571068297), VL auc: 0.9737310980759664 VL ap: 0.7558162700230266 VL f1: 0.6914893617021277 \n",
      "Epoch: 28, TR loss: 0.12474927934897745, VAL loss: (0.10435772097713601, 0.23174220957654587), VL auc: 0.9759351173741511 VL ap: 0.7887120323211912 VL f1: 0.723404255319149 \n",
      "Epoch: 30, TR loss: 0.12439543676913165, VAL loss: (0.10402899737459217, 0.22005103496795006), VL auc: 0.9755764580657852 VL ap: 0.7885989253954654 VL f1: 0.723404255319149 \n",
      "Epoch: 32, TR loss: 0.1240596239087165, VAL loss: (0.1038556910520536, 0.21812816376381733), VL auc: 0.9749516966899218 VL ap: 0.7891999648553306 VL f1: 0.723404255319149 \n",
      "Epoch: 34, TR loss: 0.1243152739776447, VAL loss: (0.10375967435437483, 0.21407486530060463), VL auc: 0.9774160332280495 VL ap: 0.7928565917308398 VL f1: 0.723404255319149 \n",
      "Epoch: 36, TR loss: 0.1242110765394602, VAL loss: (0.10357666015625, 0.2165599376597303), VL auc: 0.9753277104809506 VL ap: 0.7986156465272867 VL f1: 0.723404255319149 \n",
      "Epoch: 38, TR loss: 0.1237037946240784, VAL loss: (0.10352145802268131, 0.22673533825164147), VL auc: 0.9777573380537528 VL ap: 0.8181053736173858 VL f1: 0.7340425531914893 \n",
      "Epoch: 40, TR loss: 0.12329840228304347, VAL loss: (0.10345824896608635, 0.22670461776408743), VL auc: 0.9794175835618341 VL ap: 0.8239413891776626 VL f1: 0.7340425531914893 \n",
      "Epoch: 42, TR loss: 0.1241448953696693, VAL loss: (0.10328931533622639, 0.22387196155304603), VL auc: 0.9786944801175478 VL ap: 0.8228255369881211 VL f1: 0.7340425531914893 \n",
      "Epoch: 44, TR loss: 0.12332871073434823, VAL loss: (0.10331041544529976, 0.23098864453904172), VL auc: 0.9803200166603034 VL ap: 0.8427238990220914 VL f1: 0.7446808510638298 \n",
      "Epoch: 46, TR loss: 0.12325712214249113, VAL loss: (0.10324691598217017, 0.22822992852393617), VL auc: 0.9812050952761098 VL ap: 0.8423451670551687 VL f1: 0.7553191489361702 \n",
      "Epoch: 48, TR loss: 0.12326765317165321, VAL loss: (0.10315517240921526, 0.22855377197265625), VL auc: 0.9811183228627954 VL ap: 0.8508033685244216 VL f1: 0.7553191489361702 \n",
      "Epoch: 50, TR loss: 0.12351783606799513, VAL loss: (0.10307158511121874, 0.22433696909153716), VL auc: 0.9793915518378397 VL ap: 0.8422414122463838 VL f1: 0.7553191489361702 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52, TR loss: 0.12322172742816553, VAL loss: (0.1030381883597361, 0.23135589031462975), VL auc: 0.9817835780315387 VL ap: 0.8551447349934831 VL f1: 0.7659574468085105 \n",
      "Epoch: 54, TR loss: 0.12417735483969652, VAL loss: (0.1030411588037189, 0.21680831909179688), VL auc: 0.9797241794222113 VL ap: 0.8373446495668811 VL f1: 0.7446808510638298 \n",
      "Epoch: 56, TR loss: 0.12316698101415274, VAL loss: (0.1029814429228691, 0.2293979157792761), VL auc: 0.9798398759732975 VL ap: 0.8537433843266657 VL f1: 0.7553191489361702 \n",
      "Epoch: 58, TR loss: 0.12313373731854962, VAL loss: (0.10290792858295948, 0.2289992190421896), VL auc: 0.9795853435609087 VL ap: 0.8542839425917833 VL f1: 0.7659574468085105 \n",
      "Epoch: 60, TR loss: 0.12301682795722053, VAL loss: (0.10300505712279942, 0.2320107196239715), VL auc: 0.9826917959575626 VL ap: 0.8645353493431932 VL f1: 0.7872340425531915 \n",
      "Epoch: 62, TR loss: 0.12376937081985362, VAL loss: (0.10285831884951485, 0.22755933315195936), VL auc: 0.98124558906899 VL ap: 0.8558969497635384 VL f1: 0.7659574468085105 \n",
      "Epoch: 64, TR loss: 0.12304553808211346, VAL loss: (0.10279085326285516, 0.22538430640038024), VL auc: 0.9811356773454585 VL ap: 0.8558766875050817 VL f1: 0.7659574468085105 \n",
      "Epoch: 66, TR loss: 0.12300436250497124, VAL loss: (0.10277219256867268, 0.23453083444148937), VL auc: 0.9832529242303287 VL ap: 0.8680954270364634 VL f1: 0.776595744680851 \n",
      "Epoch: 68, TR loss: 0.12313824101399978, VAL loss: (0.10267786852622435, 0.22748044196595538), VL auc: 0.9806844607962236 VL ap: 0.8547009717644806 VL f1: 0.7659574468085105 \n",
      "Epoch: 70, TR loss: 0.12365730125065008, VAL loss: (0.10256745596175741, 0.2232342983813996), VL auc: 0.9803489407980749 VL ap: 0.8568913058980033 VL f1: 0.7659574468085105 \n",
      "Epoch: 72, TR loss: 0.12330256241300325, VAL loss: (0.10263996632479014, 0.2291590913813165), VL auc: 0.9820265407888191 VL ap: 0.8683182100961696 VL f1: 0.7872340425531915 \n",
      "Epoch: 74, TR loss: 0.12326436689304944, VAL loss: (0.10260036317079595, 0.22200959794064787), VL auc: 0.9822347945807736 VL ap: 0.8598917159602465 VL f1: 0.7659574468085105 \n",
      "Epoch: 76, TR loss: 0.12318509289054852, VAL loss: (0.10270747339809765, 0.22971591543644032), VL auc: 0.983536380780489 VL ap: 0.8751580120537678 VL f1: 0.8085106382978723 \n",
      "Epoch: 78, TR loss: 0.12331893405550202, VAL loss: (0.10247830945295507, 0.21868206592316322), VL auc: 0.9803952194185093 VL ap: 0.8610684280650795 VL f1: 0.776595744680851 \n",
      "Epoch: 80, TR loss: 0.12393151132487457, VAL loss: (0.10259599047811735, 0.21645428272003822), VL auc: 0.9811761711383384 VL ap: 0.858282578475746 VL f1: 0.7659574468085105 \n",
      "Epoch: 82, TR loss: 0.1236686463806481, VAL loss: (0.10254555101171663, 0.22049792269442944), VL auc: 0.9798456608008517 VL ap: 0.8620968939333963 VL f1: 0.776595744680851 \n",
      "Epoch: 84, TR loss: 0.12313939868032611, VAL loss: (0.10257004472858042, 0.2303045962719207), VL auc: 0.9834669628498374 VL ap: 0.8811787377769312 VL f1: 0.8085106382978723 \n",
      "Epoch: 86, TR loss: 0.12256048336019716, VAL loss: (0.1025096069800588, 0.23263308342466962), VL auc: 0.9834206842294031 VL ap: 0.8837293560558894 VL f1: 0.8085106382978723 \n",
      "Epoch: 88, TR loss: 0.12263260223791989, VAL loss: (0.10265430411027138, 0.2376646894089719), VL auc: 0.9833888676778545 VL ap: 0.8882458006773831 VL f1: 0.8191489361702128 \n",
      "Epoch: 90, TR loss: 0.12296975948503656, VAL loss: (0.1024679460883335, 0.23055289654021568), VL auc: 0.9827843531984313 VL ap: 0.8838829290862767 VL f1: 0.7978723404255319 \n",
      "Epoch: 92, TR loss: 0.1226076862710513, VAL loss: (0.10253585143345993, 0.23522646883700757), VL auc: 0.9835971214698092 VL ap: 0.8872674569921105 VL f1: 0.8191489361702128 \n",
      "Epoch: 94, TR loss: 0.1228959451863069, VAL loss: (0.10248726227155129, 0.22512026036039312), VL auc: 0.9816100332049102 VL ap: 0.8676443918640198 VL f1: 0.7872340425531915 \n",
      "Epoch: 96, TR loss: 0.1230906497247652, VAL loss: (0.10243039237474086, 0.23043985569730718), VL auc: 0.9836115835386947 VL ap: 0.8866207991439042 VL f1: 0.8191489361702128 \n",
      "Epoch: 98, TR loss: 0.12274748008156663, VAL loss: (0.10238098177720568, 0.23043811067621758), VL auc: 0.9834958869876089 VL ap: 0.8843847455333467 VL f1: 0.8191489361702128 \n",
      "Epoch: 100, TR loss: 0.12318933517747338, VAL loss: (0.10236027164262167, 0.22041430371872922), VL auc: 0.9824372635451737 VL ap: 0.8707088881175009 VL f1: 0.7978723404255319 \n",
      "Epoch: 102, TR loss: 0.12274548590795935, VAL loss: (0.10232280090232464, 0.2262796442559425), VL auc: 0.9832008607823401 VL ap: 0.8747146245718177 VL f1: 0.7978723404255319 \n",
      "Epoch: 104, TR loss: 0.12299975424611095, VAL loss: (0.10228006135775643, 0.22706021653844954), VL auc: 0.9823273518216421 VL ap: 0.8787734836529317 VL f1: 0.8085106382978723 \n",
      "Epoch: 106, TR loss: 0.12280210699453928, VAL loss: (0.10240248015809798, 0.23118798276211353), VL auc: 0.9826050235442483 VL ap: 0.8799420392587582 VL f1: 0.8191489361702128 \n",
      "Epoch: 108, TR loss: 0.12292930091413516, VAL loss: (0.10245084529210849, 0.23043257124880526), VL auc: 0.9850115118068331 VL ap: 0.879941362616904 VL f1: 0.8191489361702128 \n",
      "Epoch: 110, TR loss: 0.1227103899462341, VAL loss: (0.10232575475164832, 0.2338133467004654), VL auc: 0.9833223421609802 VL ap: 0.889592347845643 VL f1: 0.8191489361702128 \n",
      "Epoch: 112, TR loss: 0.12239158357759346, VAL loss: (0.10232317428215487, 0.23357381211950423), VL auc: 0.9826570869922369 VL ap: 0.8845536648887086 VL f1: 0.8085106382978723 \n",
      "Epoch: 114, TR loss: 0.12277540598088366, VAL loss: (0.10244385064328865, 0.23450275177651264), VL auc: 0.9845487256024898 VL ap: 0.8824105015776499 VL f1: 0.8191489361702128 \n",
      "Epoch: 116, TR loss: 0.12253699393699523, VAL loss: (0.10220715272290477, 0.22463118776361993), VL auc: 0.9831083035414714 VL ap: 0.8737566201336391 VL f1: 0.7978723404255319 \n",
      "Epoch: 118, TR loss: 0.12302894237516443, VAL loss: (0.1022133010441094, 0.22308589042501248), VL auc: 0.9829289738872885 VL ap: 0.87317720750628 VL f1: 0.8085106382978723 \n",
      "Epoch: 120, TR loss: 0.12247374054268768, VAL loss: (0.10230932603911773, 0.2313788596619951), VL auc: 0.9827322897504426 VL ap: 0.8821031023614896 VL f1: 0.8191489361702128 \n",
      "Epoch: 122, TR loss: 0.123165464844706, VAL loss: (0.10234229962679275, 0.23033856331033908), VL auc: 0.9838082676755405 VL ap: 0.8910693265323087 VL f1: 0.8191489361702128 \n",
      "Epoch: 124, TR loss: 0.12283426024315115, VAL loss: (0.10222883364504741, 0.22579566468583775), VL auc: 0.9844098897411868 VL ap: 0.8772006477146743 VL f1: 0.8085106382978723 \n",
      "Epoch: 126, TR loss: 0.12262526786158147, VAL loss: (0.10222348186748063, 0.2281440572535738), VL auc: 0.9825934538891397 VL ap: 0.8789228480225642 VL f1: 0.8085106382978723 \n",
      "Epoch: 128, TR loss: 0.12240258514210108, VAL loss: (0.10223748775977944, 0.2285946988044901), VL auc: 0.9822926428563166 VL ap: 0.8731979669543891 VL f1: 0.8085106382978723 \n",
      "Epoch: 130, TR loss: 0.12281581973889501, VAL loss: (0.10237719819492591, 0.23492212498441656), VL auc: 0.9834438235396203 VL ap: 0.8913076653611471 VL f1: 0.8191489361702128 \n",
      "Epoch: 132, TR loss: 0.1228520883045766, VAL loss: (0.10230852949547987, 0.23482525602300117), VL auc: 0.9827438594055513 VL ap: 0.8864793817339212 VL f1: 0.8191489361702128 \n",
      "Epoch: 134, TR loss: 0.12331524446088779, VAL loss: (0.10220468841602519, 0.22514568491184966), VL auc: 0.9823967697522937 VL ap: 0.8793381949820646 VL f1: 0.8085106382978723 \n",
      "Epoch: 136, TR loss: 0.12281036003512374, VAL loss: (0.10251997864200993, 0.24268604846710853), VL auc: 0.9854714055973992 VL ap: 0.8980479013055078 VL f1: 0.8404255319148938 \n",
      "Epoch: 138, TR loss: 0.12281257827318129, VAL loss: (0.10226549954437704, 0.23782478494847076), VL auc: 0.9844041049136326 VL ap: 0.8941849057860498 VL f1: 0.8297872340425532 \n",
      "Epoch: 140, TR loss: 0.12315050480824385, VAL loss: (0.10220929343393148, 0.23078319874215633), VL auc: 0.9830562400934829 VL ap: 0.881992384780653 VL f1: 0.8191489361702128 \n",
      "Epoch: 142, TR loss: 0.12327202242843321, VAL loss: (0.10207216346827845, 0.2223356328112014), VL auc: 0.9831198731965801 VL ap: 0.8848029776617089 VL f1: 0.8191489361702128 \n",
      "Epoch: 144, TR loss: 0.12250307804804134, VAL loss: (0.10223611040307232, 0.23073062490909657), VL auc: 0.9838892552613008 VL ap: 0.8860096754477457 VL f1: 0.8191489361702128 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 146, TR loss: 0.12329082143580977, VAL loss: (0.10215401662439676, 0.2235350507370969), VL auc: 0.9840223062950495 VL ap: 0.8795816803447695 VL f1: 0.8191489361702128 \n",
      "Epoch: 148, TR loss: 0.12271832182777319, VAL loss: (0.10216469528754163, 0.22546167576566656), VL auc: 0.9847569793944444 VL ap: 0.8826892265608566 VL f1: 0.8297872340425532 \n",
      "Epoch: 150, TR loss: 0.12291269773837112, VAL loss: (0.10207579769862613, 0.2270416097438082), VL auc: 0.9828769104392999 VL ap: 0.8819078452608875 VL f1: 0.8085106382978723 \n",
      "Epoch: 152, TR loss: 0.12315455290597849, VAL loss: (0.10211866170313774, 0.22467073481133643), VL auc: 0.9824546180278367 VL ap: 0.881214965931181 VL f1: 0.8191489361702128 \n",
      "Epoch: 154, TR loss: 0.1232926737019319, VAL loss: (0.10231681852771123, 0.23122328900276345), VL auc: 0.9842247752594495 VL ap: 0.8903929157677792 VL f1: 0.8297872340425532 \n",
      "Epoch: 156, TR loss: 0.12419419701754084, VAL loss: (0.10218328960308762, 0.2185176078309404), VL auc: 0.9824546180278366 VL ap: 0.8724690761682112 VL f1: 0.8191489361702128 \n",
      "Epoch: 158, TR loss: 0.12244700218495702, VAL loss: (0.10234292192650982, 0.24093337769203998), VL auc: 0.9843636111207525 VL ap: 0.8980137991148596 VL f1: 0.8404255319148938 \n",
      "Epoch: 160, TR loss: 0.12312086855028986, VAL loss: (0.10217915753296629, 0.23009490966796875), VL auc: 0.9833165573334259 VL ap: 0.884615368293552 VL f1: 0.8191489361702128 \n",
      "Epoch: 162, TR loss: 0.12242997328673749, VAL loss: (0.10244460570027869, 0.2500852219601895), VL auc: 0.9853065380121018 VL ap: 0.9045188260468754 VL f1: 0.8404255319148938 \n",
      "Epoch: 164, TR loss: 0.12306282838885829, VAL loss: (0.10223759562506372, 0.2409100633986453), VL auc: 0.9840570152603751 VL ap: 0.8953658755722347 VL f1: 0.8404255319148938 \n",
      "Epoch: 166, TR loss: 0.12315414958996804, VAL loss: (0.10207252855077913, 0.23142715210610248), VL auc: 0.9842565918109983 VL ap: 0.8852843968529651 VL f1: 0.8297872340425532 \n",
      "Epoch: 168, TR loss: 0.12300254011410916, VAL loss: (0.10207232111754011, 0.22934244034138132), VL auc: 0.9839644580195065 VL ap: 0.8890428381203729 VL f1: 0.8191489361702128 \n",
      "Epoch: 170, TR loss: 0.12276737700474945, VAL loss: (0.1021330160832781, 0.23453121996940451), VL auc: 0.9837272800897805 VL ap: 0.8924420929058676 VL f1: 0.8297872340425532 \n",
      "Epoch: 172, TR loss: 0.12249173291804331, VAL loss: (0.1022357121312534, 0.2383878586140085), VL auc: 0.9845602952575984 VL ap: 0.8968603895779208 VL f1: 0.8297872340425532 \n",
      "Epoch: 174, TR loss: 0.1221698867416942, VAL loss: (0.1022758214223508, 0.24481856569330743), VL auc: 0.9851792718059076 VL ap: 0.9009328875656197 VL f1: 0.8404255319148938 \n",
      "Epoch: 176, TR loss: 0.12311460968331268, VAL loss: (0.1021279879015642, 0.23006272823252577), VL auc: 0.9823099973389795 VL ap: 0.884127964658681 VL f1: 0.8191489361702128 \n",
      "Epoch: 178, TR loss: 0.12313447673123547, VAL loss: (0.10210934380204086, 0.22889094657086312), VL auc: 0.9850982842201476 VL ap: 0.8873896627167183 VL f1: 0.8191489361702128 \n",
      "Epoch: 180, TR loss: 0.12204054180337585, VAL loss: (0.10227099237654635, 0.2408149597492624), VL auc: 0.9858850207675309 VL ap: 0.8972489320741188 VL f1: 0.8404255319148938 \n",
      "Epoch: 182, TR loss: 0.12277128319499893, VAL loss: (0.10223382034011351, 0.2421335666737658), VL auc: 0.9845602952575984 VL ap: 0.8939542346426351 VL f1: 0.8404255319148938 \n",
      "Epoch: 184, TR loss: 0.12225783203842006, VAL loss: (0.10222941445811667, 0.24494272597292635), VL auc: 0.9851040690477018 VL ap: 0.894054714411176 VL f1: 0.8404255319148938 \n",
      "Epoch: 186, TR loss: 0.12303415560804042, VAL loss: (0.10213889888993678, 0.2383368471835522), VL auc: 0.9863362373167657 VL ap: 0.8893967985754968 VL f1: 0.8297872340425532 \n",
      "Epoch: 188, TR loss: 0.12294155723956421, VAL loss: (0.10241432874471094, 0.23994593924664437), VL auc: 0.9847801187046614 VL ap: 0.8949361237211635 VL f1: 0.8297872340425532 \n",
      "Epoch: 190, TR loss: 0.12282587276389653, VAL loss: (0.10218226073422207, 0.23884728614320147), VL auc: 0.9848697835317529 VL ap: 0.8986281560574139 VL f1: 0.8297872340425532 \n",
      "Epoch: 192, TR loss: 0.1233639037906686, VAL loss: (0.10274873601800401, 0.25066266161330203), VL auc: 0.9857172607684567 VL ap: 0.8989089525095233 VL f1: 0.8510638297872339 \n",
      "Epoch: 194, TR loss: 0.12237285178955197, VAL loss: (0.10216679451192054, 0.23380096922529506), VL auc: 0.9826165931993568 VL ap: 0.8829798850509116 VL f1: 0.8191489361702128 \n",
      "Epoch: 196, TR loss: 0.12243535083354365, VAL loss: (0.10216192397946829, 0.23826434764456242), VL auc: 0.9837735587102148 VL ap: 0.8841444958986591 VL f1: 0.8191489361702128 \n",
      "Epoch: 198, TR loss: 0.12196770591941232, VAL loss: (0.10223699821733534, 0.24313027808006774), VL auc: 0.9834380387120661 VL ap: 0.8819919301698339 VL f1: 0.8297872340425532 \n",
      "Epoch: 200, TR loss: 0.12198287508269472, VAL loss: (0.10216106935452351, 0.23515930581600109), VL auc: 0.9832066456098945 VL ap: 0.879297056002169 VL f1: 0.8191489361702128 \n",
      "Epoch: 202, TR loss: 0.12292470759290489, VAL loss: (0.10219849860817275, 0.23864145481840093), VL auc: 0.9860469959390511 VL ap: 0.8911832007241459 VL f1: 0.8297872340425532 \n",
      "Epoch: 204, TR loss: 0.1223331998506715, VAL loss: (0.10220565090625425, 0.24004059649528342), VL auc: 0.9850288662894959 VL ap: 0.886709532015182 VL f1: 0.8191489361702128 \n",
      "Epoch: 206, TR loss: 0.12319141897686077, VAL loss: (0.10233901388428664, 0.2372058908990089), VL auc: 0.985069360082376 VL ap: 0.8879529409375472 VL f1: 0.8404255319148938 \n",
      "Epoch: 208, TR loss: 0.12263974989388307, VAL loss: (0.10221055462802474, 0.23603887760892828), VL auc: 0.9847164856015642 VL ap: 0.8863837179212563 VL f1: 0.8191489361702128 \n",
      "Epoch: 210, TR loss: 0.12237406920639837, VAL loss: (0.1021454122936421, 0.23743765404883851), VL auc: 0.9848784607730844 VL ap: 0.8906142278149476 VL f1: 0.8297872340425532 \n",
      "Epoch: 212, TR loss: 0.12260456430637773, VAL loss: (0.10205395082989227, 0.2334091714087953), VL auc: 0.9830417780245971 VL ap: 0.8858365387119822 VL f1: 0.8191489361702128 \n",
      "Epoch: 214, TR loss: 0.12254267770521675, VAL loss: (0.10222207961878484, 0.23618844214906085), VL auc: 0.9859833628359539 VL ap: 0.8916515712126561 VL f1: 0.8297872340425532 \n",
      "Epoch: 216, TR loss: 0.12341593155601827, VAL loss: (0.10220866283688486, 0.23731730846648522), VL auc: 0.9859139449053024 VL ap: 0.8993313044024371 VL f1: 0.8404255319148938 \n",
      "Epoch: 218, TR loss: 0.12354566487271705, VAL loss: (0.10224090625955852, 0.2270662226575486), VL auc: 0.9845660800851528 VL ap: 0.8882045919284383 VL f1: 0.8297872340425532 \n",
      "Epoch: 220, TR loss: 0.12210347403863803, VAL loss: (0.10218314025115552, 0.23675076504971118), VL auc: 0.9836636469866833 VL ap: 0.8789081846518213 VL f1: 0.8297872340425532 \n",
      "Epoch: 222, TR loss: 0.12207461453744493, VAL loss: (0.10215488784400065, 0.23668666596108295), VL auc: 0.9842652690523297 VL ap: 0.8824857673728554 VL f1: 0.8297872340425532 \n",
      "Epoch: 224, TR loss: 0.12215978143498761, VAL loss: (0.1021969801968631, 0.24419961077101687), VL auc: 0.9843751807758612 VL ap: 0.8963905220612584 VL f1: 0.8404255319148938 \n",
      "Epoch: 226, TR loss: 0.12268059684316493, VAL loss: (0.10203334026326298, 0.2306456464402219), VL auc: 0.9846846690500157 VL ap: 0.879913713225255 VL f1: 0.8191489361702128 \n",
      "Epoch: 228, TR loss: 0.12227748249070759, VAL loss: (0.10214001073209794, 0.24155570091085232), VL auc: 0.9851272083579189 VL ap: 0.889285204720505 VL f1: 0.8297872340425532 \n",
      "Epoch: 230, TR loss: 0.12239895529800691, VAL loss: (0.1022074431294394, 0.242320162184695), VL auc: 0.9860354262839426 VL ap: 0.8966169224121965 VL f1: 0.8404255319148938 \n",
      "Epoch: 232, TR loss: 0.12241676095298734, VAL loss: (0.10208082588034002, 0.2353454752171293), VL auc: 0.9847164856015642 VL ap: 0.8816302826005478 VL f1: 0.8191489361702128 \n",
      "Epoch: 234, TR loss: 0.12314860772923167, VAL loss: (0.10201396599873827, 0.2215135858414021), VL auc: 0.9820728194092535 VL ap: 0.8678603524317589 VL f1: 0.8085106382978723 \n",
      "Epoch: 236, TR loss: 0.12357359824084908, VAL loss: (0.10200026710763323, 0.222451554968002), VL auc: 0.9816649890666759 VL ap: 0.8828680495236328 VL f1: 0.8191489361702128 \n",
      "Epoch: 238, TR loss: 0.12262354256531449, VAL loss: (0.10225353479515022, 0.23637154761781085), VL auc: 0.9841987435354552 VL ap: 0.8793461747171061 VL f1: 0.8297872340425532 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240, TR loss: 0.12188832735350129, VAL loss: (0.10215817358650676, 0.24407072270170171), VL auc: 0.9844330290514041 VL ap: 0.8815647766471699 VL f1: 0.8404255319148938 \n",
      "Epoch: 242, TR loss: 0.12304254308729504, VAL loss: (0.10210885425959676, 0.23452367173864486), VL auc: 0.9845140166371641 VL ap: 0.8880128843248823 VL f1: 0.8297872340425532 \n",
      "Epoch: 244, TR loss: 0.12220594618055555, VAL loss: (0.10227552271848661, 0.24318169532938205), VL auc: 0.9852081959436789 VL ap: 0.8778430953327213 VL f1: 0.8297872340425532 \n",
      "Epoch: 246, TR loss: 0.12218309160662934, VAL loss: (0.10219138779673906, 0.24264826673142453), VL auc: 0.9870043848992862 VL ap: 0.8889356137079296 VL f1: 0.8297872340425532 \n",
      "Epoch: 248, TR loss: 0.12345916852610285, VAL loss: (0.10200614991429191, 0.22738671810068983), VL auc: 0.9838314069857578 VL ap: 0.8789816861703704 VL f1: 0.8191489361702128 \n",
      "Epoch: 250, TR loss: 0.12273393165114109, VAL loss: (0.10201391621476091, 0.2303494595466776), VL auc: 0.9839181793990722 VL ap: 0.8798483006293315 VL f1: 0.8191489361702128 \n",
      "Epoch: 252, TR loss: 0.12288237434943634, VAL loss: (0.1020846094626198, 0.23884010314941406), VL auc: 0.9847945807735472 VL ap: 0.8904452641247487 VL f1: 0.8404255319148938 \n",
      "Epoch: 254, TR loss: 0.12282869597596978, VAL loss: (0.10219417569947152, 0.24223469673319065), VL auc: 0.985387525597862 VL ap: 0.902137280322235 VL f1: 0.8404255319148938 \n",
      "Epoch: 256, TR loss: 0.12319121731885554, VAL loss: (0.10211372479204901, 0.22893552577241938), VL auc: 0.9829752525077227 VL ap: 0.8808002247310679 VL f1: 0.8297872340425532 \n",
      "Epoch: 258, TR loss: 0.12229728978811032, VAL loss: (0.1021194582467756, 0.23787392961218), VL auc: 0.985150347668136 VL ap: 0.8800379108735322 VL f1: 0.8297872340425532 \n",
      "Epoch: 260, TR loss: 0.12223534343642926, VAL loss: (0.10219813352567207, 0.24420999973378282), VL auc: 0.9863188828341027 VL ap: 0.8799988048696429 VL f1: 0.8297872340425532 \n",
      "Epoch: 262, TR loss: 0.12349195662399046, VAL loss: (0.10214520486040307, 0.23068470650530876), VL auc: 0.9835913366422548 VL ap: 0.8774407700539327 VL f1: 0.8191489361702128 \n",
      "Epoch: 264, TR loss: 0.1229310038039571, VAL loss: (0.10193066910727637, 0.22989297420420546), VL auc: 0.9845198014647184 VL ap: 0.8856014496464548 VL f1: 0.8297872340425532 \n",
      "Epoch: 266, TR loss: 0.12327062576002662, VAL loss: (0.10214610926932521, 0.2326255554848529), VL auc: 0.9856767669755766 VL ap: 0.8869440098972952 VL f1: 0.8297872340425532 \n",
      "Epoch: 268, TR loss: 0.12301608854453469, VAL loss: (0.10204295686822407, 0.2255616695322889), VL auc: 0.9847020235326785 VL ap: 0.8878076965044944 VL f1: 0.8297872340425532 \n",
      "Epoch: 270, TR loss: 0.12295292477600725, VAL loss: (0.10193066080994681, 0.23094838730832365), VL auc: 0.9831430125067974 VL ap: 0.8830628482193704 VL f1: 0.8191489361702128 \n",
      "Epoch: 272, TR loss: 0.12286161104371253, VAL loss: (0.102016994524028, 0.2296860268775453), VL auc: 0.9834091145742945 VL ap: 0.8903869603475949 VL f1: 0.8191489361702128 \n",
      "Epoch: 274, TR loss: 0.12189649076830565, VAL loss: (0.1024053427367965, 0.2541878882874834), VL auc: 0.9863796235234228 VL ap: 0.9014746133852781 VL f1: 0.8510638297872339 \n",
      "Epoch: 276, TR loss: 0.1225570178300332, VAL loss: (0.10197853640151322, 0.2252565140419818), VL auc: 0.9848235049113186 VL ap: 0.8843838485977856 VL f1: 0.8297872340425532 \n",
      "Epoch: 278, TR loss: 0.1224772434169267, VAL loss: (0.10196118668540137, 0.23357362950101812), VL auc: 0.9832240000925573 VL ap: 0.8871725052041374 VL f1: 0.8191489361702128 \n",
      "Epoch: 280, TR loss: 0.12211959174142652, VAL loss: (0.10215470530275031, 0.24403821661117228), VL auc: 0.9854569435285135 VL ap: 0.8944877689060179 VL f1: 0.8297872340425532 \n",
      "Epoch: 282, TR loss: 0.12325695035974593, VAL loss: (0.10233290704972982, 0.24593700246608002), VL auc: 0.9905765159140607 VL ap: 0.9044005228753318 VL f1: 0.8510638297872339 \n",
      "Epoch: 284, TR loss: 0.12276029656812133, VAL loss: (0.10222527409066579, 0.23912894472162774), VL auc: 0.988812143510002 VL ap: 0.8919385855590373 VL f1: 0.8404255319148938 \n",
      "Epoch: 286, TR loss: 0.12331477392554224, VAL loss: (0.1020380282544649, 0.22040754683474276), VL auc: 0.9866081242118173 VL ap: 0.880127769076727 VL f1: 0.8191489361702128 \n",
      "Epoch: 288, TR loss: 0.12272529023217618, VAL loss: (0.10208498284245004, 0.23695239614933095), VL auc: 0.9855986718035936 VL ap: 0.8955116303942449 VL f1: 0.8404255319148938 \n",
      "Epoch: 290, TR loss: 0.1225894623624304, VAL loss: (0.10217514992278837, 0.24515894626049287), VL auc: 0.987221315932572 VL ap: 0.9043226227034428 VL f1: 0.8510638297872339 \n",
      "Epoch: 292, TR loss: 0.12285439616841425, VAL loss: (0.10200490531485777, 0.23221409574468085), VL auc: 0.9851908414610162 VL ap: 0.8897696791937878 VL f1: 0.8297872340425532 \n",
      "Epoch: 294, TR loss: 0.1223472337540726, VAL loss: (0.1020252088802933, 0.2361809953730157), VL auc: 0.9841495725012437 VL ap: 0.8824226889472258 VL f1: 0.8297872340425532 \n",
      "Epoch: 296, TR loss: 0.12318934264628839, VAL loss: (0.10206435568116164, 0.23662475829428814), VL auc: 0.9854916524938392 VL ap: 0.8883907644777939 VL f1: 0.8404255319148938 \n",
      "Epoch: 298, TR loss: 0.12199755130418655, VAL loss: (0.10212992117935189, 0.24748694643061211), VL auc: 0.9849768028415072 VL ap: 0.890725213596185 VL f1: 0.8404255319148938 \n",
      "Epoch: 300, TR loss: 0.12226477056756302, VAL loss: (0.10204057553464009, 0.2367947355229804), VL auc: 0.9832876331956544 VL ap: 0.8810008218139126 VL f1: 0.8085106382978723 \n",
      "Stopping at epoch 240, TR loss: 0.12188832735350129, VAL loss: 0.10215817358650676, VAL auc: 0.9844330290514041,TS loss: 0.10215817358650676, TS auc: 0.9844330290514041 TS ap: 0.8815647766471699 TS f1: 0.8404255319148938\n",
      "Final training run 3: 0.9844330290514041, (0.9844330290514041, 0.8815647766471699, 0.8404255319148938)\n",
      "Normal Cls: 0\n",
      "Epoch: 1, TR loss: 0.8852742214268233, VAL loss: (0.7534680182419113, 0.7976704049617687), VL auc: 0.7619080675205072 VL ap: 0.2684465408774714 VL f1: 0.2553191489361702 \n",
      "Epoch: 2, TR loss: 0.6589344713656388, VAL loss: (0.5424067220848627, 0.6795526463934716), VL auc: 0.9079923177490079 VL ap: 0.3776838701463315 VL f1: 0.35106382978723405 \n",
      "Epoch: 4, TR loss: 0.32989603529009115, VAL loss: (0.2747727228675996, 0.5005358838020487), VL auc: 0.8949012529936483 VL ap: 0.42251007145093433 VL f1: 0.42553191489361697 \n",
      "Epoch: 6, TR loss: 0.20360436348487979, VAL loss: (0.1725802398233585, 0.35434134463046457), VL auc: 0.9226626404266889 VL ap: 0.46538161767956693 VL f1: 0.43617021276595747 \n",
      "Epoch: 8, TR loss: 0.14453092137213963, VAL loss: (0.12211963182691, 0.2801227975398936), VL auc: 0.9208114956093159 VL ap: 0.4578598496199249 VL f1: 0.43617021276595747 \n",
      "Epoch: 10, TR loss: 0.13388057042148496, VAL loss: (0.1136976929706617, 0.25003701067985373), VL auc: 0.935105804495968 VL ap: 0.5137222385559068 VL f1: 0.46808510638297873 \n",
      "Epoch: 12, TR loss: 0.13030665281410608, VAL loss: (0.11038896686166055, 0.22829366237559218), VL auc: 0.9406303148103156 VL ap: 0.528683504091062 VL f1: 0.48936170212765956 \n",
      "Epoch: 14, TR loss: 0.12864025562228493, VAL loss: (0.1085751125383403, 0.21387203703535365), VL auc: 0.9457672416785257 VL ap: 0.5455620433254076 VL f1: 0.5212765957446809 \n",
      "Epoch: 16, TR loss: 0.1276927766879283, VAL loss: (0.10744144840043417, 0.2053152449587558), VL auc: 0.9515694237154793 VL ap: 0.5603280653287444 VL f1: 0.5425531914893617 \n",
      "Epoch: 18, TR loss: 0.12707116215239536, VAL loss: (0.10665510218061787, 0.19869347836108917), VL auc: 0.9520900581953652 VL ap: 0.5728995165132283 VL f1: 0.5638297872340425 \n",
      "Epoch: 20, TR loss: 0.1261071621992398, VAL loss: (0.10614592825478351, 0.1986631434014503), VL auc: 0.9583261022988906 VL ap: 0.6136158635132838 VL f1: 0.5851063829787234 \n",
      "Epoch: 22, TR loss: 0.1258496972082645, VAL loss: (0.10560636950809713, 0.1951104427905793), VL auc: 0.9593384471208914 VL ap: 0.6243721150736603 VL f1: 0.5957446808510638 \n",
      "Epoch: 24, TR loss: 0.12567700326763645, VAL loss: (0.10529691230479371, 0.19447087227030002), VL auc: 0.9627948815845799 VL ap: 0.6432604546955396 VL f1: 0.6063829787234043 \n",
      "Epoch: 26, TR loss: 0.125058413602182, VAL loss: (0.104958397853368, 0.19275991967383851), VL auc: 0.9644695891615472 VL ap: 0.656952375106193 VL f1: 0.6276595744680851 \n",
      "Epoch: 28, TR loss: 0.12502045708430845, VAL loss: (0.10466518682134482, 0.19331516103541596), VL auc: 0.9670554070783149 VL ap: 0.679471009366129 VL f1: 0.648936170212766 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, TR loss: 0.1247091195306764, VAL loss: (0.10450721396383479, 0.19376557938596037), VL auc: 0.9681776636238473 VL ap: 0.6871211493278482 VL f1: 0.6595744680851063 \n",
      "Epoch: 32, TR loss: 0.12486649493172189, VAL loss: (0.10427062390873522, 0.1938196750397378), VL auc: 0.9708502539539297 VL ap: 0.7013822849777828 VL f1: 0.6702127659574468 \n",
      "Epoch: 34, TR loss: 0.12439132145206191, VAL loss: (0.10412524639749864, 0.19473400521785655), VL auc: 0.9722443973945136 VL ap: 0.7086269992795888 VL f1: 0.6702127659574468 \n",
      "Epoch: 36, TR loss: 0.1242842261136541, VAL loss: (0.10394141076374729, 0.1949517067442549), VL auc: 0.9708849629192554 VL ap: 0.7193624389716659 VL f1: 0.6702127659574468 \n",
      "Epoch: 38, TR loss: 0.12429882764699583, VAL loss: (0.10380480352985658, 0.1989959554469332), VL auc: 0.9748244304837272 VL ap: 0.7461659551775377 VL f1: 0.6808510638297872 \n",
      "Epoch: 40, TR loss: 0.12479454036792936, VAL loss: (0.10364171122000748, 0.19367794280356548), VL auc: 0.97300799463168 VL ap: 0.7362353050226541 VL f1: 0.6808510638297872 \n",
      "Epoch: 42, TR loss: 0.12397247777519656, VAL loss: (0.10365254753241401, 0.19813748623462432), VL auc: 0.9743674291069385 VL ap: 0.7422343160307392 VL f1: 0.6914893617021277 \n",
      "Epoch: 44, TR loss: 0.12404030955310436, VAL loss: (0.10349211866535396, 0.19724963573699303), VL auc: 0.9742372704869667 VL ap: 0.7485113167331937 VL f1: 0.6914893617021277 \n",
      "Epoch: 46, TR loss: 0.12381876956232027, VAL loss: (0.10354635830869359, 0.20333807519141664), VL auc: 0.9778094015017412 VL ap: 0.7777938747797281 VL f1: 0.7021276595744681 \n",
      "Epoch: 48, TR loss: 0.12387550268112534, VAL loss: (0.10328828646736082, 0.19589085274554313), VL auc: 0.9747260884153044 VL ap: 0.7608005902231392 VL f1: 0.6914893617021277 \n",
      "Epoch: 50, TR loss: 0.12407908017181382, VAL loss: (0.10332813024391228, 0.1963492860185339), VL auc: 0.9696875036155171 VL ap: 0.7617885216216254 VL f1: 0.6914893617021277 \n",
      "Epoch: 52, TR loss: 0.12339277822949171, VAL loss: (0.1031773843604498, 0.20069317107504986), VL auc: 0.9745641132437843 VL ap: 0.7804575489984709 VL f1: 0.7127659574468085 \n",
      "Epoch: 54, TR loss: 0.12370620705132618, VAL loss: (0.10300139800046305, 0.19750189273915392), VL auc: 0.9759061932363796 VL ap: 0.7805188465085003 VL f1: 0.723404255319149 \n",
      "Epoch: 56, TR loss: 0.12343102603115058, VAL loss: (0.10315707249768472, 0.20618950052464263), VL auc: 0.9784920111531474 VL ap: 0.7993400142833869 VL f1: 0.723404255319149 \n",
      "Epoch: 58, TR loss: 0.12336952780837004, VAL loss: (0.10306023436437942, 0.2088059364481175), VL auc: 0.9769995256441406 VL ap: 0.8099362781927445 VL f1: 0.7446808510638298 \n",
      "Epoch: 60, TR loss: 0.1239054600981247, VAL loss: (0.10298599815679803, 0.2039266139902967), VL auc: 0.9779250980528271 VL ap: 0.8039766877686191 VL f1: 0.723404255319149 \n",
      "Epoch: 62, TR loss: 0.12309758078509316, VAL loss: (0.10307804873094667, 0.20823292022055767), VL auc: 0.9792122221836568 VL ap: 0.8089993570286002 VL f1: 0.7446808510638298 \n",
      "Epoch: 64, TR loss: 0.12347592107816706, VAL loss: (0.10296427574800758, 0.20439101280050076), VL auc: 0.9769416773685976 VL ap: 0.8010572204962848 VL f1: 0.723404255319149 \n",
      "Epoch: 66, TR loss: 0.12323762853531878, VAL loss: (0.10287146181953932, 0.21050601309918343), VL auc: 0.9781564911549987 VL ap: 0.8197078859465557 VL f1: 0.7446808510638298 \n",
      "Epoch: 68, TR loss: 0.12365601661446861, VAL loss: (0.10284342514295303, 0.20875141468453914), VL auc: 0.9791109877014568 VL ap: 0.8237530390514752 VL f1: 0.7553191489361702 \n",
      "Epoch: 70, TR loss: 0.12372900934354733, VAL loss: (0.10272991767455988, 0.20717961737450133), VL auc: 0.9791109877014565 VL ap: 0.8251140289116063 VL f1: 0.7446808510638298 \n",
      "Epoch: 72, TR loss: 0.12320988935637696, VAL loss: (0.10265520022186396, 0.20919572546126994), VL auc: 0.9787060497726563 VL ap: 0.8313264340515312 VL f1: 0.7659574468085105 \n",
      "Epoch: 74, TR loss: 0.12311537150244356, VAL loss: (0.10272289813375136, 0.2121840334953146), VL auc: 0.979377089768954 VL ap: 0.8289892857454151 VL f1: 0.7553191489361702 \n",
      "Epoch: 76, TR loss: 0.12293975725514715, VAL loss: (0.1027360494011054, 0.21610454802817486), VL auc: 0.9799035090763946 VL ap: 0.8367879947237927 VL f1: 0.7659574468085105 \n",
      "Epoch: 78, TR loss: 0.12276222352239353, VAL loss: (0.10270791315656437, 0.2190478913327481), VL auc: 0.9816852359631159 VL ap: 0.8451999340813011 VL f1: 0.776595744680851 \n",
      "Epoch: 80, TR loss: 0.12343508159770022, VAL loss: (0.10266728943103419, 0.21164463936014377), VL auc: 0.9781159973621186 VL ap: 0.8335707464335073 VL f1: 0.7553191489361702 \n",
      "Epoch: 82, TR loss: 0.12287868475482211, VAL loss: (0.10262074970952709, 0.21509740707722116), VL auc: 0.9798456608008514 VL ap: 0.8506536821723002 VL f1: 0.7659574468085105 \n",
      "Epoch: 84, TR loss: 0.1233599826627891, VAL loss: (0.10255100235723814, 0.21440359886656415), VL auc: 0.9807596635544296 VL ap: 0.8469326276469906 VL f1: 0.7553191489361702 \n",
      "Epoch: 86, TR loss: 0.12321779136265602, VAL loss: (0.10252093283490943, 0.21391383637773229), VL auc: 0.9809592401050524 VL ap: 0.8468948664717633 VL f1: 0.7659574468085105 \n",
      "Epoch: 88, TR loss: 0.12281958402165932, VAL loss: (0.10259254708634957, 0.21679393281327916), VL auc: 0.9803026621776405 VL ap: 0.8462012209145239 VL f1: 0.776595744680851 \n",
      "Epoch: 90, TR loss: 0.12300471353927665, VAL loss: (0.10246633640639868, 0.2117694489499356), VL auc: 0.9796374070088972 VL ap: 0.8426468552019737 VL f1: 0.7659574468085105 \n",
      "Epoch: 92, TR loss: 0.12376908700488329, VAL loss: (0.10244531097329136, 0.20534517409953665), VL auc: 0.9795332801129198 VL ap: 0.829027129149136 VL f1: 0.7446808510638298 \n",
      "Epoch: 94, TR loss: 0.12273641129772393, VAL loss: (0.10246912430913115, 0.21679521114268202), VL auc: 0.9811414621730127 VL ap: 0.849695942320897 VL f1: 0.7872340425531915 \n",
      "Epoch: 96, TR loss: 0.12314490319698743, VAL loss: (0.10240771577305091, 0.20950773929027802), VL auc: 0.9807885876922009 VL ap: 0.8380676396174355 VL f1: 0.7553191489361702 \n",
      "Epoch: 98, TR loss: 0.12285141611122584, VAL loss: (0.10245062126421034, 0.21966333592191656), VL auc: 0.9834322538845118 VL ap: 0.85167575986532 VL f1: 0.7872340425531915 \n",
      "Epoch: 100, TR loss: 0.1226085526535923, VAL loss: (0.10255209760474018, 0.22229555819896943), VL auc: 0.982541390441151 VL ap: 0.8487150937553212 VL f1: 0.7872340425531915 \n",
      "Epoch: 102, TR loss: 0.12264790583987244, VAL loss: (0.10241934033176574, 0.21887073111026845), VL auc: 0.9810026263117096 VL ap: 0.8472465909646677 VL f1: 0.776595744680851 \n",
      "Epoch: 104, TR loss: 0.12267329234208654, VAL loss: (0.10250155857038472, 0.22161548695665725), VL auc: 0.9813265766547499 VL ap: 0.8505263164069647 VL f1: 0.7872340425531915 \n",
      "Epoch: 106, TR loss: 0.12271961393276967, VAL loss: (0.10244326983021938, 0.22280197955192405), VL auc: 0.9839702428470607 VL ap: 0.8691191892192106 VL f1: 0.7978723404255319 \n",
      "Epoch: 108, TR loss: 0.12311082299410334, VAL loss: (0.10242516505711749, 0.21600351942346452), VL auc: 0.9818327490657504 VL ap: 0.8643890904378416 VL f1: 0.7978723404255319 \n",
      "Epoch: 110, TR loss: 0.12331852327067655, VAL loss: (0.1024892121439981, 0.2180418055108253), VL auc: 0.9784341628776045 VL ap: 0.8638176534252177 VL f1: 0.7978723404255319 \n",
      "Epoch: 112, TR loss: 0.1235031598465033, VAL loss: (0.10258073998638441, 0.2212515080228765), VL auc: 0.9848495366353128 VL ap: 0.8739171823909483 VL f1: 0.7978723404255319 \n",
      "Epoch: 114, TR loss: 0.12321522209029308, VAL loss: (0.10228766171163421, 0.21294883971518658), VL auc: 0.9816968056182245 VL ap: 0.8477609512323948 VL f1: 0.776595744680851 \n",
      "Epoch: 116, TR loss: 0.12273230344946923, VAL loss: (0.10233857412581991, 0.22610711036844455), VL auc: 0.9831661518170144 VL ap: 0.8713448307867411 VL f1: 0.7978723404255319 \n",
      "Epoch: 118, TR loss: 0.12337859494979045, VAL loss: (0.10225376712037793, 0.2172475368418592), VL auc: 0.9818529959621903 VL ap: 0.8568592122717806 VL f1: 0.7978723404255319 \n",
      "Epoch: 120, TR loss: 0.12269301748252416, VAL loss: (0.10235992315478011, 0.22237716837132232), VL auc: 0.9838314069857579 VL ap: 0.8584487633766662 VL f1: 0.7872340425531915 \n",
      "Epoch: 122, TR loss: 0.12254103456591486, VAL loss: (0.1022779455387184, 0.22140259438372673), VL auc: 0.9810199807943724 VL ap: 0.8542827412774336 VL f1: 0.7872340425531915 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124, TR loss: 0.12286197701564794, VAL loss: (0.10226898442279261, 0.2195272445678711), VL auc: 0.9815174759640416 VL ap: 0.8573095097401755 VL f1: 0.7872340425531915 \n",
      "Epoch: 126, TR loss: 0.12337939411299637, VAL loss: (0.10226247931641687, 0.20990753173828125), VL auc: 0.9800365601101431 VL ap: 0.8468894222731173 VL f1: 0.776595744680851 \n",
      "Epoch: 128, TR loss: 0.12284293900619112, VAL loss: (0.10235462945852025, 0.21793349245761304), VL auc: 0.9815464001018129 VL ap: 0.8550949886711764 VL f1: 0.776595744680851 \n",
      "Epoch: 130, TR loss: 0.12242709032414419, VAL loss: (0.10219827458027461, 0.2179440234569793), VL auc: 0.9807885876922009 VL ap: 0.8538546293119755 VL f1: 0.776595744680851 \n",
      "Epoch: 132, TR loss: 0.12278288973352224, VAL loss: (0.10232288387562025, 0.22366199087589345), VL auc: 0.9815174759640415 VL ap: 0.8578642183915237 VL f1: 0.7978723404255319 \n",
      "Epoch: 134, TR loss: 0.12278728139674727, VAL loss: (0.10221239663518726, 0.22085806664000165), VL auc: 0.9809621325188298 VL ap: 0.8573939786148231 VL f1: 0.7978723404255319 \n",
      "Epoch: 136, TR loss: 0.12281092766506439, VAL loss: (0.10225696988958843, 0.21717649825075838), VL auc: 0.9818529959621904 VL ap: 0.8533523728344079 VL f1: 0.776595744680851 \n",
      "Epoch: 138, TR loss: 0.12244600883256088, VAL loss: (0.1021879195129826, 0.22252310083267537), VL auc: 0.9831661518170145 VL ap: 0.8668563499376343 VL f1: 0.7978723404255319 \n",
      "Epoch: 140, TR loss: 0.1229417140846794, VAL loss: (0.10221095289984367, 0.21679324292122049), VL auc: 0.9828855876806313 VL ap: 0.8549355649366386 VL f1: 0.7872340425531915 \n",
      "Epoch: 142, TR loss: 0.1230095608002172, VAL loss: (0.10221929171605237, 0.21720752310245595), VL auc: 0.9817720083764304 VL ap: 0.8614543700237586 VL f1: 0.7978723404255319 \n",
      "Epoch: 144, TR loss: 0.1230562931757258, VAL loss: (0.10215424065229489, 0.214820293670005), VL auc: 0.9791804056321082 VL ap: 0.8534488169941031 VL f1: 0.7659574468085105 \n",
      "Epoch: 146, TR loss: 0.12290276421440972, VAL loss: (0.10218743826786807, 0.2216922273027136), VL auc: 0.9830273159557115 VL ap: 0.8697842391589855 VL f1: 0.7978723404255319 \n",
      "Epoch: 148, TR loss: 0.1225852798260256, VAL loss: (0.10225734326941867, 0.22845079543742727), VL auc: 0.9830041766454942 VL ap: 0.8786406573251416 VL f1: 0.8085106382978723 \n",
      "Epoch: 150, TR loss: 0.1230754805614828, VAL loss: (0.10217071085147329, 0.22379158912821018), VL auc: 0.9839875973297237 VL ap: 0.8808446067290523 VL f1: 0.7978723404255319 \n",
      "Epoch: 152, TR loss: 0.12316256694448269, VAL loss: (0.10213806915698069, 0.22205332492260224), VL auc: 0.982304212511425 VL ap: 0.8778010282820159 VL f1: 0.7978723404255319 \n",
      "Epoch: 154, TR loss: 0.1225484809744784, VAL loss: (0.10221106076512795, 0.22939292420732213), VL auc: 0.9832182152650031 VL ap: 0.8843482769001495 VL f1: 0.7978723404255319 \n",
      "Epoch: 156, TR loss: 0.12308282987545123, VAL loss: (0.10210730265896886, 0.2178111380719124), VL auc: 0.9830678097485915 VL ap: 0.8724006074765219 VL f1: 0.7978723404255319 \n",
      "Epoch: 158, TR loss: 0.12327710122263905, VAL loss: (0.10215642284996941, 0.21680819734613946), VL auc: 0.981945553203059 VL ap: 0.8658256646716715 VL f1: 0.7872340425531915 \n",
      "Epoch: 160, TR loss: 0.12244418644169879, VAL loss: (0.10220193370261096, 0.22177980301227976), VL auc: 0.9832529242303287 VL ap: 0.8672840237259705 VL f1: 0.7872340425531915 \n",
      "Epoch: 162, TR loss: 0.1224580634999847, VAL loss: (0.10221088652120718, 0.22394287839848945), VL auc: 0.9826165931993568 VL ap: 0.8709691585089754 VL f1: 0.7872340425531915 \n",
      "Epoch: 164, TR loss: 0.12340476567758046, VAL loss: (0.10213741366794539, 0.2188065305669257), VL auc: 0.9815579697569217 VL ap: 0.8679862917014726 VL f1: 0.7978723404255319 \n",
      "Epoch: 166, TR loss: 0.12364879427035533, VAL loss: (0.10238207702470772, 0.2186224511329164), VL auc: 0.9850057269792788 VL ap: 0.8758257842469568 VL f1: 0.7872340425531915 \n",
      "Epoch: 168, TR loss: 0.12294992978118882, VAL loss: (0.10234382633543196, 0.22980113739662983), VL auc: 0.9860296414563883 VL ap: 0.8801708130306402 VL f1: 0.7978723404255319 \n",
      "Epoch: 170, TR loss: 0.12199412311809762, VAL loss: (0.10229833207744952, 0.2350966068024331), VL auc: 0.9842305600870039 VL ap: 0.8827233860962589 VL f1: 0.8191489361702128 \n",
      "Epoch: 172, TR loss: 0.12288908134531402, VAL loss: (0.10229303008386012, 0.2261910134173454), VL auc: 0.9813150069996412 VL ap: 0.8736478370299903 VL f1: 0.7872340425531915 \n",
      "Epoch: 174, TR loss: 0.12268123169244065, VAL loss: (0.10216035578418128, 0.2242027242132958), VL auc: 0.9817141601008874 VL ap: 0.8762127746492527 VL f1: 0.7872340425531915 \n",
      "Epoch: 176, TR loss: 0.12247064098445913, VAL loss: (0.10211570785381406, 0.22606468200683594), VL auc: 0.98132657665475 VL ap: 0.8707300101977339 VL f1: 0.7872340425531915 \n",
      "Epoch: 178, TR loss: 0.1223877819507541, VAL loss: (0.10209620912934594, 0.22659800915007897), VL auc: 0.9811356773454584 VL ap: 0.8767225169078497 VL f1: 0.7978723404255319 \n",
      "Epoch: 180, TR loss: 0.12258014128129971, VAL loss: (0.10224761050184374, 0.2254774215373587), VL auc: 0.9840685849154837 VL ap: 0.8784501345103123 VL f1: 0.7978723404255319 \n",
      "Epoch: 182, TR loss: 0.12243257243436047, VAL loss: (0.10228699792526934, 0.23319611650832156), VL auc: 0.9858387421470967 VL ap: 0.8831095838774677 VL f1: 0.8085106382978723 \n",
      "Epoch: 184, TR loss: 0.1228389954718666, VAL loss: (0.10216782338078609, 0.22660308188580452), VL auc: 0.9845429407749355 VL ap: 0.8766417194949246 VL f1: 0.7872340425531915 \n",
      "Epoch: 186, TR loss: 0.12313064522913607, VAL loss: (0.10214034262528038, 0.22951771350617103), VL auc: 0.9836057987111406 VL ap: 0.8895432364206034 VL f1: 0.8297872340425532 \n",
      "Epoch: 188, TR loss: 0.12240684236665596, VAL loss: (0.10216549183117948, 0.22470106977097531), VL auc: 0.9813005449307557 VL ap: 0.8626026865090464 VL f1: 0.776595744680851 \n",
      "Epoch: 190, TR loss: 0.12312365441828806, VAL loss: (0.10212540743207076, 0.22461844505147732), VL auc: 0.9804472828664977 VL ap: 0.870135490095649 VL f1: 0.8085106382978723 \n",
      "Epoch: 192, TR loss: 0.1230710291477377, VAL loss: (0.10207452820720331, 0.22281382946257897), VL auc: 0.9833859752640774 VL ap: 0.8757789399355173 VL f1: 0.8191489361702128 \n",
      "Epoch: 194, TR loss: 0.12276721269081926, VAL loss: (0.10231151653412181, 0.22623358381555436), VL auc: 0.9847540869806672 VL ap: 0.8796790937030482 VL f1: 0.8085106382978723 \n",
      "Epoch: 196, TR loss: 0.12316643579065713, VAL loss: (0.1020911477583138, 0.22606301814951796), VL auc: 0.9834669628498373 VL ap: 0.8797527363256826 VL f1: 0.8297872340425532 \n",
      "Epoch: 198, TR loss: 0.1226211152004367, VAL loss: (0.10232977065915579, 0.22867833807113322), VL auc: 0.9833917600916318 VL ap: 0.8829565263448813 VL f1: 0.8297872340425532 \n",
      "Epoch: 200, TR loss: 0.12331454239227699, VAL loss: (0.10224266529342543, 0.2194368078353557), VL auc: 0.9850057269792788 VL ap: 0.8755130674555416 VL f1: 0.7872340425531915 \n",
      "Epoch: 202, TR loss: 0.12292915153783499, VAL loss: (0.10215862993963261, 0.22407523621904088), VL auc: 0.9827438594055511 VL ap: 0.88085661030052 VL f1: 0.7978723404255319 \n",
      "Epoch: 204, TR loss: 0.12243088448216853, VAL loss: (0.10214616735063213, 0.23135507867691366), VL auc: 0.9858271724919879 VL ap: 0.8922157223698419 VL f1: 0.8085106382978723 \n",
      "Epoch: 206, TR loss: 0.12318902895605803, VAL loss: (0.10194104906655706, 0.22453072730531085), VL auc: 0.9834640704360603 VL ap: 0.8880442890829405 VL f1: 0.8191489361702128 \n",
      "Epoch: 208, TR loss: 0.12218215800475328, VAL loss: (0.10216216460202555, 0.23680800579963845), VL auc: 0.9842797311212153 VL ap: 0.9022764573388062 VL f1: 0.8191489361702128 \n",
      "Epoch: 210, TR loss: 0.12227684764143187, VAL loss: (0.1022134918826893, 0.2397736894323471), VL auc: 0.9852024111161246 VL ap: 0.9025292977881167 VL f1: 0.8191489361702128 \n",
      "Epoch: 212, TR loss: 0.12240292870759147, VAL loss: (0.10209269106161212, 0.2346926750020778), VL auc: 0.9841090787083638 VL ap: 0.8962336248815838 VL f1: 0.8191489361702128 \n",
      "Epoch: 214, TR loss: 0.12242896499671133, VAL loss: (0.10205438229102944, 0.2313314803103183), VL auc: 0.9831777214721229 VL ap: 0.8907848214501496 VL f1: 0.8191489361702128 \n",
      "Epoch: 216, TR loss: 0.12283169843960322, VAL loss: (0.10210946826198426, 0.22944413854720744), VL auc: 0.9854511587009592 VL ap: 0.8865080696419019 VL f1: 0.8191489361702128 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 218, TR loss: 0.12273107109499282, VAL loss: (0.10201380834947661, 0.22706461967305935), VL auc: 0.9830388856108199 VL ap: 0.8796047190701727 VL f1: 0.7978723404255319 \n",
      "Epoch: 220, TR loss: 0.12337417341130537, VAL loss: (0.10197072031706685, 0.22221307551607172), VL auc: 0.9817604387213218 VL ap: 0.8743012997906239 VL f1: 0.7978723404255319 \n",
      "Epoch: 222, TR loss: 0.12256238043920935, VAL loss: (0.10208722312143148, 0.235517440958226), VL auc: 0.983397544919186 VL ap: 0.893468579286129 VL f1: 0.8191489361702128 \n",
      "Epoch: 224, TR loss: 0.12297227647569445, VAL loss: (0.10229114659004979, 0.23247479377908908), VL auc: 0.9855234690453878 VL ap: 0.8954105874359755 VL f1: 0.8297872340425532 \n",
      "Epoch: 226, TR loss: 0.12285413475988895, VAL loss: (0.10209956125048855, 0.23023339535327667), VL auc: 0.9842305600870038 VL ap: 0.8905222106576164 VL f1: 0.8297872340425532 \n",
      "Epoch: 228, TR loss: 0.12265910906238528, VAL loss: (0.10210289677697203, 0.23062801361083984), VL auc: 0.9846673145673528 VL ap: 0.8806849787190869 VL f1: 0.7978723404255319 \n",
      "Epoch: 230, TR loss: 0.12250893359900805, VAL loss: (0.10234195943628076, 0.2310601904037151), VL auc: 0.9852833987018847 VL ap: 0.8865275565826704 VL f1: 0.8085106382978723 \n",
      "Epoch: 232, TR loss: 0.12295266336748195, VAL loss: (0.10212934036628263, 0.22870717150099734), VL auc: 0.9861974014554626 VL ap: 0.8774763569297787 VL f1: 0.7872340425531915 \n",
      "Epoch: 234, TR loss: 0.12208750571214971, VAL loss: (0.10205268963579901, 0.2327783057030211), VL auc: 0.981563754584476 VL ap: 0.8805051701162989 VL f1: 0.8191489361702128 \n",
      "Epoch: 236, TR loss: 0.12283958550825226, VAL loss: (0.10208252683290002, 0.23081903254732172), VL auc: 0.985352816632536 VL ap: 0.8748755571601242 VL f1: 0.7872340425531915 \n",
      "Epoch: 238, TR loss: 0.12194420902739538, VAL loss: (0.10207280236265463, 0.2315423437889586), VL auc: 0.9831661518170143 VL ap: 0.8719098232888832 VL f1: 0.7872340425531915 \n",
      "Epoch: 240, TR loss: 0.12272856157314993, VAL loss: (0.10219079038901067, 0.22290970416779213), VL auc: 0.9743905684171554 VL ap: 0.8577809400242955 VL f1: 0.7872340425531915 \n",
      "Epoch: 242, TR loss: 0.12254129597444016, VAL loss: (0.10206828861537351, 0.22827436568889212), VL auc: 0.9840917242257008 VL ap: 0.8730715189888348 VL f1: 0.7872340425531915 \n",
      "Epoch: 244, TR loss: 0.12254145281955534, VAL loss: (0.10216257117117404, 0.23057219322691572), VL auc: 0.9824256938900652 VL ap: 0.8798646712772077 VL f1: 0.8085106382978723 \n",
      "Epoch: 246, TR loss: 0.12216661540072045, VAL loss: (0.10205299663699276, 0.2320763405333174), VL auc: 0.9835248111253804 VL ap: 0.8843685735937986 VL f1: 0.7978723404255319 \n",
      "Epoch: 248, TR loss: 0.12213853265628824, VAL loss: (0.10214666519040579, 0.2371341218339636), VL auc: 0.985986255249731 VL ap: 0.8933912673645237 VL f1: 0.8297872340425532 \n",
      "Epoch: 250, TR loss: 0.12290855254604136, VAL loss: (0.10209917127599918, 0.23151424083303898), VL auc: 0.9849710180139531 VL ap: 0.8729043506561578 VL f1: 0.7978723404255319 \n",
      "Epoch: 252, TR loss: 0.1221070142569521, VAL loss: (0.10215020815012829, 0.23956582901325632), VL auc: 0.9855118993902793 VL ap: 0.884276455070303 VL f1: 0.8191489361702128 \n",
      "Epoch: 254, TR loss: 0.12331303369164526, VAL loss: (0.10227028710353368, 0.21468539948159077), VL auc: 0.9816852359631161 VL ap: 0.8678773872612422 VL f1: 0.7872340425531915 \n",
      "Epoch: 256, TR loss: 0.122150527573192, VAL loss: (0.10216723427038726, 0.23790826188757064), VL auc: 0.9861337683523654 VL ap: 0.8910903210038311 VL f1: 0.8191489361702128 \n",
      "Epoch: 258, TR loss: 0.1221965653489048, VAL loss: (0.10217850204393097, 0.23412148495937915), VL auc: 0.978682910462439 VL ap: 0.8826861742824577 VL f1: 0.8085106382978723 \n",
      "Epoch: 260, TR loss: 0.12239227817738925, VAL loss: (0.10207008083855866, 0.23185512867379696), VL auc: 0.9824488332002823 VL ap: 0.8876428791392075 VL f1: 0.8191489361702128 \n",
      "Epoch: 262, TR loss: 0.1234821426010692, VAL loss: (0.10195087310475717, 0.22639124444190492), VL auc: 0.9826628718197912 VL ap: 0.8877433594675184 VL f1: 0.8191489361702128 \n",
      "Epoch: 264, TR loss: 0.12217379293194368, VAL loss: (0.10198535680641228, 0.23423247641705452), VL auc: 0.9843173325003182 VL ap: 0.8910327037911254 VL f1: 0.8297872340425532 \n",
      "Epoch: 266, TR loss: 0.12313350578528436, VAL loss: (0.10210278891168774, 0.23449045546511385), VL auc: 0.9879617738595212 VL ap: 0.8931595183896549 VL f1: 0.8297872340425532 \n",
      "Epoch: 268, TR loss: 0.12265851155718459, VAL loss: (0.10204836672709777, 0.22906628060848155), VL auc: 0.9836462925040205 VL ap: 0.8745501874995449 VL f1: 0.7872340425531915 \n",
      "Epoch: 270, TR loss: 0.12290941892858236, VAL loss: (0.10207470245112409, 0.23106875318161985), VL auc: 0.9855784249071536 VL ap: 0.8814898766201864 VL f1: 0.8297872340425532 \n",
      "Epoch: 272, TR loss: 0.12299835010888935, VAL loss: (0.10213907313385756, 0.23423247641705452), VL auc: 0.9845198014647185 VL ap: 0.8992778880699686 VL f1: 0.8297872340425532 \n",
      "Epoch: 274, TR loss: 0.12303517883569659, VAL loss: (0.10216525950595177, 0.23892544685526096), VL auc: 0.9872242083463492 VL ap: 0.9032799526375939 VL f1: 0.8297872340425532 \n",
      "Epoch: 276, TR loss: 0.12275254393814244, VAL loss: (0.10190425040995446, 0.22982678514845828), VL auc: 0.9842421297421124 VL ap: 0.8856364273689861 VL f1: 0.8191489361702128 \n",
      "Epoch: 278, TR loss: 0.12232640322901371, VAL loss: (0.101978370454922, 0.23345356799186545), VL auc: 0.9847107007740099 VL ap: 0.8918227697381178 VL f1: 0.8191489361702128 \n",
      "Epoch: 280, TR loss: 0.12236073737160809, VAL loss: (0.10196163474119767, 0.233558086638755), VL auc: 0.9844388138789583 VL ap: 0.8932730011014469 VL f1: 0.8191489361702128 \n",
      "Epoch: 282, TR loss: 0.12358566784590293, VAL loss: (0.10198733986817733, 0.22474006896323345), VL auc: 0.9846586373260213 VL ap: 0.8979918642318377 VL f1: 0.8191489361702128 \n",
      "Epoch: 284, TR loss: 0.12273100387565773, VAL loss: (0.10197937443179887, 0.22968549931302984), VL auc: 0.9850028345655016 VL ap: 0.8875260678350423 VL f1: 0.8191489361702128 \n",
      "Epoch: 286, TR loss: 0.12270936671857792, VAL loss: (0.10195489730959421, 0.2263443926547436), VL auc: 0.9811472470005669 VL ap: 0.8674780966294671 VL f1: 0.7978723404255319 \n",
      "Epoch: 288, TR loss: 0.12236152159718398, VAL loss: (0.10199167107420813, 0.23574080365769406), VL auc: 0.9861569076625827 VL ap: 0.8914506926078889 VL f1: 0.8191489361702128 \n",
      "Epoch: 290, TR loss: 0.12240153950799988, VAL loss: (0.10201864569261063, 0.23552874301342255), VL auc: 0.9838892552613008 VL ap: 0.8855754515925224 VL f1: 0.8191489361702128 \n",
      "Epoch: 292, TR loss: 0.12239835032399123, VAL loss: (0.10211685288529347, 0.23944398190112823), VL auc: 0.9876841021369154 VL ap: 0.896399010293561 VL f1: 0.8297872340425532 \n",
      "Epoch: 294, TR loss: 0.12273568682266811, VAL loss: (0.10195942765153446, 0.22874044864735704), VL auc: 0.9833512662987518 VL ap: 0.8831511701030553 VL f1: 0.8085106382978723 \n",
      "Epoch: 296, TR loss: 0.12252016669678092, VAL loss: (0.1020720887923124, 0.23675039981273896), VL auc: 0.9850693600823759 VL ap: 0.8889604329018452 VL f1: 0.8191489361702128 \n",
      "Epoch: 298, TR loss: 0.12219446661188739, VAL loss: (0.10202458658057623, 0.23583580585236244), VL auc: 0.9850664676685988 VL ap: 0.8846159593668641 VL f1: 0.8085106382978723 \n",
      "Epoch: 300, TR loss: 0.12229599768311383, VAL loss: (0.10194791925543349, 0.23094779887097946), VL auc: 0.981526153205373 VL ap: 0.8787113157209684 VL f1: 0.8085106382978723 \n",
      "Stopping at epoch 238, TR loss: 0.12194420902739538, VAL loss: 0.10207280236265463, VAL auc: 0.9831661518170143,TS loss: 0.10207280236265463, TS auc: 0.9831661518170143 TS ap: 0.8719098232888832 TS f1: 0.7872340425531915\n",
      "Final training run 4: 0.9831661518170143, (0.9831661518170143, 0.8719098232888832, 0.7872340425531915)\n",
      "Normal Cls: 0\n",
      "Epoch: 1, TR loss: 0.901940015735897, VAL loss: (0.8225258292550299, 0.8879340151523022), VL auc: 0.8888156144065346 VL ap: 0.6666455280362396 VL f1: 0.648936170212766 \n",
      "Epoch: 2, TR loss: 0.7675759739812775, VAL loss: (0.6371946184449769, 0.7871410288709275), VL auc: 0.9259657769601888 VL ap: 0.5962724274610149 VL f1: 0.5957446808510638 \n",
      "Epoch: 4, TR loss: 0.39526247686459864, VAL loss: (0.35260052911738715, 0.5517723002332322), VL auc: 0.9214593962953966 VL ap: 0.4338111379599839 VL f1: 0.5425531914893617 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, TR loss: 0.23402541464470908, VAL loss: (0.20058917013959693, 0.44047168975180767), VL auc: 0.9424236113521455 VL ap: 0.6118676189767478 VL f1: 0.6170212765957447 \n",
      "Epoch: 8, TR loss: 0.1556217830319995, VAL loss: (0.1251440997224842, 0.2847403262523895), VL auc: 0.9367486955213865 VL ap: 0.5366359662152811 VL f1: 0.5425531914893617 \n",
      "Epoch: 10, TR loss: 0.13349892891217724, VAL loss: (0.11290744700595177, 0.24489510312993476), VL auc: 0.9431640692790949 VL ap: 0.5825871314469877 VL f1: 0.5638297872340425 \n",
      "Epoch: 12, TR loss: 0.13012465272997736, VAL loss: (0.10955026449231495, 0.22478377565424493), VL auc: 0.9433376141057235 VL ap: 0.5980564263346371 VL f1: 0.5638297872340425 \n",
      "Epoch: 14, TR loss: 0.12817585964268233, VAL loss: (0.10773178025909971, 0.21091552490883686), VL auc: 0.9463920030543889 VL ap: 0.6029113267973862 VL f1: 0.5638297872340425 \n",
      "Epoch: 16, TR loss: 0.12724655980405652, VAL loss: (0.10662475884641365, 0.20438646762929064), VL auc: 0.9485497437321393 VL ap: 0.6333696835737845 VL f1: 0.5957446808510638 \n",
      "Epoch: 18, TR loss: 0.1262449319608878, VAL loss: (0.10590396982745803, 0.2030757335906333), VL auc: 0.9515752085430333 VL ap: 0.6517196162415081 VL f1: 0.6063829787234043 \n",
      "Epoch: 20, TR loss: 0.1260170285397164, VAL loss: (0.10550101831466235, 0.1997169738120221), VL auc: 0.953952772667847 VL ap: 0.6581900426534829 VL f1: 0.6170212765957447 \n",
      "Epoch: 22, TR loss: 0.12580712496271568, VAL loss: (0.1049594350195631, 0.19532355856388173), VL auc: 0.9554799671421794 VL ap: 0.6701583695208531 VL f1: 0.6170212765957447 \n",
      "Epoch: 24, TR loss: 0.12536494123894854, VAL loss: (0.10464439371346519, 0.197079820835844), VL auc: 0.9581699119549246 VL ap: 0.689208992618439 VL f1: 0.648936170212766 \n",
      "Epoch: 26, TR loss: 0.12502033758326833, VAL loss: (0.10442299606879163, 0.19887993183541805), VL auc: 0.9624622540002082 VL ap: 0.713107344290945 VL f1: 0.6595744680851063 \n",
      "Epoch: 28, TR loss: 0.12437584606736417, VAL loss: (0.1041284574640387, 0.19904785967887717), VL auc: 0.9643596774380155 VL ap: 0.7250089846236519 VL f1: 0.6702127659574468 \n",
      "Epoch: 30, TR loss: 0.12467081944731247, VAL loss: (0.10403560204892265, 0.20137060449478475), VL auc: 0.9670959008711951 VL ap: 0.7447365858935041 VL f1: 0.6808510638297872 \n",
      "Epoch: 32, TR loss: 0.12438411404557866, VAL loss: (0.10379538606080496, 0.20108886475258686), VL auc: 0.9679578401767843 VL ap: 0.7520388979480318 VL f1: 0.6808510638297872 \n",
      "Epoch: 34, TR loss: 0.12376490446847849, VAL loss: (0.10360941801335645, 0.20654361806017288), VL auc: 0.971434521536913 VL ap: 0.7740215965605722 VL f1: 0.7021276595744681 \n",
      "Epoch: 36, TR loss: 0.12385760740036482, VAL loss: (0.1035287181860471, 0.2039641928165517), VL auc: 0.9710469380907756 VL ap: 0.7724997497681506 VL f1: 0.6914893617021277 \n",
      "Epoch: 38, TR loss: 0.12390053814903405, VAL loss: (0.1033194678318507, 0.2014858773414125), VL auc: 0.9708010829197183 VL ap: 0.7747890466216644 VL f1: 0.7021276595744681 \n",
      "Epoch: 40, TR loss: 0.12422222748026798, VAL loss: (0.10325447484940015, 0.20313433383373505), VL auc: 0.9715212939502275 VL ap: 0.78943888022048 VL f1: 0.723404255319149 \n",
      "Epoch: 42, TR loss: 0.12375319336654506, VAL loss: (0.10323339963231545, 0.2095345030439661), VL auc: 0.9741678525563152 VL ap: 0.8005288102415681 VL f1: 0.7340425531914893 \n",
      "Epoch: 44, TR loss: 0.1238817391416575, VAL loss: (0.10313661958031708, 0.20383930206298828), VL auc: 0.977144146332998 VL ap: 0.7960133053685635 VL f1: 0.7340425531914893 \n",
      "Epoch: 46, TR loss: 0.12427102124871894, VAL loss: (0.10291258338484316, 0.20027716616366772), VL auc: 0.9746624553122072 VL ap: 0.7888303680968288 VL f1: 0.7340425531914893 \n",
      "Epoch: 48, TR loss: 0.12347905051165566, VAL loss: (0.10300898175968172, 0.2069521761955099), VL auc: 0.9761751877176542 VL ap: 0.7990415880772205 VL f1: 0.723404255319149 \n",
      "Epoch: 50, TR loss: 0.12423158590547372, VAL loss: (0.1030035553061489, 0.19623577848393867), VL auc: 0.9748302153112816 VL ap: 0.7790529771658155 VL f1: 0.7127659574468085 \n",
      "Epoch: 52, TR loss: 0.12307822908540596, VAL loss: (0.10299781355409275, 0.21028147352502702), VL auc: 0.979458077354714 VL ap: 0.8087152140418077 VL f1: 0.7446808510638298 \n",
      "Epoch: 54, TR loss: 0.12365625561654889, VAL loss: (0.10291149643467068, 0.20233531708412983), VL auc: 0.9769416773685976 VL ap: 0.8018592107911696 VL f1: 0.7446808510638298 \n",
      "Epoch: 56, TR loss: 0.12390705842453653, VAL loss: (0.1028391022342518, 0.19424578484068525), VL auc: 0.972371663600708 VL ap: 0.7727490492907583 VL f1: 0.7127659574468085 \n",
      "Epoch: 58, TR loss: 0.12335134124382419, VAL loss: (0.1029109820002379, 0.20641168634942236), VL auc: 0.9776532111577755 VL ap: 0.8113269694758146 VL f1: 0.7446808510638298 \n",
      "Epoch: 60, TR loss: 0.12329843215830351, VAL loss: (0.10279726709860573, 0.20905939061590967), VL auc: 0.9772482732289751 VL ap: 0.8220063308513402 VL f1: 0.7553191489361702 \n",
      "Epoch: 62, TR loss: 0.12317073782810206, VAL loss: (0.10268773405107226, 0.20299909469929148), VL auc: 0.9719609408443535 VL ap: 0.793498455361928 VL f1: 0.7340425531914893 \n",
      "Epoch: 64, TR loss: 0.12327389710100037, VAL loss: (0.10266086729795405, 0.20433730267463845), VL auc: 0.9759640415119226 VL ap: 0.80455716898965 VL f1: 0.7446808510638298 \n",
      "Epoch: 66, TR loss: 0.12354722585505384, VAL loss: (0.10262386950544199, 0.2100714419750457), VL auc: 0.9783936690847246 VL ap: 0.8237473953936761 VL f1: 0.7659574468085105 \n",
      "Epoch: 68, TR loss: 0.12330988185171164, VAL loss: (0.10257003643125084, 0.2073479510368185), VL auc: 0.9792671780454225 VL ap: 0.8142348066386411 VL f1: 0.7659574468085105 \n",
      "Epoch: 70, TR loss: 0.12387374750959833, VAL loss: (0.10255702621849935, 0.19989005555497838), VL auc: 0.9772309187463122 VL ap: 0.8016569832707836 VL f1: 0.7446808510638298 \n",
      "Epoch: 72, TR loss: 0.12342340037102682, VAL loss: (0.10251488408165953, 0.19904607407590177), VL auc: 0.9764326125438201 VL ap: 0.815561782179935 VL f1: 0.7553191489361702 \n",
      "Epoch: 74, TR loss: 0.1236292334438479, VAL loss: (0.10253308012538659, 0.19382383468303274), VL auc: 0.9773697546076151 VL ap: 0.7962148168896827 VL f1: 0.723404255319149 \n",
      "Epoch: 76, TR loss: 0.12307087977143753, VAL loss: (0.10257301517256322, 0.20918032463560712), VL auc: 0.9797704580426458 VL ap: 0.818532169580404 VL f1: 0.7659574468085105 \n",
      "Epoch: 78, TR loss: 0.12301794827947182, VAL loss: (0.10238089050658052, 0.2053553804438165), VL auc: 0.9785787835664619 VL ap: 0.8240230859480129 VL f1: 0.7659574468085105 \n",
      "Epoch: 80, TR loss: 0.12312012913760402, VAL loss: (0.10260442056495123, 0.2044659878345246), VL auc: 0.96962387051242 VL ap: 0.814105465274037 VL f1: 0.7553191489361702 \n",
      "Epoch: 82, TR loss: 0.12352100284555877, VAL loss: (0.10243115572906046, 0.2012338638305664), VL auc: 0.9797762428702 VL ap: 0.8362271360147558 VL f1: 0.776595744680851 \n",
      "Epoch: 84, TR loss: 0.12319525047896017, VAL loss: (0.10234879643583894, 0.205318735000935), VL auc: 0.9788448856339592 VL ap: 0.8181557688522656 VL f1: 0.776595744680851 \n",
      "Epoch: 86, TR loss: 0.12291868772800799, VAL loss: (0.10240988967339587, 0.21335151347708195), VL auc: 0.9802505987296519 VL ap: 0.83993392383341 VL f1: 0.7872340425531915 \n",
      "Epoch: 88, TR loss: 0.12319537744881531, VAL loss: (0.10228104873997416, 0.20046473563985623), VL auc: 0.9780870732243472 VL ap: 0.8172268525791972 VL f1: 0.776595744680851 \n",
      "Epoch: 90, TR loss: 0.12341301124934992, VAL loss: (0.10217207161352128, 0.2015938048667096), VL auc: 0.9795101408027027 VL ap: 0.8312025274825269 VL f1: 0.776595744680851 \n",
      "Epoch: 92, TR loss: 0.1227557331221511, VAL loss: (0.1022966394222191, 0.20324132797565866), VL auc: 0.9786077077042334 VL ap: 0.8276297773984169 VL f1: 0.7659574468085105 \n",
      "Epoch: 94, TR loss: 0.12324867491271645, VAL loss: (0.10219482289117728, 0.20347191425079994), VL auc: 0.978101535293233 VL ap: 0.8231914459220697 VL f1: 0.7659574468085105 \n",
      "Epoch: 96, TR loss: 0.12312461789542416, VAL loss: (0.10234246557338397, 0.21136992028419008), VL auc: 0.9817806856177618 VL ap: 0.8539266445799797 VL f1: 0.7978723404255319 \n",
      "Epoch: 98, TR loss: 0.12336205152454648, VAL loss: (0.10229804996824446, 0.2067483942559425), VL auc: 0.9818819200999618 VL ap: 0.837388684109832 VL f1: 0.7872340425531915 \n",
      "Epoch: 100, TR loss: 0.1226081194623218, VAL loss: (0.10224576849468121, 0.21174286781473362), VL auc: 0.9790820635636852 VL ap: 0.832546826454202 VL f1: 0.776595744680851 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102, TR loss: 0.12404264729220203, VAL loss: (0.10220460544272958, 0.20158167088285406), VL auc: 0.9788824870130621 VL ap: 0.8328573391630163 VL f1: 0.776595744680851 \n",
      "Epoch: 104, TR loss: 0.12232748620718994, VAL loss: (0.10217296772511385, 0.21231616811549409), VL auc: 0.9802274594194347 VL ap: 0.8360227731857839 VL f1: 0.7872340425531915 \n",
      "Epoch: 106, TR loss: 0.12283449924523143, VAL loss: (0.10215083874717493, 0.20729286112683884), VL auc: 0.9799064014901715 VL ap: 0.8165849306458061 VL f1: 0.776595744680851 \n",
      "Epoch: 108, TR loss: 0.12314996705356324, VAL loss: (0.10222149050838601, 0.21515180709514212), VL auc: 0.9807133849339953 VL ap: 0.8442619993266764 VL f1: 0.7978723404255319 \n",
      "Epoch: 110, TR loss: 0.12368724373001942, VAL loss: (0.10214182784727178, 0.20468009786402924), VL auc: 0.9767912718521861 VL ap: 0.8280297585297497 VL f1: 0.776595744680851 \n",
      "Epoch: 112, TR loss: 0.12301868769215767, VAL loss: (0.10218764570110708, 0.20649246459311627), VL auc: 0.9767797021970777 VL ap: 0.8106571888954176 VL f1: 0.7659574468085105 \n",
      "Epoch: 114, TR loss: 0.12259038102667645, VAL loss: (0.10223550469801437, 0.20389840957966257), VL auc: 0.9752525077227447 VL ap: 0.7953359887774165 VL f1: 0.7446808510638298 \n",
      "Epoch: 116, TR loss: 0.12290313018634515, VAL loss: (0.10230911030854914, 0.2036843198411008), VL auc: 0.970242847060729 VL ap: 0.8052473449525758 VL f1: 0.7553191489361702 \n",
      "Epoch: 118, TR loss: 0.12334025752235148, VAL loss: (0.10223453391045575, 0.2064226231676467), VL auc: 0.9796402994226744 VL ap: 0.8290220165875325 VL f1: 0.776595744680851 \n",
      "Epoch: 120, TR loss: 0.12253859226340706, VAL loss: (0.10215028282609434, 0.20715561318904796), VL auc: 0.9772222415049808 VL ap: 0.8218466039404768 VL f1: 0.7872340425531915 \n",
      "Epoch: 122, TR loss: 0.12283808427643554, VAL loss: (0.10218994406139546, 0.19890594482421875), VL auc: 0.9742112387629724 VL ap: 0.7860810750084645 VL f1: 0.7340425531914893 \n",
      "Epoch: 124, TR loss: 0.12327891614468613, VAL loss: (0.10208747204131831, 0.20029979056500374), VL auc: 0.9763284856478429 VL ap: 0.8251695459860486 VL f1: 0.776595744680851 \n",
      "Epoch: 126, TR loss: 0.1228086646141168, VAL loss: (0.10208828517961528, 0.2031484766209379), VL auc: 0.978599030462902 VL ap: 0.8234810981796755 VL f1: 0.776595744680851 \n",
      "Epoch: 128, TR loss: 0.1231363364661726, VAL loss: (0.10216347558009618, 0.19886262366112242), VL auc: 0.9755128249626879 VL ap: 0.8035483085245712 VL f1: 0.776595744680851 \n",
      "Epoch: 130, TR loss: 0.12262618652582752, VAL loss: (0.10216472847685988, 0.20012644503978974), VL auc: 0.9774912359862553 VL ap: 0.7942558901302786 VL f1: 0.7446808510638298 \n",
      "Epoch: 132, TR loss: 0.12307610794194353, VAL loss: (0.10212082730615314, 0.2024150401987928), VL auc: 0.9797097173533257 VL ap: 0.836448458614665 VL f1: 0.776595744680851 \n",
      "Epoch: 134, TR loss: 0.12221620833237733, VAL loss: (0.10216983133453983, 0.211789293492094), VL auc: 0.9814943366538245 VL ap: 0.8404499943292112 VL f1: 0.7978723404255319 \n",
      "Epoch: 136, TR loss: 0.12219132224076878, VAL loss: (0.10217207161352128, 0.2148230329472968), VL auc: 0.9826165931993567 VL ap: 0.8511279386829816 VL f1: 0.7978723404255319 \n",
      "Epoch: 138, TR loss: 0.122838487592446, VAL loss: (0.10206915153764784, 0.20375805712760764), VL auc: 0.9778094015017412 VL ap: 0.8374157848505288 VL f1: 0.7872340425531915 \n",
      "Epoch: 140, TR loss: 0.12310679730281372, VAL loss: (0.10210262296509652, 0.20792043969986287), VL auc: 0.9755851353071165 VL ap: 0.8426006326411893 VL f1: 0.7872340425531915 \n",
      "Epoch: 142, TR loss: 0.12231852362917967, VAL loss: (0.10205548583586103, 0.209838664278071), VL auc: 0.9804819918318235 VL ap: 0.8514246044867284 VL f1: 0.7978723404255319 \n",
      "Epoch: 144, TR loss: 0.12243298321918594, VAL loss: (0.10205414166847217, 0.2056248441655585), VL auc: 0.9787638980481994 VL ap: 0.8390243085512966 VL f1: 0.7978723404255319 \n",
      "Epoch: 146, TR loss: 0.12303548505711194, VAL loss: (0.10216601456294182, 0.20772217689676487), VL auc: 0.981922413892842 VL ap: 0.8398898596723645 VL f1: 0.7978723404255319 \n",
      "Epoch: 148, TR loss: 0.12288535440662475, VAL loss: (0.10201224845151917, 0.209471824321341), VL auc: 0.9819686925132762 VL ap: 0.8595804347971769 VL f1: 0.8191489361702128 \n",
      "Epoch: 150, TR loss: 0.1227738748738069, VAL loss: (0.10204915497340607, 0.21270155399403673), VL auc: 0.980910069070841 VL ap: 0.8482066458023353 VL f1: 0.8191489361702128 \n",
      "Epoch: 152, TR loss: 0.12330738726749878, VAL loss: (0.10206414824792262, 0.20777221436196186), VL auc: 0.979336595976074 VL ap: 0.8338621258797243 VL f1: 0.7978723404255319 \n",
      "Epoch: 154, TR loss: 0.1232228328127868, VAL loss: (0.10206615620167635, 0.21466851741709608), VL auc: 0.9841842814665696 VL ap: 0.8534174685394403 VL f1: 0.8297872340425532 \n",
      "Epoch: 156, TR loss: 0.12255411246099486, VAL loss: (0.10228469126765141, 0.2080221784875748), VL auc: 0.9804183587287263 VL ap: 0.8303385499256135 VL f1: 0.7978723404255319 \n",
      "Epoch: 158, TR loss: 0.12284331244694154, VAL loss: (0.10199937099604064, 0.20192811844196726), VL auc: 0.9785672139113534 VL ap: 0.8240716969376802 VL f1: 0.776595744680851 \n",
      "Epoch: 160, TR loss: 0.12352900941524796, VAL loss: (0.10196107882011708, 0.1910606140786029), VL auc: 0.9752640773778534 VL ap: 0.806866356618927 VL f1: 0.7553191489361702 \n",
      "Epoch: 162, TR loss: 0.12286857197930051, VAL loss: (0.10196303698989345, 0.19755797690533577), VL auc: 0.9774160332280495 VL ap: 0.8264012538347647 VL f1: 0.776595744680851 \n",
      "Epoch: 164, TR loss: 0.12279322657349409, VAL loss: (0.10192652044249592, 0.1967415505267204), VL auc: 0.9777689077088613 VL ap: 0.8116861321370059 VL f1: 0.7553191489361702 \n",
      "Epoch: 166, TR loss: 0.1230463521829494, VAL loss: (0.10209619253468682, 0.20042703506794382), VL auc: 0.9782548332234217 VL ap: 0.8213634437367242 VL f1: 0.776595744680851 \n",
      "Epoch: 168, TR loss: 0.12206341131493209, VAL loss: (0.10209035121467594, 0.21663663742390085), VL auc: 0.9817893628590932 VL ap: 0.8458054966480846 VL f1: 0.7978723404255319 \n",
      "Epoch: 170, TR loss: 0.12239399600484123, VAL loss: (0.10208604490063383, 0.21183042323335688), VL auc: 0.9827120428540026 VL ap: 0.8396515613727555 VL f1: 0.7978723404255319 \n",
      "Epoch: 172, TR loss: 0.12241038258497002, VAL loss: (0.10210192598941341, 0.21310574957664977), VL auc: 0.9812745132067614 VL ap: 0.849099502015009 VL f1: 0.7978723404255319 \n",
      "Epoch: 174, TR loss: 0.12281199570561062, VAL loss: (0.10211290335642248, 0.20711708068847656), VL auc: 0.9781854152927703 VL ap: 0.8284904242559195 VL f1: 0.776595744680851 \n",
      "Epoch: 176, TR loss: 0.12247961103128442, VAL loss: (0.10216423063708623, 0.21204930163444358), VL auc: 0.9805514097624751 VL ap: 0.8339239531165401 VL f1: 0.7872340425531915 \n",
      "Epoch: 178, TR loss: 0.1225405416241243, VAL loss: (0.10219234198963856, 0.20686490485008727), VL auc: 0.9766640056459918 VL ap: 0.8071939619424878 VL f1: 0.7659574468085105 \n",
      "Epoch: 180, TR loss: 0.12281410191144304, VAL loss: (0.1019771839367948, 0.2084180547835979), VL auc: 0.9803258014878578 VL ap: 0.8472281993688388 VL f1: 0.7978723404255319 \n",
      "Epoch: 182, TR loss: 0.1232763692787682, VAL loss: (0.10233872347775201, 0.2149432973658785), VL auc: 0.9806728911411151 VL ap: 0.8388805139119938 VL f1: 0.7872340425531915 \n",
      "Epoch: 184, TR loss: 0.12286905745227607, VAL loss: (0.10216313538958419, 0.19939875095448595), VL auc: 0.9767016070250947 VL ap: 0.8288517710276527 VL f1: 0.776595744680851 \n",
      "Epoch: 186, TR loss: 0.12365446310094683, VAL loss: (0.10216533418191782, 0.20485046062063664), VL auc: 0.9777168442608726 VL ap: 0.8204978174843628 VL f1: 0.7872340425531915 \n",
      "Epoch: 188, TR loss: 0.12308983562392926, VAL loss: (0.10205327044886828, 0.20061854098705537), VL auc: 0.9788795945992851 VL ap: 0.8193052307092791 VL f1: 0.776595744680851 \n",
      "Epoch: 190, TR loss: 0.12260575184796409, VAL loss: (0.1021012041217416, 0.20416507315128407), VL auc: 0.9770400194370206 VL ap: 0.816885130821614 VL f1: 0.7659574468085105 \n",
      "Epoch: 192, TR loss: 0.12217865513051425, VAL loss: (0.10206438057315032, 0.21716905147471327), VL auc: 0.9831401200930201 VL ap: 0.8485911006755995 VL f1: 0.7978723404255319 \n",
      "Epoch: 194, TR loss: 0.12266067751353708, VAL loss: (0.10207854411471078, 0.21163818684030086), VL auc: 0.9805167007971493 VL ap: 0.8467203514653958 VL f1: 0.7978723404255319 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 196, TR loss: 0.12281151023263506, VAL loss: (0.10211488641818753, 0.19964104510368186), VL auc: 0.9797646732150914 VL ap: 0.825205174308745 VL f1: 0.776595744680851 \n",
      "Epoch: 198, TR loss: 0.12332134648274978, VAL loss: (0.1019838052057844, 0.20271680710163523), VL auc: 0.9786366318420048 VL ap: 0.8443087460497242 VL f1: 0.7978723404255319 \n",
      "Epoch: 200, TR loss: 0.1221834052968597, VAL loss: (0.10205095549392078, 0.2138205183313248), VL auc: 0.9811327849316811 VL ap: 0.8559854063129014 VL f1: 0.8085106382978723 \n",
      "Epoch: 202, TR loss: 0.1226794391768386, VAL loss: (0.10194078355201111, 0.2043106809575507), VL auc: 0.9787320814966507 VL ap: 0.8439179628269905 VL f1: 0.7978723404255319 \n",
      "Epoch: 204, TR loss: 0.12315504584776905, VAL loss: (0.1020234249544377, 0.206927035717254), VL auc: 0.9805571945900293 VL ap: 0.8517656602669912 VL f1: 0.8085106382978723 \n",
      "Epoch: 206, TR loss: 0.12333024931024, VAL loss: (0.10193319979279244, 0.19240795297825591), VL auc: 0.9795303876991428 VL ap: 0.830786106757828 VL f1: 0.7872340425531915 \n",
      "Epoch: 208, TR loss: 0.12235992327077215, VAL loss: (0.10209194430195163, 0.213491216618964), VL auc: 0.9813323614823043 VL ap: 0.8527231808887349 VL f1: 0.8085106382978723 \n",
      "Epoch: 210, TR loss: 0.12182351297685695, VAL loss: (0.1020971301329272, 0.22093996088555518), VL auc: 0.9844098897411868 VL ap: 0.8683708252212056 VL f1: 0.8191489361702128 \n",
      "Epoch: 212, TR loss: 0.12223443970981324, VAL loss: (0.10204323068009957, 0.20711068904146235), VL auc: 0.9806034732104637 VL ap: 0.841267464082341 VL f1: 0.7978723404255319 \n",
      "Epoch: 214, TR loss: 0.12217531657020543, VAL loss: (0.10208549727688282, 0.21802155514980884), VL auc: 0.982761213888214 VL ap: 0.8564401902845027 VL f1: 0.8085106382978723 \n",
      "Epoch: 216, TR loss: 0.1223738227355031, VAL loss: (0.10206101185734859, 0.21824668316130943), VL auc: 0.9835074566427174 VL ap: 0.8574514320566758 VL f1: 0.8085106382978723 \n",
      "Epoch: 218, TR loss: 0.12247393473187791, VAL loss: (0.10208429416409649, 0.21909336333579205), VL auc: 0.9832644938854372 VL ap: 0.8748551440895767 VL f1: 0.8297872340425532 \n",
      "Epoch: 220, TR loss: 0.12315557613363466, VAL loss: (0.10187465383541072, 0.19917051842872133), VL auc: 0.9814162414818415 VL ap: 0.8457762691347779 VL f1: 0.7978723404255319 \n",
      "Epoch: 222, TR loss: 0.12293172081019793, VAL loss: (0.10207873495329069, 0.21074051552630485), VL auc: 0.9822868580287623 VL ap: 0.8432486128035421 VL f1: 0.7978723404255319 \n",
      "Epoch: 224, TR loss: 0.12172499183807896, VAL loss: (0.10212869317457687, 0.22058495054853725), VL auc: 0.9815232607915959 VL ap: 0.8544588806090002 VL f1: 0.8085106382978723 \n",
      "Epoch: 226, TR loss: 0.12244724118703729, VAL loss: (0.1020411895370276, 0.22121510607130984), VL auc: 0.9845255862922728 VL ap: 0.8772918287660432 VL f1: 0.8297872340425532 \n",
      "Epoch: 228, TR loss: 0.12315104256292447, VAL loss: (0.10194393653724426, 0.20979461264103016), VL auc: 0.9803894345909547 VL ap: 0.8438436718563296 VL f1: 0.8085106382978723 \n",
      "Epoch: 230, TR loss: 0.12366362733696235, VAL loss: (0.10222874237442224, 0.2122174932601604), VL auc: 0.9838661159510835 VL ap: 0.8589700343919829 VL f1: 0.8191489361702128 \n",
      "Epoch: 232, TR loss: 0.12259354780424009, VAL loss: (0.10199040988011487, 0.21086607588098405), VL auc: 0.9805745490726923 VL ap: 0.8544863812941516 VL f1: 0.8085106382978723 \n",
      "Epoch: 234, TR loss: 0.12295063184979962, VAL loss: (0.10198456026277443, 0.20390015460075217), VL auc: 0.9788738097717308 VL ap: 0.831577619898725 VL f1: 0.776595744680851 \n",
      "Epoch: 236, TR loss: 0.12285184183368132, VAL loss: (0.10207315085049619, 0.21894692360086643), VL auc: 0.9834843173325003 VL ap: 0.8703294677242388 VL f1: 0.8085106382978723 \n",
      "Epoch: 238, TR loss: 0.1220329833625872, VAL loss: (0.10201119469066493, 0.21803837634147483), VL auc: 0.9842884083625467 VL ap: 0.8678763502882865 VL f1: 0.8297872340425532 \n",
      "Epoch: 240, TR loss: 0.12260388464421194, VAL loss: (0.10191125335610386, 0.20544169811492272), VL auc: 0.9841958511216782 VL ap: 0.85118321103068 VL f1: 0.8085106382978723 \n",
      "Epoch: 242, TR loss: 0.12291865785274796, VAL loss: (0.10196519429557929, 0.2130972273806308), VL auc: 0.9817922552728704 VL ap: 0.8637481127248999 VL f1: 0.8191489361702128 \n",
      "Epoch: 244, TR loss: 0.12248825245024933, VAL loss: (0.10195220897481648, 0.21273574423282704), VL auc: 0.984135110432358 VL ap: 0.8581681148997696 VL f1: 0.8085106382978723 \n",
      "Epoch: 246, TR loss: 0.12298644481776569, VAL loss: (0.10187205677125816, 0.20525707082545502), VL auc: 0.9767218539215347 VL ap: 0.8532570499794414 VL f1: 0.8085106382978723 \n",
      "Epoch: 248, TR loss: 0.12311813496399672, VAL loss: (0.10207077781424177, 0.20335225856050532), VL auc: 0.9776185021924497 VL ap: 0.8194797663552683 VL f1: 0.7872340425531915 \n",
      "Epoch: 250, TR loss: 0.12270155433807896, VAL loss: (0.10191199181843479, 0.2036993960116772), VL auc: 0.9765830180602317 VL ap: 0.8338143672209194 VL f1: 0.7872340425531915 \n",
      "Epoch: 252, TR loss: 0.12268943992013506, VAL loss: (0.10187706835831294, 0.20780885980484334), VL auc: 0.9831314428516886 VL ap: 0.8472485051584729 VL f1: 0.8085106382978723 \n",
      "Epoch: 254, TR loss: 0.12388244867908331, VAL loss: (0.10213115748145646, 0.21605357717960438), VL auc: 0.9831835062996772 VL ap: 0.846831465776806 VL f1: 0.8085106382978723 \n",
      "Epoch: 256, TR loss: 0.12267012556452292, VAL loss: (0.10197306846133258, 0.21434812342866938), VL auc: 0.9850462207721588 VL ap: 0.8631097355190743 VL f1: 0.8191489361702128 \n",
      "Epoch: 258, TR loss: 0.12222931610271735, VAL loss: (0.10196269679938146, 0.2105649582883145), VL auc: 0.9813786401027385 VL ap: 0.8505076577327 VL f1: 0.8085106382978723 \n",
      "Epoch: 260, TR loss: 0.12227354642519808, VAL loss: (0.10201114490668757, 0.2108128324468085), VL auc: 0.9813207918271958 VL ap: 0.8457978151852605 VL f1: 0.8085106382978723 \n",
      "Epoch: 262, TR loss: 0.1225869304341425, VAL loss: (0.10197757391128416, 0.20640509179297914), VL auc: 0.9788738097717309 VL ap: 0.8395248371550726 VL f1: 0.7872340425531915 \n",
      "Epoch: 264, TR loss: 0.1219758842718467, VAL loss: (0.10194792755276305, 0.2127767319374896), VL auc: 0.979637407008897 VL ap: 0.8499809122437584 VL f1: 0.8085106382978723 \n",
      "Epoch: 266, TR loss: 0.1223121826052374, VAL loss: (0.10198675905510808, 0.20317767528777428), VL auc: 0.9776792428817697 VL ap: 0.8119231967562661 VL f1: 0.776595744680851 \n",
      "Epoch: 268, TR loss: 0.12259377933750536, VAL loss: (0.10201543462607056, 0.21630837054962807), VL auc: 0.9836462925040206 VL ap: 0.8414359297158674 VL f1: 0.7978723404255319 \n",
      "Epoch: 270, TR loss: 0.12256509908787246, VAL loss: (0.102013385185669, 0.2092498616969332), VL auc: 0.9799729270070459 VL ap: 0.8419270860677507 VL f1: 0.7872340425531915 \n",
      "Epoch: 272, TR loss: 0.12256323935293532, VAL loss: (0.10196502005165851, 0.19922573008435837), VL auc: 0.9790010759779251 VL ap: 0.8243690383824781 VL f1: 0.7872340425531915 \n",
      "Epoch: 274, TR loss: 0.12257696703492107, VAL loss: (0.10195838218800979, 0.19871190253724444), VL auc: 0.9752149063436418 VL ap: 0.8156102865957412 VL f1: 0.7659574468085105 \n",
      "Epoch: 276, TR loss: 0.12309994093063586, VAL loss: (0.10192489416590199, 0.19005586745891165), VL auc: 0.976293776682517 VL ap: 0.803041379455893 VL f1: 0.7446808510638298 \n",
      "Epoch: 278, TR loss: 0.12188864851254666, VAL loss: (0.10198085965379028, 0.21424553242135555), VL auc: 0.9846644221535756 VL ap: 0.8615558058036777 VL f1: 0.8085106382978723 \n",
      "Epoch: 280, TR loss: 0.12285791398028328, VAL loss: (0.10205921133683388, 0.20926854965534616), VL auc: 0.9804154663149494 VL ap: 0.8369295452462511 VL f1: 0.8085106382978723 \n",
      "Epoch: 282, TR loss: 0.12257956618254405, VAL loss: (0.10193596280353623, 0.2022372509570832), VL auc: 0.9776705656404383 VL ap: 0.8360159514084103 VL f1: 0.7978723404255319 \n",
      "Epoch: 284, TR loss: 0.12298799086247247, VAL loss: (0.10181617425666548, 0.19947922483403632), VL auc: 0.972909652563257 VL ap: 0.8450009458455947 VL f1: 0.7978723404255319 \n",
      "Epoch: 286, TR loss: 0.12287228398035976, VAL loss: (0.1019606722509686, 0.2097553293755714), VL auc: 0.9825645297513682 VL ap: 0.8604734589734325 VL f1: 0.8085106382978723 \n",
      "Epoch: 288, TR loss: 0.12282588770152655, VAL loss: (0.1017910416454255, 0.19868217630589263), VL auc: 0.9801956428678861 VL ap: 0.8470483454743473 VL f1: 0.8085106382978723 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 290, TR loss: 0.12272284046085337, VAL loss: (0.1017750609886912, 0.20330506182731467), VL auc: 0.9790328925294736 VL ap: 0.8476544452255212 VL f1: 0.8085106382978723 \n",
      "Epoch: 292, TR loss: 0.12288541415714482, VAL loss: (0.10196682886950278, 0.21103700678399268), VL auc: 0.9794696470098225 VL ap: 0.844722638860065 VL f1: 0.8191489361702128 \n",
      "Epoch: 294, TR loss: 0.12320124793741205, VAL loss: (0.10218579539661501, 0.21112245194455412), VL auc: 0.9800018511448174 VL ap: 0.8370995203050917 VL f1: 0.7978723404255319 \n",
      "Epoch: 296, TR loss: 0.12281177910997537, VAL loss: (0.10207268620004079, 0.2040798714820375), VL auc: 0.9790473545983595 VL ap: 0.8253199201851014 VL f1: 0.776595744680851 \n",
      "Epoch: 298, TR loss: 0.12292833743699905, VAL loss: (0.10188602947423872, 0.19560270106538813), VL auc: 0.9789027339095022 VL ap: 0.8501121251491585 VL f1: 0.7978723404255319 \n",
      "Epoch: 300, TR loss: 0.12273328186423535, VAL loss: (0.10194888174566255, 0.20474997987138463), VL auc: 0.9832297849201116 VL ap: 0.8510919605277599 VL f1: 0.7978723404255319 \n",
      "Stopping at epoch 224, TR loss: 0.12172499183807896, VAL loss: 0.10212869317457687, VAL auc: 0.9815232607915959,TS loss: 0.10212869317457687, TS auc: 0.9815232607915959 TS ap: 0.8544588806090002 TS f1: 0.8085106382978723\n",
      "Final training run 5: 0.9815232607915959, (0.9815232607915959, 0.8544588806090002, 0.8085106382978723)\n",
      "F1:0.825531914893617AUC:0.9839621440884848\n"
     ]
    }
   ],
   "source": [
    "risk_assesser.risk_assessment(runExperiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-youth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
