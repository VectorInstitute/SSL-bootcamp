{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal, Union\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from deepod.models.icl import ICLNet\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ModelSummary,\n",
    "    RichProgressBar,\n",
    ")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_tabular.config import DataConfig\n",
    "from pytorch_tabular.tabular_datamodule import TabularDatamodule\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryF1Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2048\n",
    "SEED = 42\n",
    "\n",
    "if os.environ.get(\"USER\") is not None and os.environ.get(\"SLURM_JOB_ID\") is not None:\n",
    "    CKPT_DIR = f\"/checkpoint/{os.environ['USER']}/{os.environ['SLURM_JOB_ID']}\"\n",
    "    DATA_DIR = \"/ssd003/projects/aieng/public/ssl_bootcamp_resources/datasets/bank_account_fraud_dataset/\"\n",
    "else:  # not on Vector's cluster\n",
    "    CKPT_DIR = \"checkpoint\"\n",
    "    DATA_DIR = \"data\"  # NOTE: download the data from https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022\n",
    "\n",
    "seed_everything(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"Base.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = [\n",
    "    \"income\",\n",
    "    \"name_email_similarity\",\n",
    "    \"prev_address_months_count\",\n",
    "    \"current_address_months_count\",\n",
    "    \"customer_age\",\n",
    "    \"days_since_request\",\n",
    "    \"intended_balcon_amount\",\n",
    "    \"zip_count_4w\",\n",
    "    \"velocity_6h\",\n",
    "    \"velocity_24h\",\n",
    "    \"velocity_4w\",\n",
    "    \"bank_branch_count_8w\",\n",
    "    \"date_of_birth_distinct_emails_4w\",\n",
    "    \"credit_risk_score\",\n",
    "    \"bank_months_count\",\n",
    "    \"proposed_credit_limit\",\n",
    "    \"session_length_in_minutes\",\n",
    "    \"device_distinct_emails_8w\",\n",
    "    \"device_fraud_count\",\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"payment_type\",\n",
    "    \"employment_status\",\n",
    "    \"housing_status\",\n",
    "    \"device_os\",\n",
    "    \"email_is_free\",\n",
    "    \"phone_home_valid\",\n",
    "    \"phone_mobile_valid\",\n",
    "    \"has_other_cards\",\n",
    "    \"foreign_request\",\n",
    "    \"keep_alive_session\",\n",
    "    \"source\",\n",
    "]\n",
    "\n",
    "target_col = [\"fraud_bool\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud_bool</th>\n",
       "      <th>income</th>\n",
       "      <th>name_email_similarity</th>\n",
       "      <th>prev_address_months_count</th>\n",
       "      <th>current_address_months_count</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>days_since_request</th>\n",
       "      <th>intended_balcon_amount</th>\n",
       "      <th>zip_count_4w</th>\n",
       "      <th>velocity_6h</th>\n",
       "      <th>...</th>\n",
       "      <th>phone_mobile_valid_0</th>\n",
       "      <th>phone_mobile_valid_1</th>\n",
       "      <th>has_other_cards_0</th>\n",
       "      <th>has_other_cards_1</th>\n",
       "      <th>foreign_request_0</th>\n",
       "      <th>foreign_request_1</th>\n",
       "      <th>keep_alive_session_0</th>\n",
       "      <th>keep_alive_session_1</th>\n",
       "      <th>source_INTERNET</th>\n",
       "      <th>source_TELEAPP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.192631</td>\n",
       "      <td>-1</td>\n",
       "      <td>104</td>\n",
       "      <td>40</td>\n",
       "      <td>0.030592</td>\n",
       "      <td>-1.044454</td>\n",
       "      <td>804</td>\n",
       "      <td>7905.711839</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.322989</td>\n",
       "      <td>148</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>1.628119</td>\n",
       "      <td>-1.409803</td>\n",
       "      <td>3306</td>\n",
       "      <td>5391.470463</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.879403</td>\n",
       "      <td>-1</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>0.018563</td>\n",
       "      <td>34.692760</td>\n",
       "      <td>1522</td>\n",
       "      <td>8063.102636</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.762112</td>\n",
       "      <td>-1</td>\n",
       "      <td>189</td>\n",
       "      <td>20</td>\n",
       "      <td>0.015352</td>\n",
       "      <td>94.661055</td>\n",
       "      <td>1418</td>\n",
       "      <td>8092.641762</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.697452</td>\n",
       "      <td>-1</td>\n",
       "      <td>321</td>\n",
       "      <td>20</td>\n",
       "      <td>2.655916</td>\n",
       "      <td>9.908499</td>\n",
       "      <td>951</td>\n",
       "      <td>6169.630036</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fraud_bool  income  name_email_similarity  prev_address_months_count  \\\n",
       "999995           0     0.6               0.192631                         -1   \n",
       "999996           0     0.8               0.322989                        148   \n",
       "999997           0     0.8               0.879403                         -1   \n",
       "999998           0     0.9               0.762112                         -1   \n",
       "999999           0     0.2               0.697452                         -1   \n",
       "\n",
       "        current_address_months_count  customer_age  days_since_request  \\\n",
       "999995                           104            40            0.030592   \n",
       "999996                             9            50            1.628119   \n",
       "999997                            30            20            0.018563   \n",
       "999998                           189            20            0.015352   \n",
       "999999                           321            20            2.655916   \n",
       "\n",
       "        intended_balcon_amount  zip_count_4w  velocity_6h  ...  \\\n",
       "999995               -1.044454           804  7905.711839  ...   \n",
       "999996               -1.409803          3306  5391.470463  ...   \n",
       "999997               34.692760          1522  8063.102636  ...   \n",
       "999998               94.661055          1418  8092.641762  ...   \n",
       "999999                9.908499           951  6169.630036  ...   \n",
       "\n",
       "        phone_mobile_valid_0  phone_mobile_valid_1  has_other_cards_0  \\\n",
       "999995                 False                  True               True   \n",
       "999996                 False                  True               True   \n",
       "999997                 False                  True               True   \n",
       "999998                  True                 False               True   \n",
       "999999                 False                  True               True   \n",
       "\n",
       "        has_other_cards_1  foreign_request_0  foreign_request_1  \\\n",
       "999995              False               True              False   \n",
       "999996              False               True              False   \n",
       "999997              False               True              False   \n",
       "999998              False               True              False   \n",
       "999999              False               True              False   \n",
       "\n",
       "        keep_alive_session_0  keep_alive_session_1  source_INTERNET  \\\n",
       "999995                 False                  True             True   \n",
       "999996                  True                 False             True   \n",
       "999997                  True                 False             True   \n",
       "999998                 False                  True             True   \n",
       "999999                  True                 False             True   \n",
       "\n",
       "        source_TELEAPP  \n",
       "999995           False  \n",
       "999996           False  \n",
       "999997           False  \n",
       "999998           False  \n",
       "999999           False  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode categorical columns in for loop\n",
    "df_concat_lst = []\n",
    "for col in categorical_cols:\n",
    "    one_hot = pd.get_dummies(df[col], prefix=col)\n",
    "    df = df.drop(col, axis=1)\n",
    "    df_concat_lst.append(one_hot)\n",
    "\n",
    "df = pd.concat([df] + df_concat_lst, axis=1)\n",
    "\n",
    "df.tail()  # noqa: B018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['payment_type_AA', 'payment_type_AB', 'payment_type_AC', 'payment_type_AD', 'payment_type_AE', 'employment_status_CA', 'employment_status_CB', 'employment_status_CC', 'employment_status_CD', 'employment_status_CE', 'employment_status_CF', 'employment_status_CG', 'housing_status_BA', 'housing_status_BB', 'housing_status_BC', 'housing_status_BD', 'housing_status_BE', 'housing_status_BF', 'housing_status_BG', 'device_os_linux', 'device_os_macintosh', 'device_os_other', 'device_os_windows', 'device_os_x11', 'email_is_free_0', 'email_is_free_1', 'phone_home_valid_0', 'phone_home_valid_1', 'phone_mobile_valid_0', 'phone_mobile_valid_1', 'has_other_cards_0', 'has_other_cards_1', 'foreign_request_0', 'foreign_request_1', 'keep_alive_session_0', 'keep_alive_session_1', 'source_INTERNET', 'source_TELEAPP']\n"
     ]
    }
   ],
   "source": [
    "# update the categorical columns\n",
    "categorical_cols = [\n",
    "    col for col in df.columns if col.startswith(tuple(categorical_cols))\n",
    "]\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_df, test_df = train_test_split(\n",
    "#     df, test_size=0.2, random_state=SEED, stratify=df[target_col]\n",
    "# )\n",
    "\n",
    "# drop the month column, but keep it for later\n",
    "month = df[\"month\"].values\n",
    "df = df.drop(columns=[\"month\"])\n",
    "\n",
    "# select the first 6 months of data for training and the last 2 months for testing\n",
    "train_df = df[month < 6]\n",
    "test_df = df[month >= 6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cfg = DataConfig(\n",
    "    target=target_col,\n",
    "    continuous_cols=continuous_cols,\n",
    "    categorical_cols=categorical_cols,\n",
    "    continuous_feature_transform=\"quantile_normal\",\n",
    "    normalize_continuous_features=True,\n",
    "    validation_split=0.3,\n",
    ")\n",
    "data_cfg = OmegaConf.structured(data_cfg)\n",
    "\n",
    "cfg = OmegaConf.merge(\n",
    "    OmegaConf.to_container(data_cfg),\n",
    "    OmegaConf.create(\n",
    "        {\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"task\": \"classification\",\n",
    "            \"num_workers\": 4,\n",
    "            \"pin_memory\": True,\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "\n",
    "data = TabularDatamodule(train=train_df, test=test_df, config=cfg, seed=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLModule(LightningModule):\n",
    "    \"\"\"Lightning module for ICL model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features : int\n",
    "        The number of features in the data.\n",
    "    rep_dim : int, optional, default=128\n",
    "        The dimension of the representation.\n",
    "    hidden_dims : tuple, optional, default=(128, 64)\n",
    "        The number of hidden units in each layer of the encoders.\n",
    "    lr : float, optional, default=1e-3\n",
    "        The learning rate.\n",
    "    act : str, optional, default=\"LeakyReLU\"\n",
    "        The activation function.\n",
    "    bias : bool, optional, default=False\n",
    "        Whether to use bias in the encoders.\n",
    "    kernel_size : int or str, optional, default=\"auto\"\n",
    "        The kernel size of the sliding window.\n",
    "    temperature : float, optional, default=0.01\n",
    "        The temperature parameter for the softmax function.\n",
    "    max_negatives : int, optional, default=1000\n",
    "        The maximum number of negative samples.\n",
    "    contamination : float, optional, default=0.01\n",
    "        The contamination rate.\n",
    "    train_method : str, optional, default=\"loe_hard\"\n",
    "        The training method. Choose from \"blind\", \"loe_hard\", \"loe_soft\", \"refine\", or \"gt\".\n",
    "        - \"blind\": train the model without any labels.\n",
    "        - \"loe_hard\": train the model with the hard labels from the LOE.\n",
    "        - \"loe_soft\": train the model with the soft labels from the LOE.\n",
    "        - \"refine\": use the loss on the normal samples to rank the samples and\n",
    "            select the top (1 - contamination) as anomalies.\n",
    "        - \"gt\": use the ground truth labels to train the model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        rep_dim: int = 128,\n",
    "        hidden_dims: tuple = (128, 64),\n",
    "        lr: float = 1e-3,\n",
    "        act: str = \"LeakyReLU\",\n",
    "        bias: bool = False,\n",
    "        kernel_size: Union[int, str] = \"auto\",\n",
    "        temperature: float = 0.01,\n",
    "        max_negatives: int = 1000,\n",
    "        contamination: float = 0.01,\n",
    "        train_method: Literal[\n",
    "            \"blind\", \"loe_hard\", \"loe_soft\", \"refine\", \"gt\"\n",
    "        ] = \"loe_hard\",\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the ICL lightning module.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        assert train_method in [\n",
    "            \"blind\",\n",
    "            \"loe_hard\",\n",
    "            \"loe_soft\",\n",
    "            \"refine\",\n",
    "            \"gt\",\n",
    "        ], \"Unknown training method. Please choose from 'blind', 'loe_hard', 'loe_soft', 'refine', or 'gt'.\"\n",
    "\n",
    "        if n_features < 3:\n",
    "            raise ValueError(\n",
    "                \"ICL model cannot handle the data that have less than three features.\"\n",
    "            )\n",
    "\n",
    "        if kernel_size == \"auto\":\n",
    "            if n_features < 40:\n",
    "                kernel_size = 2\n",
    "            elif 40 <= n_features <= 160:\n",
    "                kernel_size = 10\n",
    "            elif 160 < n_features <= 240:\n",
    "                kernel_size = n_features - 150\n",
    "            elif 240 < n_features <= 480:\n",
    "                kernel_size = n_features - 200\n",
    "            else:\n",
    "                kernel_size = n_features - 400\n",
    "\n",
    "        print(\"Number of features:\", n_features)\n",
    "        print(\"Kernel size:\", kernel_size)\n",
    "\n",
    "        self.model = ICLNet(\n",
    "            n_features=n_features,\n",
    "            kernel_size=kernel_size,\n",
    "            hidden_dims=hidden_dims,\n",
    "            rep_dim=rep_dim,\n",
    "            activation=act,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        self.contamination = contamination\n",
    "        self.train_method = train_method\n",
    "        self.max_negatives = max_negatives\n",
    "        self.tau = temperature\n",
    "\n",
    "        self.auroc = BinaryAUROC(thresholds=10)\n",
    "        self.f1_score = BinaryF1Score(threshold=0.5)\n",
    "\n",
    "    def _cal_logit(self, query: torch.Tensor, pos: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Calculate the logits for the query and positive samples.\"\"\"\n",
    "        n_pos = query.shape[1]\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # get negatives\n",
    "        negative_index = np.random.choice(\n",
    "            np.arange(n_pos), min(self.max_negatives, n_pos), replace=False\n",
    "        )\n",
    "        negative = pos.permute(0, 2, 1)[:, :, negative_index]\n",
    "\n",
    "        pos_multiplication = (query * pos).sum(dim=2).unsqueeze(2)\n",
    "\n",
    "        neg_multiplication = torch.matmul(query, negative)  # [batch_size, n_neg, n_neg]\n",
    "\n",
    "        # Removal of the diagonals\n",
    "        identity_matrix = torch.eye(n_pos).unsqueeze(0).to(self.device)\n",
    "        identity_matrix = identity_matrix.repeat(batch_size, 1, 1)\n",
    "        identity_matrix = identity_matrix[:, :, negative_index]\n",
    "\n",
    "        neg_multiplication.masked_fill_(identity_matrix == 1, -float(\"inf\"))\n",
    "\n",
    "        logit = torch.cat((pos_multiplication, neg_multiplication), dim=2)\n",
    "        logit = torch.div(logit, self.tau)\n",
    "\n",
    "        return logit\n",
    "\n",
    "    def forward(self, batch: dict, loss_reduction: str = \"mean\") -> torch.Tensor:\n",
    "        \"\"\"Run forward pass.\"\"\"\n",
    "        # combine batch['continuous'] and batch['categorical'] into x\n",
    "        cont_x = batch[\"continuous\"]\n",
    "        cat_x = batch[\"categorical\"]\n",
    "        x = torch.cat((cont_x, cat_x), dim=1)\n",
    "\n",
    "        # positives are sub-vectors, query are their complements\n",
    "        # shape: [batch_size, n_features - 1, rep_dim]\n",
    "        positives, query = self.model(x.float())\n",
    "\n",
    "        logit = self._cal_logit(query, positives)\n",
    "        logit = logit.permute(0, 2, 1)  # [batch_size, n_pos, n_pos - 1]\n",
    "\n",
    "        correct_class = torch.zeros(\n",
    "            (logit.shape[0], logit.shape[2]), dtype=torch.long\n",
    "        ).to(self.device)\n",
    "\n",
    "        loss_n = F.cross_entropy(logit.float(), correct_class, reduction=loss_reduction)\n",
    "\n",
    "        complement_of_logit = torch.where(logit == float(\"-inf\"), logit, 1 - logit)\n",
    "        loss_a = F.cross_entropy(\n",
    "            complement_of_logit, correct_class, reduction=loss_reduction\n",
    "        )\n",
    "\n",
    "        return loss_n.float().mean(dim=1), loss_a.float().mean(dim=1)\n",
    "\n",
    "    def _compute_mean_loss(\n",
    "        self, loss_n: torch.Tensor, loss_a: torch.Tensor, labels: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the loss based on the training method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss_n : torch.Tensor\n",
    "            The loss on the normal samples.\n",
    "        loss_a : torch.Tensor\n",
    "            The loss on the anomaly samples.\n",
    "        labels : torch.Tensor, optional, default=None\n",
    "            The ground truth labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            The loss.\n",
    "        \"\"\"\n",
    "        score = loss_n - loss_a\n",
    "\n",
    "        if self.train_method == \"blind\":\n",
    "            loss = loss_n.mean()\n",
    "        elif self.train_method == \"loe_hard\":\n",
    "            _, idx_n = torch.topk(\n",
    "                score,\n",
    "                int(score.shape[0] * (1 - self.contamination)),\n",
    "                largest=False,\n",
    "                sorted=False,\n",
    "            )\n",
    "            _, idx_a = torch.topk(\n",
    "                score,\n",
    "                int(score.shape[0] * self.contamination),\n",
    "                largest=True,\n",
    "                sorted=False,\n",
    "            )\n",
    "            loss = torch.cat([loss_n[idx_n], loss_a[idx_a]], 0).mean()\n",
    "        elif self.train_method == \"loe_soft\":\n",
    "            _, idx_n = torch.topk(\n",
    "                score,\n",
    "                int(score.shape[0] * (1 - self.contamination)),\n",
    "                largest=False,\n",
    "                sorted=False,\n",
    "            )\n",
    "            _, idx_a = torch.topk(\n",
    "                score,\n",
    "                int(score.shape[0] * self.contamination),\n",
    "                largest=True,\n",
    "                sorted=False,\n",
    "            )\n",
    "            loss = torch.cat(\n",
    "                [loss_n[idx_n], 0.5 * loss_n[idx_a] + 0.5 * loss_a[idx_a]], 0\n",
    "            ).mean()\n",
    "        elif self.train_method == \"refine\":\n",
    "            _, idx_n = torch.topk(\n",
    "                loss_n,\n",
    "                int(loss_n.shape[0] * (1 - self.contamination)),\n",
    "                largest=False,\n",
    "                sorted=False,\n",
    "            )\n",
    "            loss = loss_n[idx_n].mean()\n",
    "        elif self.train_method == \"gt\" and labels is not None:\n",
    "            loss = torch.cat([loss_n[labels == 0], loss_a[labels == 1]], 0).mean()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown training method: {self.train_method}. \"\n",
    "                \"Please choose from 'blind', 'loe_hard', 'loe_soft', 'refine', \"\n",
    "                \"or 'gt'.\"\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Run training step.\"\"\"\n",
    "        loss_n, loss_a = self(batch, loss_reduction=\"none\")\n",
    "        loss = self._compute_mean_loss(loss_n, loss_a, labels=batch[\"target\"].squeeze())\n",
    "\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int) -> None:\n",
    "        \"\"\"Run validation step.\"\"\"\n",
    "        loss_n, loss_a = self(batch, loss_reduction=\"none\")\n",
    "        loss = self._compute_mean_loss(loss_n, loss_a, labels=batch[\"target\"].squeeze())\n",
    "\n",
    "        self.log(\"val/loss\", loss, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        targets = batch[\"target\"].squeeze()\n",
    "\n",
    "        # get the predictions\n",
    "        if self.contamination is None:\n",
    "            self.contamination = targets.float().mean().item()\n",
    "\n",
    "        scores = loss_n - loss_a\n",
    "        threshold = torch.quantile(scores, q=(1 - self.contamination)).to(self.device)\n",
    "        preds = (scores > threshold).long().ravel()\n",
    "\n",
    "        # compute the metrics\n",
    "        self.auroc(preds, targets)\n",
    "        self.log(\"val/auroc\", self.auroc, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        self.f1_score(preds, targets)\n",
    "        self.log(\"val/f1_score\", self.f1_score, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def test_step(self, batch: dict, batch_idx: int) -> None:\n",
    "        \"\"\"Run test step.\"\"\"\n",
    "        targets = batch[\"target\"].squeeze()\n",
    "\n",
    "        loss_n, loss_a = self(batch, loss_reduction=\"none\")\n",
    "        loss = self._compute_mean_loss(loss_n, loss_a, labels=targets)\n",
    "\n",
    "        self.log(\"test/loss\", loss, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        # get the predictions\n",
    "        if self.contamination is None:\n",
    "            self.contamination = targets.float().mean().item()\n",
    "\n",
    "        scores = loss_n - loss_a\n",
    "        threshold = torch.quantile(scores, q=(1 - self.contamination)).to(self.device)\n",
    "        preds = (scores > threshold).long().ravel()\n",
    "\n",
    "        # compute the metrics\n",
    "        self.auroc(preds, targets)\n",
    "        self.log(\"test/auroc\", self.auroc, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        self.f1_score(preds, targets)\n",
    "        self.log(\"test/f1_score\", self.f1_score, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        \"\"\"Configure the optimizer.\"\"\"\n",
    "        return torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.hparams.lr, weight_decay=1e-5\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICL + LOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env \"WANDB_NOTEBOOK_NAME\" \"ICL_LOE_BAF.ipynb\"\n",
    "%env WANDB_SILENT=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training_method in [\"blind\", \"refine\", \"loe_hard\", \"loe_soft\", \"gt\"]:\n",
    "    model = ICLModule(\n",
    "        n_features=(len(continuous_cols) + len(categorical_cols)),\n",
    "        train_method=training_method,\n",
    "        hidden_dims=(192, 96),\n",
    "        rep_dim=128,\n",
    "        bias=True,\n",
    "        lr=1e-2,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        precision=\"16\",\n",
    "        benchmark=True,\n",
    "        max_epochs=-1,\n",
    "        log_every_n_steps=10,\n",
    "        default_root_dir=os.path.join(CKPT_DIR, training_method),\n",
    "        callbacks=[\n",
    "            RichProgressBar(),\n",
    "            ModelSummary(max_depth=5),\n",
    "            EarlyStopping(monitor=\"val/loss\", patience=25, stopping_threshold=1e-2),\n",
    "            ModelCheckpoint(\n",
    "                dirpath=CKPT_DIR,\n",
    "                filename=\"ICL-BAF-{training_method}-{epoch:02d}\",\n",
    "                monitor=\"val/loss\",\n",
    "                save_top_k=3,\n",
    "                mode=\"min\",\n",
    "            ),\n",
    "        ],\n",
    "        logger=WandbLogger(\n",
    "            project=\"icl-baf\",\n",
    "            name=f\"icl-baf-{training_method}\",\n",
    "            entity=\"vector-ssl-bootcamp\",\n",
    "            save_dir=CKPT_DIR,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=data)\n",
    "    trainer.test(model, dataloaders=data.test_dataloader())\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(model, datamodule=data)\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(adf: pd.DataFrame, month: np.ndarray):\n",
    "    \"\"\"Yield indices of the training and validation sets (split by month).\"\"\"\n",
    "    for i in range(6):\n",
    "        train_idx = adf[(month != i) & (month < 6)].index.values\n",
    "        val_idx = adf[month == i].index.values\n",
    "        yield train_idx, val_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_idx, v_idx in get_indices(df, month):\n",
    "    print(len(t_idx), len(v_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_prop = df[target_col].mean().values[0]\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 11),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000, 100),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 0.7),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1e-2),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1, 4),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 10),\n",
    "    }\n",
    "\n",
    "    params.update(\n",
    "        {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"auc\",\n",
    "            \"scale_pos_weight\": (1 - pos_prop) / pos_prop,\n",
    "        }\n",
    "    )\n",
    "    model = XGBClassifier(\n",
    "        **params,\n",
    "        random_state=SEED,\n",
    "        early_stopping_rounds=20,\n",
    "    )\n",
    "    model.fit(\n",
    "        train_df[continuous_cols + categorical_cols],\n",
    "        train_df[target_col],\n",
    "        eval_set=[(test_df[continuous_cols + categorical_cols], test_df[target_col])],\n",
    "        verbose=False,\n",
    "    )\n",
    "    pred = model.predict_proba(test_df[continuous_cols + categorical_cols])[:, 1]\n",
    "    auc = roc_auc_score(test_df[target_col], pred)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-21 14:19:13,020] A new study created in memory with name: no-name-511eaa94-fbdf-4b24-af75-92fdd0ad2908\n",
      "[I 2023-09-21 14:20:22,573] Trial 0 finished with value: 0.893791366913425 and parameters: {'max_depth': 5, 'n_estimators': 1000, 'min_child_weight': 37, 'subsample': 0.7993292420985183, 'learning_rate': 0.10922148812330114, 'colsample_bylevel': 0.5779972601681014, 'colsample_bytree': 0.5290418060840998, 'gamma': 0.008661762795987894, 'lambda': 2.8033450352296265, 'alpha': 7.080754970702675}. Best is trial 0 with value: 0.893791366913425.\n",
      "[I 2023-09-21 14:21:55,628] Trial 1 finished with value: 0.894775287060374 and parameters: {'max_depth': 1, 'n_estimators': 1000, 'min_child_weight': 42, 'subsample': 0.6061695553391381, 'learning_rate': 0.12728565879529838, 'colsample_bylevel': 0.5917022549267169, 'colsample_bytree': 0.6521211214797689, 'gamma': 0.005247569068758062, 'lambda': 2.2958350559263474, 'alpha': 2.9123622790663997}. Best is trial 1 with value: 0.894775287060374.\n",
      "[I 2023-09-21 14:22:35,903] Trial 2 finished with value: 0.8776524942447793 and parameters: {'max_depth': 7, 'n_estimators': 200, 'min_child_weight': 15, 'subsample': 0.6831809216468459, 'learning_rate': 0.319254428252083, 'colsample_bylevel': 0.8925879806965068, 'colsample_bytree': 0.5998368910791798, 'gamma': 0.005142349241791732, 'lambda': 2.7772437065861273, 'alpha': 0.4645994821587052}. Best is trial 1 with value: 0.894775287060374.\n",
      "[I 2023-09-21 14:23:08,595] Trial 3 finished with value: 0.871317610505364 and parameters: {'max_depth': 7, 'n_estimators': 200, 'min_child_weight': 4, 'subsample': 0.9744427686266666, 'learning_rate': 0.6759427668318607, 'colsample_bylevel': 0.9041986740582306, 'colsample_bytree': 0.6523068845866853, 'gamma': 0.0009767301633426986, 'lambda': 3.052699079536471, 'alpha': 4.401580922146639}. Best is trial 1 with value: 0.894775287060374.\n",
      "[I 2023-09-21 14:24:14,608] Trial 4 finished with value: 0.8946214568465398 and parameters: {'max_depth': 2, 'n_estimators': 500, 'min_child_weight': 2, 'subsample': 0.954660201039391, 'learning_rate': 0.18115339932019586, 'colsample_bylevel': 0.831261142176991, 'colsample_bytree': 0.6558555380447055, 'gamma': 0.005200685011097896, 'lambda': 2.640130838029839, 'alpha': 1.8486260698097179}. Best is trial 1 with value: 0.894775287060374.\n",
      "[I 2023-09-21 14:25:07,226] Trial 5 finished with value: 0.8642582323384894 and parameters: {'max_depth': 11, 'n_estimators': 800, 'min_child_weight': 47, 'subsample': 0.9474136752138245, 'learning_rate': 0.4185340061679715, 'colsample_bylevel': 0.9609371175115584, 'colsample_bytree': 0.5442462510259598, 'gamma': 0.0019598366643628277, 'lambda': 1.1356818667316142, 'alpha': 3.2533707745995675}. Best is trial 1 with value: 0.894775287060374.\n",
      "[I 2023-09-21 14:26:04,819] Trial 6 finished with value: 0.8892661992305159 and parameters: {'max_depth': 5, 'n_estimators': 300, 'min_child_weight': 42, 'subsample': 0.6783766633467947, 'learning_rate': 0.19666134743606967, 'colsample_bylevel': 0.7713480415791243, 'colsample_bytree': 0.5704621124873813, 'gamma': 0.008021971785570589, 'lambda': 1.2236519310393126, 'alpha': 9.868870677311513}. Best is trial 1 with value: 0.894775287060374.\n",
      "[I 2023-09-21 14:26:51,367] Trial 7 finished with value: 0.856684805575638 and parameters: {'max_depth': 9, 'n_estimators': 200, 'min_child_weight': 1, 'subsample': 0.9077307142274171, 'learning_rate': 0.4948030721198935, 'colsample_bylevel': 0.8645035840204937, 'colsample_bytree': 0.8856351733429728, 'gamma': 0.0007404557768943864, 'lambda': 2.0753971856328177, 'alpha': 1.1587790083453446}. Best is trial 1 with value: 0.894775287060374.\n",
      "[I 2023-09-21 14:27:47,512] Trial 8 finished with value: 0.8661035640371464 and parameters: {'max_depth': 10, 'n_estimators': 700, 'min_child_weight': 17, 'subsample': 0.5317791751430119, 'learning_rate': 0.2176945153777464, 'colsample_bylevel': 0.6625916610133735, 'colsample_bytree': 0.864803089169032, 'gamma': 0.0063755783379774176, 'lambda': 3.6616382277289796, 'alpha': 4.722202030126977}. Best is trial 1 with value: 0.894775287060374.\n",
      "[I 2023-09-21 14:28:20,292] Trial 9 finished with value: 0.892781109343762 and parameters: {'max_depth': 2, 'n_estimators': 800, 'min_child_weight': 39, 'subsample': 0.7806385987847482, 'learning_rate': 0.5396793162963931, 'colsample_bylevel': 0.7468977981821954, 'colsample_bytree': 0.7613664146909971, 'gamma': 0.004275415908175313, 'lambda': 1.0762573802322857, 'alpha': 1.0790034807903452}. Best is trial 1 with value: 0.894775287060374.\n",
      "[I 2023-09-21 14:28:34,800] Trial 10 finished with value: 0.8155515227870989 and parameters: {'max_depth': 1, 'n_estimators': 1000, 'min_child_weight': 29, 'subsample': 0.5089809378074097, 'learning_rate': 0.011417113054797326, 'colsample_bylevel': 0.5193625999805914, 'colsample_bytree': 0.9798853729727098, 'gamma': 0.00932616405469645, 'lambda': 2.0043855286697085, 'alpha': 6.437162994964573}. Best is trial 1 with value: 0.894775287060374.\n",
      "[I 2023-09-21 14:30:33,631] Trial 11 finished with value: 0.8960114321002781 and parameters: {'max_depth': 3, 'n_estimators': 500, 'min_child_weight': 27, 'subsample': 0.8708441534127347, 'learning_rate': 0.11869637821439707, 'colsample_bylevel': 0.659143125062404, 'colsample_bytree': 0.677399475072992, 'gamma': 0.0032616943531588046, 'lambda': 2.231820237292188, 'alpha': 2.8417706167222225}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:33:13,627] Trial 12 finished with value: 0.8904139583791951 and parameters: {'max_depth': 3, 'n_estimators': 500, 'min_child_weight': 28, 'subsample': 0.8460121022184429, 'learning_rate': 0.018337362675944463, 'colsample_bylevel': 0.6375519837163963, 'colsample_bytree': 0.7085638213231334, 'gamma': 0.003067036239403114, 'lambda': 2.0767206053797236, 'alpha': 2.975184651349456}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:34:07,690] Trial 13 finished with value: 0.891816639507684 and parameters: {'max_depth': 4, 'n_estimators': 400, 'min_child_weight': 20, 'subsample': 0.6976592221728126, 'learning_rate': 0.28067516921773394, 'colsample_bylevel': 0.6575722883148079, 'colsample_bytree': 0.7604553456772329, 'gamma': 0.00303446997442257, 'lambda': 1.7759339655916042, 'alpha': 2.6441072190362545}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:35:34,387] Trial 14 finished with value: 0.8939943738390043 and parameters: {'max_depth': 1, 'n_estimators': 700, 'min_child_weight': 34, 'subsample': 0.5964534538182333, 'learning_rate': 0.10178169297757574, 'colsample_bylevel': 0.5236016266859548, 'colsample_bytree': 0.6896868765954901, 'gamma': 0.003646510740084875, 'lambda': 2.512127318058861, 'alpha': 0.2504706927190994}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:37:14,503] Trial 15 finished with value: 0.8948083001598239 and parameters: {'max_depth': 3, 'n_estimators': 600, 'min_child_weight': 50, 'subsample': 0.8334598055272349, 'learning_rate': 0.09750635112050486, 'colsample_bylevel': 0.5862302405550639, 'colsample_bytree': 0.5010585027893083, 'gamma': 0.006732721174928694, 'lambda': 3.1644099740963436, 'alpha': 3.6971764308377413}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:38:01,295] Trial 16 finished with value: 0.8925560229203495 and parameters: {'max_depth': 4, 'n_estimators': 600, 'min_child_weight': 46, 'subsample': 0.8737726154925136, 'learning_rate': 0.25369731982856175, 'colsample_bylevel': 0.7152260414518055, 'colsample_bytree': 0.5356678431497429, 'gamma': 0.006896747355373421, 'lambda': 3.387632330805947, 'alpha': 3.7875486252239194}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:38:45,448] Trial 17 finished with value: 0.8945029603613803 and parameters: {'max_depth': 3, 'n_estimators': 400, 'min_child_weight': 50, 'subsample': 0.8338962247920462, 'learning_rate': 0.32669059993438854, 'colsample_bylevel': 0.5889814515889142, 'colsample_bytree': 0.504234277457642, 'gamma': 0.006560441783777019, 'lambda': 3.1766288990866443, 'alpha': 5.536749787200099}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:40:14,086] Trial 18 finished with value: 0.8920750372743763 and parameters: {'max_depth': 6, 'n_estimators': 600, 'min_child_weight': 8, 'subsample': 0.8850814620364958, 'learning_rate': 0.0837268428281267, 'colsample_bylevel': 0.6966644122744321, 'colsample_bytree': 0.5873089928184836, 'gamma': 0.00211480182728042, 'lambda': 3.9932229481790222, 'alpha': 1.6777707245720634}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:41:11,903] Trial 19 finished with value: 0.8937620522093649 and parameters: {'max_depth': 3, 'n_estimators': 400, 'min_child_weight': 23, 'subsample': 0.7547946326637386, 'learning_rate': 0.17625425677190482, 'colsample_bylevel': 0.6200392464368686, 'colsample_bytree': 0.6120026549920408, 'gamma': 0.009813131054452067, 'lambda': 2.9245445499483633, 'alpha': 2.1250466623398814}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:42:44,838] Trial 20 finished with value: 0.8893213803555066 and parameters: {'max_depth': 8, 'n_estimators': 100, 'min_child_weight': 33, 'subsample': 0.8125868592962723, 'learning_rate': 0.05266732451594951, 'colsample_bylevel': 0.5035645265696091, 'colsample_bytree': 0.5086792017528843, 'gamma': 0.007611985597876604, 'lambda': 2.509529687098839, 'alpha': 3.9705307608309326}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:44:09,841] Trial 21 finished with value: 0.8950938810896589 and parameters: {'max_depth': 1, 'n_estimators': 900, 'min_child_weight': 43, 'subsample': 0.7414280700143904, 'learning_rate': 0.1515328382142938, 'colsample_bylevel': 0.5814812128925255, 'colsample_bytree': 0.6451106796040591, 'gamma': 0.005699059057242647, 'lambda': 2.4216752077026067, 'alpha': 2.830641109746152}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:45:12,550] Trial 22 finished with value: 0.8951774512454966 and parameters: {'max_depth': 2, 'n_estimators': 800, 'min_child_weight': 50, 'subsample': 0.7501006600185389, 'learning_rate': 0.13675819377577295, 'colsample_bylevel': 0.556351223571144, 'colsample_bytree': 0.6136954670147452, 'gamma': 0.005999590570266618, 'lambda': 2.33684547422399, 'alpha': 3.7156308685886223}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:46:18,590] Trial 23 finished with value: 0.8956713404494505 and parameters: {'max_depth': 2, 'n_estimators': 900, 'min_child_weight': 45, 'subsample': 0.7430731316535911, 'learning_rate': 0.13793945073707908, 'colsample_bylevel': 0.5526112444266462, 'colsample_bytree': 0.6278311480236229, 'gamma': 0.005954287727299381, 'lambda': 2.258178824973645, 'alpha': 2.397033128885984}. Best is trial 11 with value: 0.8960114321002781.\n",
      "[I 2023-09-21 14:47:15,310] Trial 24 finished with value: 0.8955235430121081 and parameters: {'max_depth': 2, 'n_estimators': 800, 'min_child_weight': 32, 'subsample': 0.7529951468334517, 'learning_rate': 0.24171200139862895, 'colsample_bylevel': 0.5466104004290245, 'colsample_bytree': 0.7032821825703729, 'gamma': 0.004354221606022277, 'lambda': 1.6809543212932248, 'alpha': 2.1529483736954123}. Best is trial 11 with value: 0.8960114321002781.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    load_if_exists=True,\n",
    ")\n",
    "study.optimize(objective, callbacks=[optuna.study.MaxTrialsCallback(25)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3,\n",
       " 'n_estimators': 500,\n",
       " 'min_child_weight': 27,\n",
       " 'subsample': 0.8708441534127347,\n",
       " 'learning_rate': 0.11869637821439707,\n",
       " 'colsample_bylevel': 0.659143125062404,\n",
       " 'colsample_bytree': 0.677399475072992,\n",
       " 'gamma': 0.0032616943531588046,\n",
       " 'lambda': 2.231820237292188,\n",
       " 'alpha': 2.8417706167222225,\n",
       " 'objective': 'binary:logistic',\n",
       " 'eval_metric': 'auc',\n",
       " 'scale_pos_weight': 89.67005168192946}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "best_params.update(\n",
    "    {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"scale_pos_weight\": (1 - pos_prop) / pos_prop,\n",
    "    }\n",
    ")\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(alpha=2.8417706167222225, base_score=None, booster=None,\n",
       "              callbacks=None, colsample_bylevel=0.659143125062404,\n",
       "              colsample_bynode=None, colsample_bytree=0.677399475072992,\n",
       "              early_stopping_rounds=20, enable_categorical=False,\n",
       "              eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.0032616943531588046, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              lambda=2.231820237292188, learning_rate=0.11869637821439707,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=27, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(alpha=2.8417706167222225, base_score=None, booster=None,\n",
       "              callbacks=None, colsample_bylevel=0.659143125062404,\n",
       "              colsample_bynode=None, colsample_bytree=0.677399475072992,\n",
       "              early_stopping_rounds=20, enable_categorical=False,\n",
       "              eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.0032616943531588046, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              lambda=2.231820237292188, learning_rate=0.11869637821439707,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=27, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(alpha=2.8417706167222225, base_score=None, booster=None,\n",
       "              callbacks=None, colsample_bylevel=0.659143125062404,\n",
       "              colsample_bynode=None, colsample_bytree=0.677399475072992,\n",
       "              early_stopping_rounds=20, enable_categorical=False,\n",
       "              eval_metric='auc', feature_types=None,\n",
       "              gamma=0.0032616943531588046, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              lambda=2.231820237292188, learning_rate=0.11869637821439707,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=27, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = XGBClassifier(**best_params, random_state=SEED, early_stopping_rounds=20)\n",
    "\n",
    "base_model.fit(\n",
    "    train_df[continuous_cols + categorical_cols],\n",
    "    train_df[target_col],\n",
    "    eval_set=[(test_df[continuous_cols + categorical_cols], test_df[target_col])],\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUC': 0.8960114321002781}\n"
     ]
    }
   ],
   "source": [
    "prediction = base_model.predict_proba(test_df[continuous_cols + categorical_cols])[:, 1]\n",
    "target = test_df[target_col]\n",
    "auc_score = {\"AUC\": roc_auc_score(target, prediction)}\n",
    "print(auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl_env",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
