{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./img/img_0.PNG\"  width=\"1000\" height=\"240\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook demonstrate an apllication of SimMTM, a simple self-supervised learning framework for time series modeling. Self-supervised learning is a learning paradigm that allows model to learn a good representation from the input data itself. The learned representation will be beneficial to some downstream tasks such as forecasting, classification and outlier detection. \n",
    "\n",
    "Self-supervised learning has a lof of success and achieves state-of-the-art performance in some domains, especially in the image domain. In this demo, we will show a self-supervised learning method, SimMTM, in the time-series domain. SimMTM adopts both masked modeling and contrastive modeling to learn a good representation of the input data. By using the learned representation and finetuning it, we achieve a significant improvement compared to the model without self-supervised learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shamvinc/ssl_time_series/SSL-Bootcamp/masked_modelling\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import Tensor, nn\n",
    "from torch.nn.modules import BatchNorm1d, Dropout, Linear, MultiheadAttention\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Masked Modeling\n",
    "\n",
    "Self-supervision via a ‘pretext task’ on input data combined with finetuning on labeled data is widely used for improving model performance in language and computer\n",
    "vision. One of the popular self-supervision tasks on language data is masked modeling. Masking modeling is to mask some of the input entries randomly and predict those masked entries by using unmasked entries. By masked modeling, the model can learn the relationship through different features and different timesteps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/img_1.PNG\"  width=\"900\" height=\"240\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/img_2.PNG\"  width=\"900\" height=\"240\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking Choice\n",
    "### Random Masking\n",
    "\n",
    "Random Masking is not a good choice to learn a good representation because the model can simply learn to take the average from the neighbour values. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./img/img_3.PNG\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Masking\n",
    "\n",
    "Instead, we choose to use the geometric masking method, which is to mask a sequence of the input data randomly. The length of the sequence is followed by a geometric distribution. In this case, the model requires to recover a masked sequence from other unmasked input data. We suggest the expected length of a masked sequence is a half of the whole time series sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def geom_noise_mask_single(L: int, lm: int, masking_ratio: float) -> Tensor:\n",
    "    \"\"\"\n",
    "    Randomly create a boolean mask of length `L`, consisting of subsequences of average length lm, masking with 0s a `masking_ratio`\n",
    "    proportion of the sequence L. The length of masking subsequences and intervals follow a geometric distribution.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        L: length of mask and sequence to be masked\n",
    "        lm: average length of masking subsequences (streaks of 0s)\n",
    "        masking_ratio: proportion of L to be masked.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        (L,) boolean numpy array intended to mask ('drop') with 0s a sequence of length L\n",
    "    \"\"\"\n",
    "    keep_mask = np.ones(L, dtype=bool)\n",
    "    p_m = 1 / lm  # probability of each masking sequence stopping. parameter of geometric distribution.\n",
    "    p_u = (\n",
    "        p_m * masking_ratio / (1 - masking_ratio)\n",
    "    )  # probability of each unmasked sequence stopping. parameter of geometric distribution.\n",
    "    p = [p_m, p_u]\n",
    "\n",
    "    # Start in state 0 with masking_ratio probability\n",
    "    state = int(np.random.rand() > masking_ratio)  # state 0 means masking, 1 means not masking\n",
    "    for i in range(L):\n",
    "        keep_mask[i] = state  # here it happens that state and masking value corresponding to state are identical\n",
    "        if np.random.rand() < p[state]:\n",
    "            state = 1 - state\n",
    "\n",
    "    return keep_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SimMTM ultilizes both contrastive learning and mask modeling to learn the data representation.\n",
    "## 1 - Contrastive Learning\n",
    "\n",
    "when we mask the input time series data, we create many masked views of the input data. We expect that the distance between two views of the same time series sequence is minimized while maximizing the distance between two different sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./img/img_5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The contrastive loss is the following: (Eq. 8 in the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"./img/img_6.PNG\"/><center/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def demo_contrastive_loss(s: Tensor, batch_size: int, tau: float = 0.05) -> Tensor:\n",
    "    s = s.squeeze(-1)\n",
    "\n",
    "    B = s.shape[0]\n",
    "    v = s.reshape(B, -1)\n",
    "\n",
    "    norm_v = torch.norm(v, p=2, dim=-1).unsqueeze(-1)\n",
    "    v = v / norm_v\n",
    "    u = torch.transpose(v, 0, 1)\n",
    "\n",
    "    R = torch.matmul(v, u)\n",
    "\n",
    "    R = torch.exp(R / tau)  # (batch + mask size) x (batch + mask size)\n",
    "\n",
    "    # number of masks\n",
    "    M = B // batch_size\n",
    "    mask = torch.eye(batch_size, device=R.device).repeat_interleave(M, dim=0).repeat_interleave(M, dim=1)\n",
    "\n",
    "    denom = R * (torch.ones_like(R) - torch.eye(R.shape[0], device=R.device))\n",
    "\n",
    "    denom = R.sum(-1).unsqueeze(-1)\n",
    "\n",
    "    loss = torch.log(R / denom)\n",
    "\n",
    "    loss = (loss * (mask - torch.eye(R.shape[0], device=R.device))).sum(1) / (M - 1)  # except no masked unit\n",
    "    loss = loss.mean(0)\n",
    "\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Masked Modeling\n",
    "\n",
    "SimMTM proposes to recover a time serie by the weighted sum of multiple masked points, which eases the reconstruction task by assembling ruined but complementary temporal variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./img/img_4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    \"\"\"Learnable Positional Encoding.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=1024).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1024) -> None:\n",
    "        \"\"\"Init of LearnablePositionalEncoding.\"\"\"\n",
    "        super(LearnablePositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Each position gets its own embedding\n",
    "        # Since indices are always 0 ... max_len, we don't have to do a look-up\n",
    "        self.pe = nn.Parameter(torch.empty(max_len, 1, d_model))  # requires_grad automatically set to True\n",
    "        nn.init.uniform_(self.pe, -0.02, 0.02)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Forward function of LearnablePositionalEncoding.\n",
    "\n",
    "        Args:\n",
    "        ----\n",
    "        x: The sequence fed to the positional encoder model.\n",
    "\n",
    "        \"\"\"\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerBatchNormEncoderLayer(nn.modules.Module):\n",
    "    \"\"\"Transformer encoder layer block.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        d_model: the number of expected features in the input.\n",
    "        nhead: the number of heads in the multiheadattention models.\n",
    "        dim_feedforward: the dimension of the feedforward network model.\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"gelu\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init of TransformerBatchNormEncoderLayer.\"\"\"\n",
    "        super(TransformerBatchNormEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = BatchNorm1d(d_model, eps=1e-5)  # normalizes each feature across batch samples and time steps\n",
    "        self.norm2 = BatchNorm1d(d_model, eps=1e-5)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = F.gelu\n",
    "\n",
    "    def __setstate__(self, state: dict) -> None:\n",
    "        \"\"\"Set state for batch statistics.\"\"\"\n",
    "        if \"activation\" not in state:\n",
    "            state[\"activation\"] = F.relu\n",
    "        super(TransformerBatchNormEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: Tensor,\n",
    "        src_mask: Optional[Tensor] = None,\n",
    "        src_key_padding_mask: Optional[Tensor] = None,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "        ----\n",
    "        src:\n",
    "            The sequence to the encoder layer (required).\n",
    "        src_mask:\n",
    "            The mask for the src sequence (optional).\n",
    "        src_key_padding_mask:\n",
    "            The mask for the src keys per batch (optional).\n",
    "\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)  # (seq_len, batch_size, d_model)\n",
    "        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        # src = src.reshape([src.shape[0], -1])  # (batch_size, seq_length * d_model)\n",
    "        src = self.norm1(src)\n",
    "        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)  # (seq_len, batch_size, d_model)\n",
    "        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        src = self.norm2(src)\n",
    "        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer with SimMTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DemoSimMTMTransformerEncoder(nn.Module):\n",
    "    r\"\"\"SimMTM Transformer\n",
    "    Args:\n",
    "        max_len: max input sequence length.\n",
    "        feat_dim: input feature dimensions.\n",
    "        out_len: output sequence length.\n",
    "        out_dim: output feature dimensions.\n",
    "        d_model: representation dimensions.\n",
    "        n_heads: number of transformer heads.\n",
    "        num_layers: number of transformer layers.\n",
    "        dim_feedforward: hidden layer dimensions.\n",
    "        dropout: dropout rate.\n",
    "        temporal_unit: default number of masked views.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_len: int,\n",
    "        feat_dim: int,\n",
    "        out_len: int,\n",
    "        out_dim: int,\n",
    "        d_model: int = 16,\n",
    "        n_heads: int = 4,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 32,\n",
    "        dropout: float = 0.2,\n",
    "        temporal_unit: int = 3,\n",
    "    ) -> None:\n",
    "        super(DemoSimMTMTransformerEncoder, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.tau = 0.05\n",
    "        self.mask_length = max_len // 2\n",
    "        self.mask_rate = 0.5\n",
    "\n",
    "        self.project_inp = nn.Linear(feat_dim, d_model)\n",
    "        self.projector_layer = nn.Linear(max_len, 1)\n",
    "        self.pos_enc1 = LearnablePositionalEncoding(d_model, dropout=dropout, max_len=max_len)\n",
    "        self.pos_enc2 = LearnablePositionalEncoding(d_model, dropout=dropout, max_len=out_len)\n",
    "\n",
    "        self.act = F.gelu\n",
    "\n",
    "        # encoder_layer = nn.TransformerEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout, activation='gelu')\n",
    "        encoder_layer = TransformerBatchNormEncoderLayer(\n",
    "            d_model, self.n_heads, dim_feedforward, dropout, activation=\"gelu\"\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, feat_dim)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout1d(dropout)\n",
    "\n",
    "        # self.predict_layer1 = nn.Conv1d(d_model, 512, 5, stride=1)\n",
    "        self.predict_layer1 = nn.Linear(max_len, out_len)\n",
    "        self.predict_layer2 = nn.Linear(d_model, out_dim)\n",
    "        # self.bn = nn.BatchNorm1d(d_model)\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        self.temporal_unit = temporal_unit\n",
    "\n",
    "        self.w1 = torch.nn.parameter.Parameter(data=torch.ones(1), requires_grad=True)\n",
    "        self.w2 = torch.nn.parameter.Parameter(data=torch.ones(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, X: Tensor, N: Optional[int] = None) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Reconstruct the input and create the projected output of X.\n",
    "\n",
    "        Args:\n",
    "        ----\n",
    "            X: (batch_size, seq_length, feat_dim) torch tensor of original input\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "            output: (batch_size, seq_length, feat_dim)\n",
    "            s: (batch_size, d_model, 1)\n",
    "        \"\"\"\n",
    "        _x = X\n",
    "\n",
    "        # Create masked views of the input X\n",
    "        if N is None:\n",
    "            N = self.temporal_unit\n",
    "        for _i in range(N):\n",
    "            mask = geom_noise_mask_single(X.shape[0] * X.shape[1] * X.shape[2], self.mask_length, self.mask_rate)\n",
    "            mask = mask.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
    "            mask = torch.from_numpy(mask).to(X.device)\n",
    "            x_masked = mask * X\n",
    "            _x = torch.cat([_x, x_masked], axis=-1)  # [batch_size, seq_length, feat_dim * temporal_unit]\n",
    "\n",
    "        _x = _x.reshape(X.shape[0] * (N + 1), X.shape[1], X.shape[2])\n",
    "\n",
    "        inp = _x.permute(1, 0, 2)\n",
    "        inp = self.project_inp(inp) * np.sqrt(\n",
    "            self.d_model\n",
    "        )  # [seq_length, batch_size, d_model] project input vectors to d_model dimensional space\n",
    "        inp = self.pos_enc1(inp)  # add positional encoding\n",
    "\n",
    "        output = self.transformer_encoder(inp)  # (seq_length, batch_size, d_model)\n",
    "        output = self.act(output)  # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n",
    "        output = self.dropout1(output)\n",
    "\n",
    "        z_hat, _s = self.project(output, self.tau, N)\n",
    "        # Most probably defining a Linear(d_model,feat_dim) vectorizes the operation over (seq_length, batch_size).\n",
    "        output = self.output_layer(z_hat)  # (batch_size, seq_length, feat_dim)\n",
    "\n",
    "        return output, _s\n",
    "\n",
    "    def project(self, z: Tensor, tau: float, N: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Output a weighted average of z.\n",
    "\n",
    "        Args:\n",
    "        ----\n",
    "            X: (batch_size, seq_length, feat_dim) torch tensor of original input\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "            z_hat: (batch_size, seq_length, d_model)\n",
    "            s: (batch_size, d_model, 1)\n",
    "        \"\"\"\n",
    "        _z = z.transpose(1, 2)  # [batch_size, d_model, seq_length]\n",
    "        _s = s = self.projector_layer(_z)  # [batch_size, d_model, 1]\n",
    "\n",
    "        if self.training:\n",
    "            mask = torch.ones(1, self.d_model, 1).to(z.device)\n",
    "            mask = self.dropout3(mask)\n",
    "            s = s * mask\n",
    "            s = s + torch.randn(s.shape).to(z.device) * 1e-2\n",
    "\n",
    "        s = s.squeeze(-1)\n",
    "        B = s.shape[0]\n",
    "        v = s.reshape(B, -1)\n",
    "\n",
    "        norm_v = torch.norm(v, p=2, dim=-1).unsqueeze(-1)\n",
    "        v = v / norm_v\n",
    "        u = torch.transpose(v, 0, 1)\n",
    "\n",
    "        R = torch.matmul(v, u)\n",
    "\n",
    "        R = torch.exp(R / tau)  # (batch + mask size) x (batch + mask size)\n",
    "        R = R * (\n",
    "            torch.ones_like(R) - torch.eye(R.shape[0], device=R.device)\n",
    "        )  # zero out the weight of no masked component\n",
    "        R = R / R.sum(-1).unsqueeze(-1)\n",
    "        M = N + 1\n",
    "        R = R[::M]  # extract every no mask unit # (batch size) x (batch + mask size)\n",
    "\n",
    "        z_hat = (R.unsqueeze(-1).unsqueeze(-1).detach() * z.unsqueeze(0)).sum(1)\n",
    "        return z_hat, _s\n",
    "\n",
    "    def predict(self, X: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Predict an output given X.\n",
    "\n",
    "        Args:\n",
    "        ----\n",
    "            z: (batch_size, seq_length, d_model) torch tensor of representations of input\n",
    "            tau: temperture of similarity matrix\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "            output: (batch_size, out_seq_len, out_dim)\n",
    "        \"\"\"\n",
    "        # permute because pytorch convention for transformers is [seq_length, batch_size, feat_dim]. padding_masks [batch_size, feat_dim]\n",
    "        inp = X.permute(1, 0, 2)\n",
    "        inp = self.project_inp(inp) * np.sqrt(\n",
    "            self.d_model\n",
    "        )  # [seq_length, batch_size, d_model] project input vectors to d_model dimensional space\n",
    "        inp = self.pos_enc1(inp)  # add positional encoding\n",
    "        # NOTE: logic for padding masks is reversed to comply with definition in MultiHeadAttention, TransformerEncoderLayer\n",
    "\n",
    "        output = self.transformer_encoder(inp)\n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n",
    "        # output = self.dropout1(output)\n",
    "\n",
    "        output = output.transpose(1, 2)  # (batch_size, d_model, seq_length)\n",
    "        output = self.predict_layer1(output)\n",
    "        # output = self.act(output)\n",
    "\n",
    "        output = output.transpose(1, 2)  # (batch_size, seq_length, d_model)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        output = self.pos_enc2(output)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        output = self.dropout2(output)\n",
    "        output = self.predict_layer2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preparation\n",
    "\n",
    "In this demo, we use a benchmask time series dataset called BeijingPM25Quality.\n",
    "This dataset is part of the Monash, UEA & UCR time series regression repository. http://tseregression.org/\n",
    "\n",
    "The goal of this dataset is to predict PM2.5 air quality in the city of Beijing. This dataset contains 17532 time series with 9 dimensions.  This includes hourly air pollutants measurments (SO2, NO2, CO and O3), temperature, pressure, dew point, rainfall and windspeed measurments from 12 nationally controlled air quality monitoring sites. The air-quality data are from the Beijing Municipal Environmental Monitoring Center. The meteorological data in each air-quality site are matched with the nearest weather station from the China Meteorological Administration. The time period is from March 1st, 2013 to February 28th, 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./datasets/BeijingPM25Quality/train_x.csv\", index_col=0)\n",
    "test_data = pd.read_csv(\"./datasets/BeijingPM25Quality/test_x.csv\", index_col=0)\n",
    "_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>-18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.2</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.5</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>1024.5</td>\n",
       "      <td>-19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1025.2</td>\n",
       "      <td>-19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>27.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>1026.3</td>\n",
       "      <td>-8.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>34.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>1026.2</td>\n",
       "      <td>-8.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>31.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>1025.8</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>40.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>1025.5</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>43.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>1025.2</td>\n",
       "      <td>-7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286032 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dim_0  dim_1   dim_2  dim_3  dim_4   dim_5  dim_6  dim_7  dim_8\n",
       "0        4.0    7.0   300.0   77.0   -0.7  1023.0  -18.8    0.0    4.4\n",
       "0        4.0    7.0   300.0   77.0   -1.1  1023.2  -18.2    0.0    4.7\n",
       "0        5.0   10.0   300.0   73.0   -1.1  1023.5  -18.2    0.0    5.6\n",
       "0       11.0   11.0   300.0   72.0   -1.4  1024.5  -19.4    0.0    3.1\n",
       "0       12.0   12.0   300.0   72.0   -2.0  1025.2  -19.5    0.0    2.0\n",
       "...      ...    ...     ...    ...    ...     ...    ...    ...    ...\n",
       "11917   27.0   96.0  3300.0    9.0   -1.4  1026.3   -8.6    0.0    1.0\n",
       "11917   34.0   99.0  3700.0    9.0   -2.5  1026.2   -8.4    0.0    1.3\n",
       "11917   31.0   95.0  3100.0    9.0   -2.7  1025.8   -8.0    0.0    0.9\n",
       "11917   40.0   99.0  4200.0   13.0   -3.5  1025.5   -7.6    0.0    0.4\n",
       "11917   43.0  104.0  4800.0   16.0   -3.4  1025.2   -7.5    0.0    1.8\n",
       "\n",
       "[286032 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Normalization\n",
    "normalizer = StandardScaler()\n",
    "data[:] = normalizer.fit_transform(data)\n",
    "test_data[:] = normalizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.578723</td>\n",
       "      <td>-1.262339</td>\n",
       "      <td>-0.827454</td>\n",
       "      <td>0.331285</td>\n",
       "      <td>-1.354746</td>\n",
       "      <td>1.282597</td>\n",
       "      <td>-1.671897</td>\n",
       "      <td>-0.077887</td>\n",
       "      <td>2.197101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.578723</td>\n",
       "      <td>-1.262339</td>\n",
       "      <td>-0.827454</td>\n",
       "      <td>0.331285</td>\n",
       "      <td>-1.390913</td>\n",
       "      <td>1.302251</td>\n",
       "      <td>-1.626944</td>\n",
       "      <td>-0.077887</td>\n",
       "      <td>2.439034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.536462</td>\n",
       "      <td>-1.177416</td>\n",
       "      <td>-0.827454</td>\n",
       "      <td>0.261866</td>\n",
       "      <td>-1.390913</td>\n",
       "      <td>1.331731</td>\n",
       "      <td>-1.626944</td>\n",
       "      <td>-0.077887</td>\n",
       "      <td>3.164833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.282896</td>\n",
       "      <td>-1.149109</td>\n",
       "      <td>-0.827454</td>\n",
       "      <td>0.244512</td>\n",
       "      <td>-1.418038</td>\n",
       "      <td>1.430000</td>\n",
       "      <td>-1.716850</td>\n",
       "      <td>-0.077887</td>\n",
       "      <td>1.148725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.240636</td>\n",
       "      <td>-1.120801</td>\n",
       "      <td>-0.827454</td>\n",
       "      <td>0.244512</td>\n",
       "      <td>-1.472287</td>\n",
       "      <td>1.498788</td>\n",
       "      <td>-1.724342</td>\n",
       "      <td>-0.077887</td>\n",
       "      <td>0.261637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>0.393278</td>\n",
       "      <td>1.257031</td>\n",
       "      <td>1.802584</td>\n",
       "      <td>-0.848838</td>\n",
       "      <td>-1.418038</td>\n",
       "      <td>1.606883</td>\n",
       "      <td>-0.907691</td>\n",
       "      <td>-0.077887</td>\n",
       "      <td>-0.544806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>0.689105</td>\n",
       "      <td>1.341954</td>\n",
       "      <td>2.153256</td>\n",
       "      <td>-0.848838</td>\n",
       "      <td>-1.517495</td>\n",
       "      <td>1.597057</td>\n",
       "      <td>-0.892707</td>\n",
       "      <td>-0.077887</td>\n",
       "      <td>-0.302873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>0.562322</td>\n",
       "      <td>1.228724</td>\n",
       "      <td>1.627248</td>\n",
       "      <td>-0.848838</td>\n",
       "      <td>-1.535578</td>\n",
       "      <td>1.557749</td>\n",
       "      <td>-0.862738</td>\n",
       "      <td>-0.077887</td>\n",
       "      <td>-0.625450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>0.942670</td>\n",
       "      <td>1.341954</td>\n",
       "      <td>2.591596</td>\n",
       "      <td>-0.779419</td>\n",
       "      <td>-1.607911</td>\n",
       "      <td>1.528269</td>\n",
       "      <td>-0.832769</td>\n",
       "      <td>-0.077887</td>\n",
       "      <td>-1.028672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>1.069453</td>\n",
       "      <td>1.483491</td>\n",
       "      <td>3.117603</td>\n",
       "      <td>-0.727355</td>\n",
       "      <td>-1.598869</td>\n",
       "      <td>1.498788</td>\n",
       "      <td>-0.825277</td>\n",
       "      <td>-0.077887</td>\n",
       "      <td>0.100349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286032 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          dim_0     dim_1     dim_2     dim_3     dim_4     dim_5     dim_6  \\\n",
       "0     -0.578723 -1.262339 -0.827454  0.331285 -1.354746  1.282597 -1.671897   \n",
       "0     -0.578723 -1.262339 -0.827454  0.331285 -1.390913  1.302251 -1.626944   \n",
       "0     -0.536462 -1.177416 -0.827454  0.261866 -1.390913  1.331731 -1.626944   \n",
       "0     -0.282896 -1.149109 -0.827454  0.244512 -1.418038  1.430000 -1.716850   \n",
       "0     -0.240636 -1.120801 -0.827454  0.244512 -1.472287  1.498788 -1.724342   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "11917  0.393278  1.257031  1.802584 -0.848838 -1.418038  1.606883 -0.907691   \n",
       "11917  0.689105  1.341954  2.153256 -0.848838 -1.517495  1.597057 -0.892707   \n",
       "11917  0.562322  1.228724  1.627248 -0.848838 -1.535578  1.557749 -0.862738   \n",
       "11917  0.942670  1.341954  2.591596 -0.779419 -1.607911  1.528269 -0.832769   \n",
       "11917  1.069453  1.483491  3.117603 -0.727355 -1.598869  1.498788 -0.825277   \n",
       "\n",
       "          dim_7     dim_8  \n",
       "0     -0.077887  2.197101  \n",
       "0     -0.077887  2.439034  \n",
       "0     -0.077887  3.164833  \n",
       "0     -0.077887  1.148725  \n",
       "0     -0.077887  0.261637  \n",
       "...         ...       ...  \n",
       "11917 -0.077887 -0.544806  \n",
       "11917 -0.077887 -0.302873  \n",
       "11917 -0.077887 -0.625450  \n",
       "11917 -0.077887 -1.028672  \n",
       "11917 -0.077887  0.100349  \n",
       "\n",
       "[286032 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_len = 24\n",
    "out_size = 6\n",
    "out_dim = 9\n",
    "\n",
    "model = DemoSimMTMTransformerEncoder(\n",
    "    max_len=18,\n",
    "    feat_dim=data.shape[1],\n",
    "    out_len=out_size,\n",
    "    out_dim=out_dim,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    num_layers=1,\n",
    "    dim_feedforward=64,\n",
    ")\n",
    "\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "model.tau = 0.05\n",
    "\n",
    "model.mask_length = max_len // 2\n",
    "model.mask_ratio = 0.5\n",
    "model.mask_views = 3\n",
    "\n",
    "model.contrastive_views = 2\n",
    "init_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer_finetune = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9534,) (2384,) (5048,) (11918,)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_indices, val_indices = train_test_split(np.array(data.index.unique()), test_size=0.2)\n",
    "test_indices = np.array(test_data.index.unique())\n",
    "\n",
    "train_dataloader = DataLoader(train_indices, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_indices, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_indices, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(train_indices.shape, val_indices.shape, test_indices.shape, np.array(data.index.unique()).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 2.86 - MSE loss: 0.54 - Contrastive loss: 2.81: 100%|█████████████████████████████████████████████████| 149/149 [00:08<00:00, 18.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 5.66 0.66 5.59 - Validation loss: 2.51 0.59 2.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 2.61 - MSE loss: 0.47 - Contrastive loss: 2.56: 100%|█████████████████████████████████████████████████| 149/149 [00:07<00:00, 19.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 2.71 0.54 2.66 - Validation loss: 2.21 0.53 2.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 2.57 - MSE loss: 0.47 - Contrastive loss: 2.52: 100%|█████████████████████████████████████████████████| 149/149 [00:07<00:00, 18.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 2.48 0.51 2.43 - Validation loss: 2.04 0.51 1.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 2.16 - MSE loss: 0.41 - Contrastive loss: 2.12: 100%|█████████████████████████████████████████████████| 149/149 [00:06<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 2.34 0.50 2.29 - Validation loss: 1.90 0.50 1.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 2.13 - MSE loss: 0.45 - Contrastive loss: 2.08: 100%|█████████████████████████████████████████████████| 149/149 [00:07<00:00, 21.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 2.24 0.49 2.19 - Validation loss: 1.81 0.51 1.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 2.07 - MSE loss: 0.43 - Contrastive loss: 2.02: 100%|█████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 2.16 0.49 2.11 - Validation loss: 1.71 0.48 1.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 2.01 - MSE loss: 0.36 - Contrastive loss: 1.98: 100%|█████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 2.10 0.49 2.05 - Validation loss: 1.65 0.48 1.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training loss: 1.81 - MSE loss: 0.35 - Contrastive loss: 1.78: 100%|█████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training loss: 2.04 0.49 1.99 - Validation loss: 1.61 0.48 1.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 2.10 - MSE loss: 0.42 - Contrastive loss: 2.05: 100%|█████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 2.01 0.49 1.96 - Validation loss: 1.57 0.48 1.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training loss: 1.85 - MSE loss: 0.57 - Contrastive loss: 1.80: 100%|█████████████████████████████████████████████████| 149/149 [00:06<00:00, 22.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training loss: 1.97 0.48 1.92 - Validation loss: 1.54 0.51 1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training loss: 2.08 - MSE loss: 0.52 - Contrastive loss: 2.03: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training loss: 1.94 0.48 1.89 - Validation loss: 1.50 0.47 1.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Training loss: 1.85 - MSE loss: 0.43 - Contrastive loss: 1.81: 100%|████████████████████████████████████████████████| 149/149 [00:06<00:00, 21.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Training loss: 1.93 0.48 1.88 - Validation loss: 1.51 0.47 1.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Training loss: 1.72 - MSE loss: 0.43 - Contrastive loss: 1.68: 100%|████████████████████████████████████████████████| 149/149 [00:06<00:00, 23.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Training loss: 1.90 0.48 1.86 - Validation loss: 1.49 0.47 1.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Training loss: 2.06 - MSE loss: 0.63 - Contrastive loss: 2.00: 100%|████████████████████████████████████████████████| 149/149 [00:06<00:00, 22.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Training loss: 1.89 0.48 1.84 - Validation loss: 1.49 0.47 1.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Training loss: 1.78 - MSE loss: 0.36 - Contrastive loss: 1.74: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Training loss: 1.86 0.48 1.81 - Validation loss: 1.47 0.47 1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Training loss: 1.74 - MSE loss: 0.44 - Contrastive loss: 1.70: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 19.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Training loss: 1.84 0.48 1.79 - Validation loss: 1.46 0.47 1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Training loss: 1.72 - MSE loss: 0.42 - Contrastive loss: 1.68: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Training loss: 1.84 0.48 1.79 - Validation loss: 1.45 0.46 1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Training loss: 1.82 - MSE loss: 0.49 - Contrastive loss: 1.77: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Training loss: 1.82 0.48 1.78 - Validation loss: 1.44 0.46 1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Training loss: 1.69 - MSE loss: 0.40 - Contrastive loss: 1.65: 100%|████████████████████████████████████████████████| 149/149 [00:06<00:00, 23.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Training loss: 1.80 0.48 1.75 - Validation loss: 1.42 0.46 1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Training loss: 1.70 - MSE loss: 0.33 - Contrastive loss: 1.66: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Training loss: 1.79 0.48 1.75 - Validation loss: 1.41 0.46 1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Training loss: 1.84 - MSE loss: 0.43 - Contrastive loss: 1.80: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Training loss: 1.78 0.48 1.74 - Validation loss: 1.42 0.46 1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Training loss: 1.76 - MSE loss: 0.43 - Contrastive loss: 1.72: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Training loss: 1.78 0.48 1.73 - Validation loss: 1.43 0.46 1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Training loss: 1.71 - MSE loss: 0.44 - Contrastive loss: 1.66: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 19.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Training loss: 1.77 0.48 1.72 - Validation loss: 1.42 0.46 1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Training loss: 1.75 - MSE loss: 0.42 - Contrastive loss: 1.71: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Training loss: 1.77 0.48 1.72 - Validation loss: 1.41 0.46 1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Training loss: 1.61 - MSE loss: 0.58 - Contrastive loss: 1.56: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Training loss: 1.76 0.48 1.71 - Validation loss: 1.39 0.46 1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Training loss: 1.78 - MSE loss: 0.67 - Contrastive loss: 1.71: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Training loss: 1.75 0.48 1.71 - Validation loss: 1.39 0.47 1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Training loss: 1.70 - MSE loss: 0.36 - Contrastive loss: 1.67: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 21.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Training loss: 1.75 0.48 1.70 - Validation loss: 1.39 0.46 1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Training loss: 1.76 - MSE loss: 0.43 - Contrastive loss: 1.71: 100%|████████████████████████████████████████████████| 149/149 [00:06<00:00, 21.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Training loss: 1.73 0.48 1.69 - Validation loss: 1.38 0.46 1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Training loss: 1.68 - MSE loss: 0.43 - Contrastive loss: 1.64: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Training loss: 1.74 0.48 1.69 - Validation loss: 1.39 0.46 1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Training loss: 1.84 - MSE loss: 0.57 - Contrastive loss: 1.78: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Training loss: 1.72 0.48 1.68 - Validation loss: 1.36 0.45 1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Training loss: 1.67 - MSE loss: 0.50 - Contrastive loss: 1.62: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Training loss: 1.73 0.48 1.68 - Validation loss: 1.38 0.45 1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Training loss: 1.75 - MSE loss: 0.47 - Contrastive loss: 1.70: 100%|████████████████████████████████████████████████| 149/149 [00:06<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Training loss: 1.72 0.48 1.68 - Validation loss: 1.37 0.46 1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Training loss: 1.72 - MSE loss: 0.61 - Contrastive loss: 1.66: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Training loss: 1.73 0.48 1.68 - Validation loss: 1.37 0.45 1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Training loss: 1.81 - MSE loss: 0.70 - Contrastive loss: 1.74: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 19.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Training loss: 1.72 0.48 1.67 - Validation loss: 1.38 0.46 1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Training loss: 1.66 - MSE loss: 1.15 - Contrastive loss: 1.55: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 19.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Training loss: 1.72 0.48 1.67 - Validation loss: 1.37 0.46 1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Training loss: 1.69 - MSE loss: 0.46 - Contrastive loss: 1.64: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 19.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Training loss: 1.70 0.48 1.66 - Validation loss: 1.37 0.46 1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Training loss: 1.87 - MSE loss: 0.49 - Contrastive loss: 1.82: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Training loss: 1.71 0.48 1.66 - Validation loss: 1.37 0.46 1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Training loss: 1.73 - MSE loss: 0.41 - Contrastive loss: 1.69: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Training loss: 1.70 0.48 1.65 - Validation loss: 1.39 0.45 1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Training loss: 1.82 - MSE loss: 0.48 - Contrastive loss: 1.77: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 21.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Training loss: 1.70 0.47 1.65 - Validation loss: 1.35 0.45 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Training loss: 1.76 - MSE loss: 0.43 - Contrastive loss: 1.72: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Training loss: 1.70 0.48 1.65 - Validation loss: 1.36 0.45 1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Training loss: 1.85 - MSE loss: 1.27 - Contrastive loss: 1.73: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Training loss: 1.69 0.48 1.65 - Validation loss: 1.36 0.45 1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Training loss: 1.72 - MSE loss: 0.44 - Contrastive loss: 1.68: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 19.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Training loss: 1.69 0.47 1.65 - Validation loss: 1.35 0.45 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Training loss: 1.69 - MSE loss: 0.44 - Contrastive loss: 1.65: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Training loss: 1.69 0.47 1.64 - Validation loss: 1.36 0.45 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Training loss: 1.68 - MSE loss: 0.40 - Contrastive loss: 1.64: 100%|████████████████████████████████████████████████| 149/149 [00:06<00:00, 22.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Training loss: 1.69 0.47 1.64 - Validation loss: 1.35 0.45 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Training loss: 1.68 - MSE loss: 0.43 - Contrastive loss: 1.63: 100%|████████████████████████████████████████████████| 149/149 [00:06<00:00, 22.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Training loss: 1.68 0.47 1.64 - Validation loss: 1.34 0.45 1.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Training loss: 1.60 - MSE loss: 0.39 - Contrastive loss: 1.56: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Training loss: 1.69 0.47 1.64 - Validation loss: 1.35 0.45 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Training loss: 1.53 - MSE loss: 0.38 - Contrastive loss: 1.50: 100%|████████████████████████████████████████████████| 149/149 [00:06<00:00, 21.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Training loss: 1.68 0.47 1.64 - Validation loss: 1.36 0.45 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Training loss: 1.70 - MSE loss: 0.35 - Contrastive loss: 1.66: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Training loss: 1.68 0.47 1.63 - Validation loss: 1.34 0.45 1.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Training loss: 1.71 - MSE loss: 0.41 - Contrastive loss: 1.67: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 19.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Training loss: 1.68 0.47 1.63 - Validation loss: 1.34 0.45 1.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Training loss: 1.81 - MSE loss: 0.56 - Contrastive loss: 1.75: 100%|████████████████████████████████████████████████| 149/149 [00:07<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Training loss: 1.68 0.47 1.64 - Validation loss: 1.35 0.46 1.31\n",
      "Best Epoch 48 - Best Validation loss: 1.3419822454452515\n"
     ]
    }
   ],
   "source": [
    "i: int = 0\n",
    "max_epoch: int = 50\n",
    "best_loss: float = 1.0e10\n",
    "best_epoch: int = 0\n",
    "device = \"cuda\"\n",
    "loss_fn = nn.MSELoss()\n",
    "best_model = copy.deepcopy(model)\n",
    "\n",
    "\n",
    "while i < max_epoch:\n",
    "    train_loss: Dict[str, List[float]] = {\"loss\": [], \"loss_mse\": [], \"loss_con\": []}\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "\n",
    "    for IDs in progress_bar:\n",
    "        model.train()\n",
    "        X = torch.tensor(data.loc[IDs].to_numpy()).to(device)\n",
    "        X = X.float()\n",
    "\n",
    "        X = X.reshape(-1, max_len, X.shape[-1])\n",
    "        X = X[:, :18, :]\n",
    "\n",
    "        #         _X = []\n",
    "        #         idx = torch.randint(low=0, high=6, size=(X.shape[0],))\n",
    "        #         for j in range(18):\n",
    "        #             _X.append(X[torch.arange(X.shape[0]),idx+j,:].unsqueeze(1))\n",
    "\n",
    "        #         X = torch.cat(_X,dim=1)\n",
    "\n",
    "        # X = X[:, :, -1:]\n",
    "\n",
    "        pred, _ = model(X, 3)  # (batch_size, padded_length, feat_dim)\n",
    "\n",
    "        loss_mse = loss_fn(pred, X)\n",
    "\n",
    "        _, s = model(X, 1)  # (batch_size, padded_length, feat_dim)\n",
    "        loss_con = demo_contrastive_loss(s, X.shape[0])\n",
    "\n",
    "        loss = 0.1 * loss_mse + loss_con\n",
    "\n",
    "        optimizer_finetune.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=4.0)\n",
    "        optimizer_finetune.step()\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        progress_bar.set_description(\n",
    "            \"Epoch {0} - Training loss: {1:.2f} - MSE loss: {2:.2f} - Contrastive loss: {3:.2f}\".format(\n",
    "                i,\n",
    "                loss.cpu().detach().numpy().item(),\n",
    "                loss_mse.cpu().detach().numpy().item(),\n",
    "                loss_con.cpu().detach().numpy().item(),\n",
    "            )\n",
    "        )\n",
    "        train_loss[\"loss\"].append(loss.item())\n",
    "        train_loss[\"loss_mse\"].append(loss_mse.item())\n",
    "        train_loss[\"loss_con\"].append(loss_con.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss: Dict[str, List[float]] = {\"loss\": [], \"loss_mse\": [], \"loss_con\": []}\n",
    "        for IDs in val_dataloader:\n",
    "            model.eval()\n",
    "            X = torch.tensor(data.loc[IDs].to_numpy()).to(device)\n",
    "            X = X.float()\n",
    "            X = X.reshape(-1, max_len, X.shape[-1])\n",
    "            X = X[:, :18, :]\n",
    "            # X = X[:, :, -1:]\n",
    "\n",
    "            #             _X = []\n",
    "            #             idx = torch.randint(low=0, high=6, size=(X.shape[0],))\n",
    "            #             for j in range(18):\n",
    "            #                 _X.append(X[torch.arange(X.shape[0]),idx+j,:].unsqueeze(1))\n",
    "\n",
    "            #             X = torch.cat(_X,dim=1)\n",
    "\n",
    "            # X = X[:, :, -1:]\n",
    "\n",
    "            pred, _ = model(X, 3)  # (batch_size, padded_length, feat_dim)\n",
    "\n",
    "            loss_mse = loss_fn(pred, X)\n",
    "\n",
    "            _, s = model(X, 1)  # (batch_size, padded_length, feat_dim)\n",
    "            loss_con = demo_contrastive_loss(s, X.shape[0])\n",
    "\n",
    "            loss = 0.1 * loss_mse + loss_con\n",
    "\n",
    "            val_loss[\"loss\"].append(loss.item())\n",
    "            val_loss[\"loss_mse\"].append(loss_mse.item())\n",
    "            val_loss[\"loss_con\"].append(loss_con.item())\n",
    "\n",
    "        if torch.tensor(val_loss[\"loss\"]).mean().item() < best_loss:\n",
    "            best_loss = torch.tensor(val_loss[\"loss\"]).mean()\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = i\n",
    "\n",
    "        progress_bar.write(\n",
    "            \"Epoch {0} - Training loss: {1:.2f} {2:.2f} {3:.2f} - Validation loss: {4:.2f} {5:.2f} {6:.2f}\".format(\n",
    "                i,\n",
    "                torch.tensor(train_loss[\"loss\"]).mean().item(),\n",
    "                torch.tensor(train_loss[\"loss_mse\"]).mean().item(),\n",
    "                torch.tensor(train_loss[\"loss_con\"]).mean().item(),\n",
    "                torch.tensor(val_loss[\"loss\"]).mean().item(),\n",
    "                torch.tensor(val_loss[\"loss_mse\"]).mean().item(),\n",
    "                torch.tensor(val_loss[\"loss_con\"]).mean().item(),\n",
    "            )\n",
    "        )\n",
    "    i += 1\n",
    "\n",
    "\n",
    "tqdm.write(\"Best Epoch {} - Best Validation loss: {}\".format(best_epoch, best_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetune_model = copy.deepcopy(best_model)\n",
    "optimizer = torch.optim.AdamW(finetune_model.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_indices, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_indices, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_indices, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.31: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 39.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.70 - Validation loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.32: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.51 - Validation loss: 0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 0.61: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 0.48 - Validation loss: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 0.26: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 0.46 - Validation loss: 0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 0.40: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 0.45 - Validation loss: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 0.43: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 43.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 0.45 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 0.24: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 0.44 - Validation loss: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training loss: 0.37: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 44.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training loss: 0.44 - Validation loss: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 0.79: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 46.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 0.43 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training loss: 0.24: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 45.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training loss: 0.43 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training loss: 0.30: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 39.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training loss: 0.43 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Training loss: 0.26: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Training loss: 0.43 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Training loss: 0.84: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Training loss: 0.43 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Training loss: 0.23: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Training loss: 0.43 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Training loss: 0.26: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Training loss: 0.43 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Training loss: 0.35: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Training loss: 0.26: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Training loss: 0.52: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Training loss: 0.22: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 44.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Training loss: 0.42 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Training loss: 0.34: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Training loss: 0.29: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 39.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Training loss: 0.29: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Training loss: 0.34: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Training loss: 0.42 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Training loss: 0.33: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Training loss: 0.21: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Training loss: 0.32: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 43.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Training loss: 0.42 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Training loss: 0.35: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Training loss: 0.39: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 43.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Training loss: 0.41: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 43.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Training loss: 0.42 - Validation loss: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Training loss: 0.31: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Training loss: 0.51: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 43.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Training loss: 0.42 - Validation loss: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Training loss: 0.29: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Training loss: 0.25: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Training loss: 0.43: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Training loss: 0.42 - Validation loss: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Training loss: 0.30: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Training loss: 0.42 - Validation loss: 0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Training loss: 0.29: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Training loss: 0.42 - Validation loss: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Training loss: 0.72: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 43.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Training loss: 0.41 - Validation loss: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Training loss: 0.30: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 43.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Training loss: 0.41 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Training loss: 0.28: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Training loss: 0.41 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Training loss: 0.27: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Training loss: 0.41 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Training loss: 0.65: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Training loss: 0.41 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Training loss: 0.27: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Training loss: 0.41 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Training loss: 0.55: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Training loss: 0.28: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Training loss: 0.42 - Validation loss: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Training loss: 0.22: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 43.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Training loss: 0.41 - Validation loss: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Training loss: 0.74: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Training loss: 0.41 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Training loss: 0.25: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Training loss: 0.42 - Validation loss: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Training loss: 0.22: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Training loss: 0.42 - Validation loss: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Training loss: 1.54: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Training loss: 0.41 - Validation loss: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Training loss: 0.83: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Training loss: 0.41 - Validation loss: 0.41\n",
      "Best Epoch 47 - Best Validation loss: 0.389247328042984\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "max_epoch = 50\n",
    "best_loss = 1.0e10\n",
    "best_finetune_model = copy.deepcopy(best_model)\n",
    "best_epoch = 0\n",
    "device = \"cuda\"\n",
    "finetune_model.to(device)\n",
    "while i < max_epoch:\n",
    "    ft_train_loss = []\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "\n",
    "    for IDs in progress_bar:\n",
    "        finetune_model.train()\n",
    "\n",
    "        X = torch.tensor(data.loc[IDs].to_numpy()).to(device)\n",
    "        X = X.float().reshape(-1, max_len, X.shape[-1])\n",
    "        targets = X[:, 18:24, :]\n",
    "        X = X[:, :18, :]\n",
    "\n",
    "        pred = finetune_model.predict(X)\n",
    "        pred = pred.reshape(X.shape[0], out_size, -1)\n",
    "        loss = loss_fn(pred, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(finetune_model.parameters(), max_norm=4.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_description(\"Epoch {} - Training loss: {:.2f}\".format(i, loss))\n",
    "        ft_train_loss.append(loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ft_val_loss = []\n",
    "        for IDs in val_dataloader:\n",
    "            finetune_model.eval()\n",
    "\n",
    "            X = torch.tensor(data.loc[IDs].to_numpy()).to(device)\n",
    "            X = X.float().reshape(-1, max_len, X.shape[-1])\n",
    "            targets = X[:, 18:24, :]\n",
    "            X = X[:, :18, :]\n",
    "\n",
    "            pred = finetune_model.predict(X.float())\n",
    "            pred = pred.reshape(X.shape[0], out_size, -1)\n",
    "            loss = loss_fn(pred, targets)\n",
    "\n",
    "            ft_val_loss.append(loss.item())\n",
    "\n",
    "        if torch.tensor(ft_val_loss).mean().item() < best_loss:\n",
    "            best_loss = torch.tensor(ft_val_loss).mean().item()\n",
    "            best_finetune_model = copy.deepcopy(finetune_model)\n",
    "            best_epoch = i\n",
    "\n",
    "    progress_bar.write(\n",
    "        \"Epoch {} - Training loss: {:.2f} - Validation loss: {:.2f}\".format(\n",
    "            i, torch.tensor(ft_train_loss).mean().item(), torch.tensor(ft_val_loss).mean().item()\n",
    "        )\n",
    "    )\n",
    "    i += 1\n",
    "\n",
    "\n",
    "tqdm.write(\"Best Epoch {} - Best Validation loss: {}\".format(best_epoch, best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE loss: 0.39245657679400864\n",
      "Test RMSE loss: 0.6264635478573424\n"
     ]
    }
   ],
   "source": [
    "ft_test_loss = []\n",
    "with torch.no_grad():\n",
    "    for IDs in test_dataloader:\n",
    "        best_finetune_model.eval()\n",
    "\n",
    "        X = torch.tensor(data.loc[IDs].to_numpy()).to(device)\n",
    "        X = X.float().reshape(-1, max_len, X.shape[-1])\n",
    "        targets = X[:, 18:24, :]\n",
    "        X = X[:, :18, :]\n",
    "\n",
    "        pred = best_finetune_model.predict(X.float())\n",
    "        pred = pred.reshape(X.shape[0], out_size, -1)\n",
    "        loss = loss_fn(pred, targets)\n",
    "\n",
    "        ft_test_loss.append(loss.item())\n",
    "\n",
    "\n",
    "print(\"Test MSE loss: {}\".format(np.mean(ft_test_loss)))\n",
    "print(\"Test RMSE loss: {}\".format(np.sqrt(np.mean(ft_test_loss))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE loss: 25944.05754264833\n",
      "Test RMSE loss: 161.07159135815456\n"
     ]
    }
   ],
   "source": [
    "ft_test_loss = []\n",
    "with torch.no_grad():\n",
    "    for IDs in test_dataloader:\n",
    "        best_finetune_model.eval()\n",
    "\n",
    "        X = torch.tensor(data.loc[IDs].to_numpy()).to(device)\n",
    "        X = X.float().reshape(-1, max_len, X.shape[-1])\n",
    "        targets = X[:, 18:24, :] * torch.tensor(normalizer.scale_).to(device) + torch.tensor(normalizer.mean_).to(\n",
    "            device\n",
    "        )\n",
    "        X = X[:, :18, :]\n",
    "\n",
    "        pred = best_finetune_model.predict(X.float())\n",
    "        pred = pred.reshape(X.shape[0], out_size, -1) * torch.tensor(normalizer.scale_).to(device) + torch.tensor(\n",
    "            normalizer.mean_\n",
    "        ).to(device)\n",
    "        loss = loss_fn(pred, targets)\n",
    "\n",
    "        ft_test_loss.append(loss.item())\n",
    "\n",
    "\n",
    "print(\"Test MSE loss: {}\".format(np.mean(ft_test_loss)))\n",
    "print(\"Test RMSE loss: {}\".format(np.sqrt(np.mean(ft_test_loss))))\n",
    "best_finetune_model_simmtm = copy.deepcopy(best_finetune_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop without SimMTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetune_model = copy.deepcopy(init_model)\n",
    "optimizer = torch.optim.AdamW(finetune_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.47: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.71 - Validation loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.35: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.51 - Validation loss: 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 0.31: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 0.48 - Validation loss: 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 0.50: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 0.47 - Validation loss: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 0.30: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 0.46 - Validation loss: 0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 0.25: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 0.45 - Validation loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 0.34: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 39.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 0.45 - Validation loss: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training loss: 0.68: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 38.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training loss: 0.44 - Validation loss: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 0.56: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 38.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 0.44 - Validation loss: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training loss: 0.32: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 39.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training loss: 0.44 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training loss: 0.58: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training loss: 0.44 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Training loss: 1.10: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 47.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Training loss: 0.43 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Training loss: 0.55: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 47.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Training loss: 0.43 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Training loss: 0.44: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 45.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Training loss: 0.43 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Training loss: 0.22: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 44.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Training loss: 0.43 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Training loss: 0.20: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 44.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Training loss: 0.43 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Training loss: 0.22: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 44.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Training loss: 0.43 - Validation loss: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Training loss: 0.70: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Training loss: 0.43 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Training loss: 0.45: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 39.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Training loss: 0.43 - Validation loss: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Training loss: 0.20: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Training loss: 0.43 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Training loss: 0.22: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Training loss: 0.43 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Training loss: 0.22: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Training loss: 0.43 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Training loss: 0.60: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Training loss: 0.43 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Training loss: 0.27: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Training loss: 0.43 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Training loss: 0.81: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Training loss: 0.24: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Training loss: 0.43 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Training loss: 0.32: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Training loss: 0.43 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Training loss: 0.54: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Training loss: 0.31: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Training loss: 0.43 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Training loss: 0.26: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Training loss: 0.29: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Training loss: 1.44: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Training loss: 0.30: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Training loss: 0.30: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 43.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Training loss: 0.33: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Training loss: 0.25: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Training loss: 0.21: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Training loss: 0.25: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 39.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Training loss: 0.27: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 39.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Training loss: 0.42 - Validation loss: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Training loss: 0.37: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 43.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Training loss: 0.59: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Training loss: 0.18: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 42.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Training loss: 0.26: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Training loss: 0.42 - Validation loss: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Training loss: 0.76: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Training loss: 0.18: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Training loss: 0.42 - Validation loss: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Training loss: 0.25: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Training loss: 0.42 - Validation loss: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Training loss: 0.27: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 41.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Training loss: 0.65: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 40.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Training loss: 0.30: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 38.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Training loss: 0.42 - Validation loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Training loss: 0.28: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:03<00:00, 39.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Training loss: 0.42 - Validation loss: 0.40\n",
      "Best Epoch 44 - Best Validation loss: 0.39469093084335327\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "max_epoch = 50\n",
    "best_loss = 1.0e10\n",
    "best_finetune_model = copy.deepcopy(init_model)\n",
    "best_epoch = 0\n",
    "device = \"cuda\"\n",
    "finetune_model.to(device)\n",
    "while i < max_epoch:\n",
    "    ft_train_loss = []\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "\n",
    "    for IDs in progress_bar:\n",
    "        finetune_model.train()\n",
    "\n",
    "        X = torch.tensor(data.loc[IDs].to_numpy()).to(device)\n",
    "        X = X.float().reshape(-1, max_len, X.shape[-1])\n",
    "        targets = X[:, 18:24, :]\n",
    "        X = X[:, :18, :]\n",
    "\n",
    "        pred = finetune_model.predict(X)\n",
    "        pred = pred.reshape(X.shape[0], out_size, -1)\n",
    "        loss = loss_fn(pred, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(finetune_model.parameters(), max_norm=4.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_description(\"Epoch {} - Training loss: {:.2f}\".format(i, loss))\n",
    "        ft_train_loss.append(loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ft_val_loss = []\n",
    "        for IDs in val_dataloader:\n",
    "            finetune_model.eval()\n",
    "\n",
    "            X = torch.tensor(data.loc[IDs].to_numpy()).to(device)\n",
    "            X = X.float().reshape(-1, max_len, X.shape[-1])\n",
    "            targets = X[:, 18:24, :]\n",
    "            X = X[:, :18, :]\n",
    "\n",
    "            pred = finetune_model.predict(X.float())\n",
    "            pred = pred.reshape(X.shape[0], out_size, -1)\n",
    "            loss = loss_fn(pred, targets)\n",
    "\n",
    "            ft_val_loss.append(loss.item())\n",
    "\n",
    "        if torch.tensor(ft_val_loss).mean().item() < best_loss:\n",
    "            best_loss = torch.tensor(ft_val_loss).mean().item()\n",
    "            best_finetune_model = copy.deepcopy(finetune_model)\n",
    "            best_epoch = i\n",
    "\n",
    "    progress_bar.write(\n",
    "        \"Epoch {} - Training loss: {:.2f} - Validation loss: {:.2f}\".format(\n",
    "            i, torch.tensor(ft_train_loss).mean().item(), torch.tensor(ft_val_loss).mean().item()\n",
    "        )\n",
    "    )\n",
    "    i += 1\n",
    "\n",
    "\n",
    "tqdm.write(\"Best Epoch {} - Best Validation loss: {}\".format(best_epoch, best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE loss: 0.4030115777933145\n",
      "Test RMSE loss: 0.6348319287758882\n"
     ]
    }
   ],
   "source": [
    "ft_test_loss = []\n",
    "with torch.no_grad():\n",
    "    for IDs in test_dataloader:\n",
    "        best_finetune_model.eval()\n",
    "\n",
    "        X = torch.tensor(data.loc[IDs].to_numpy()).to(device)\n",
    "        X = X.float().reshape(-1, max_len, X.shape[-1])\n",
    "        targets = X[:, 18:24, :]\n",
    "        X = X[:, :18, :]\n",
    "\n",
    "        pred = best_finetune_model.predict(X.float())\n",
    "        pred = pred.reshape(X.shape[0], out_size, -1)\n",
    "        loss = loss_fn(pred, targets)\n",
    "\n",
    "        ft_test_loss.append(loss.item())\n",
    "\n",
    "\n",
    "print(\"Test MSE loss: {}\".format(np.mean(ft_test_loss)))\n",
    "print(\"Test RMSE loss: {}\".format(np.sqrt(np.mean(ft_test_loss))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE loss: 28813.008208023184\n",
      "Test RMSE loss: 169.74394895849213\n"
     ]
    }
   ],
   "source": [
    "ft_test_loss = []\n",
    "with torch.no_grad():\n",
    "    for IDs in test_dataloader:\n",
    "        best_finetune_model.eval()\n",
    "\n",
    "        X = torch.tensor(data.loc[IDs].to_numpy()).to(device)\n",
    "        X = X.float().reshape(-1, max_len, X.shape[-1])\n",
    "        targets = X[:, 18:24, :] * torch.tensor(normalizer.scale_).to(device) + torch.tensor(normalizer.mean_).to(\n",
    "            device\n",
    "        )\n",
    "        X = X[:, :18, :]\n",
    "\n",
    "        pred = best_finetune_model.predict(X.float())\n",
    "        pred = pred.reshape(X.shape[0], out_size, -1) * torch.tensor(normalizer.scale_).to(device) + torch.tensor(\n",
    "            normalizer.mean_\n",
    "        ).to(device)\n",
    "        loss = loss_fn(pred, targets)\n",
    "\n",
    "        ft_test_loss.append(loss.item())\n",
    "\n",
    "\n",
    "print(\"Test MSE loss: {}\".format(np.mean(ft_test_loss)))\n",
    "print(\"Test RMSE loss: {}\".format(np.sqrt(np.mean(ft_test_loss))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X = torch.tensor(data.loc[next(iter(test_dataloader))].to_numpy()).to(device)\n",
    "# X = X.float().reshape(-1, max_len, X.shape[-1])\n",
    "# target = X[:, 18:24, :]\n",
    "# X = X[:, :18, :]\n",
    "# pred_1 = best_finetune_model.predict(X.float())\n",
    "# pred_2 = best_finetune_model_simmtm.predict(X.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i = 4\n",
    "# j = 1\n",
    "# plt.plot(torch.cat((X[i, :, j], pred_1[i, :, j])).cpu().detach().numpy(), ls=\"--\", label='no simmtm')\n",
    "# plt.plot(torch.cat((X[i, :, j], pred_2[i, :, j])).cpu().detach().numpy(), ls=\"--\", label='simmtm')\n",
    "# plt.plot(torch.cat((X[i, :, j], target[i, :, j])).cpu().detach().numpy())\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "1. https://arxiv.org/abs/2302.00861\n",
    "2. https://github.com/gzerveas/mvts_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized Results\n",
    "\n",
    "No Pretrain\n",
    "\n",
    "Test MSE loss: 0.401656836271286\n",
    "\n",
    "Test MSE loss: 0.3942021131515503\n",
    "\n",
    "Test MSE loss: 0.4026849865913391\n",
    "\n",
    "Test MSE loss: 0.3934531509876251\n",
    "\n",
    "Test MSE loss: 0.4011250138282776\n",
    "\n",
    "Pretrain\n",
    "\n",
    "Test MSE loss: 0.39376363158226013\n",
    "\n",
    "Test MSE loss: 0.3938564658164978\n",
    "\n",
    "Test MSE loss: 0.39758536219596863\n",
    "\n",
    "Test MSE loss: 0.3879236876964569\n",
    "\n",
    "Test MSE loss: 0.3867727518081665"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unnormalized Results\n",
    "\n",
    "No Pretrain\n",
    "\n",
    "Test MSE loss: 28955.188077749768\n",
    "\n",
    "Test MSE loss: 26232.348384247733\n",
    "\n",
    "Test MSE loss: 29180.849135569904\n",
    "\n",
    "Test MSE loss: 25597.15184640446\n",
    "\n",
    "Test MSE loss: 28796.784927283206\n",
    "\n",
    "Pretrain\n",
    "\n",
    "Test MSE loss: 25913.2302627422\n",
    "\n",
    "Test MSE loss: 26665.712613352596\n",
    "\n",
    "Test MSE loss: 26484.948554457686\n",
    "\n",
    "Test MSE loss: 26331.686181724996\n",
    "\n",
    "Test MSE loss: 25527.132474246446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
