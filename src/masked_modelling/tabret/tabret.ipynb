{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabRet: Pre-trainable Transformer-based Model for Unseen Columns\n",
    "\n",
    "This notebook shows how the [TabRet](https://arxiv.org/pdf/2303.15747.pdf) model can be used. TabRet is a pre-trainable Transformer-based model for tabular data and designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called retokenizing, which calibrates feature embeddings based on the masked autoencoding loss. TabRet is pretrained on a large collection of public health surveys (BRFSS dataset) and then fine-tuned on a classification task (Stroke prediction) in healthcare.\n",
    "\n",
    "A key challenge in using pretrained models for tabular data is that each table for downstream task has a different set of columns, and it is difficult to know at the pre-training phase which columns will appear in the downstream task. To address the above issue, we propose TabRet, a pre-trainable Transformer network that can adapt to unseen columns in downstream tasks. The training and fine-tuning process for TabRet consists of the following steps:\n",
    "1. Pretraining: First, TabRet is pre-trained based on the reconstruction loss with masking augmentation.\n",
    "2. Retokenizing: when unseen columns appear in a downstream task, their tokenizers are trained through masked modeling while freezing the mixer before fine-tuning.\n",
    "3. Finetuning: Finetuning the mixer for a specific target variable while freezing the rest of model parts.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/16665038/266691262-3514bd11-9dd4-4bad-a4a0-4afe4bfca8fd.png\" alt=\"tabret\" />\n",
    "</p>\n",
    "\n",
    "To execute this notebook successfully, please ensure you've completed the data preprocessing notebook first to prepare the necessary data for pretraining and finetuning.\n",
    "\n",
    "Please be aware that the pretraining phase can be both time-consuming and resource-intensive. For your convenience, checkpoints for a pretrained model are included in this notebook. It's recommended to first explore the pretrained model for retokenizing and fine-tuning tasks before opting to run the full pretraining process.\n",
    "\n",
    "The model's core implementation is built using PyTorch which can be found in the 'model' directory. To simplify the training process and enhance usability, high-level classes have been defined using PyTorch Lightning in this notebook. This allows for easier experiment tracking, distributed training, and other advanced features, making the training workflow more straightforward and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from os.path import join\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from model.tabret import TabRet\n",
    "from model.tabret_cls import TabRetClassifier\n",
    "from model.utils import get_diff_columns\n",
    "from pl_bolts.optimizers import LinearWarmupCosineAnnealingLR\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    "    RichProgressBar,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, QuantileTransformer\n",
    "from torch import Tensor, optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED = 4\n",
    "PRETRAINED_DIR = \"/ssd003/projects/aieng/public/ssl_bootcamp_resources/pretrained_ckpts/tabret\"\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "LOG_DIR = \"./logs\"\n",
    "DATA_DIR = \"./datasets\"\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "pl.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class\n",
    "\n",
    "The TabularDataset class serves as a customizable container for tabular data, compatible with PyTorch's Dataset API. The class allows for easy handling of continuous and categorical features, as well as target variables, simplifying the data preparation steps for model training. By providing an interface to return data in PyTorch-compatible formats, it seamlessly integrates with PyTorch's DataLoader for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        task: str = \"binary\",\n",
    "        continuous_columns: Optional[Sequence[str]] = None,\n",
    "        categorical_columns: Optional[Sequence[str]] = None,\n",
    "        target: Optional[Union[str, Sequence[str]]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Tabular dataset for tabular data.\n",
    "\n",
    "        Args:\n",
    "        ----\n",
    "            data (pandas.DataFrame): DataFrame.\n",
    "            task (str): One of \"binary\", \"multiclass\", \"regression\".\n",
    "                Defaults to \"binary\".\n",
    "            continuous_cols (sequence of str, optional): Sequence of names of\n",
    "                continuous features (columns). Defaults to None.\n",
    "            categorical_cols (sequence of str, optional): Sequence of names of\n",
    "                categorical features (columns). Defaults to None.\n",
    "            target (str, optional): If None, `np.zeros` is set as target.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.task = task\n",
    "        self.num = data.shape[0]\n",
    "        self.continuous_columns = continuous_columns if continuous_columns else []\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "\n",
    "        if target:\n",
    "            self.target = data[target].values\n",
    "            if isinstance(target, str):\n",
    "                self.target = self.target.reshape(-1, 1)\n",
    "        else:\n",
    "            self.target = np.zeros((self.num, 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        ----\n",
    "            idx (int): The index of the sample in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            dict[str, Tensor]:\n",
    "                The returned dict has the keys {\"target\", \"continuous\", \"categorical\"}\n",
    "                and its values. If no continuous/categorical features, the returned value is `[]`.\n",
    "        \"\"\"\n",
    "        x = {\n",
    "            \"continuous\": {key: torch.tensor(self.data[key].values[idx]).float() for key in self.continuous_columns}\n",
    "            if self.continuous_columns\n",
    "            else {},\n",
    "            \"categorical\": {key: torch.tensor(self.data[key].values[idx]).long() for key in self.categorical_columns}\n",
    "            if self.categorical_columns\n",
    "            else {},\n",
    "        }\n",
    "        if self.task == \"multiclass\":\n",
    "            x[\"target\"] = torch.LongTensor(self.target[idx])\n",
    "        elif self.task in [\"binary\", \"regression\"]:\n",
    "            x[\"target\"] = torch.tensor(self.target[idx])\n",
    "        else:\n",
    "            raise ValueError(f\"task: {self.task} must be 'multiclass' or 'binary' or 'regression'\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training Data: BRFSS\n",
    "\n",
    "The DataFrame that houses the complete BRFSS data is created by executing the preprocessing notebook. This data consists of both numeric and categorical features. For numerical features, the Quantile Transformation method from the scikit-learn library is applied for transformation. Meanwhile, categorical features are processed using the Ordinal Encoder. It's worth noting that for baseline methods, only the categorical features undergo transformation via the Ordinal Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brfss_df = pd.read_csv(join(DATA_DIR, \"brfss\", \"all.csv\"))\n",
    "brfss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_categorical_columns = list(\n",
    "    brfss_df.loc[:, (brfss_df.dtypes == \"object\") | (brfss_df.dtypes == \"int64\")].drop(\"Diabetes\", axis=1).columns\n",
    ")\n",
    "pre_continuous_columns = list(brfss_df.loc[:, brfss_df.dtypes == \"float64\"].columns)\n",
    "pre_cat_cardinality_dict = {col: len(brfss_df[col].unique()) for col in pre_categorical_columns}\n",
    "pre_target_columns = [\"Diabetes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre_categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre_continuous_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_cont_enc = QuantileTransformer(output_distribution=\"normal\").fit(brfss_df[pre_continuous_columns])\n",
    "pre_cate_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1).fit(\n",
    "    brfss_df[pre_categorical_columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A copy of the original dataframe is saved for later use for retokenization\n",
    "brfss_df_copy = brfss_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brfss_df[pre_continuous_columns] = pre_cont_enc.transform(brfss_df[pre_continuous_columns])\n",
    "brfss_df[pre_categorical_columns] = pre_cate_enc.transform(brfss_df[pre_categorical_columns])\n",
    "brfss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_df, pre_val_df = train_test_split(brfss_df, test_size=0.24, random_state=SEED, stratify=brfss_df[\"Diabetes\"])\n",
    "pre_train_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_dataset = TabularDataset(\n",
    "    data=pre_train_df,\n",
    "    categorical_columns=pre_categorical_columns,\n",
    "    continuous_columns=pre_continuous_columns,\n",
    "    target=pre_target_columns,\n",
    ")\n",
    "\n",
    "pre_val_dataset = TabularDataset(\n",
    "    data=pre_val_df,\n",
    "    categorical_columns=pre_categorical_columns,\n",
    "    continuous_columns=pre_continuous_columns,\n",
    "    target=pre_target_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabRet Model\n",
    "\n",
    "\n",
    "The model architecture is designed to effectively process and analyze tabular data through multiple specialized layers, each with its own functionality:\n",
    "\n",
    "1.  Feature Tokenizer: This initial layer converts the input data, denoted as x, into an embedded form suitable for further processing.\n",
    "\n",
    "2. Alignment Layer: Employing a linear layer, this component adjusts the token dimensions to make them compatible with the Encoder layer that follows. This is required because different dimensions for Feature Tokenizer and Encoder are employed for the model’s flexibility.\n",
    "\n",
    "3. Random Masking: In the pre-training and retokenizing phases, some tokens are randomly masked by the Random Masking process. Specifically, a certain number of tokens, determined by a set mask ratio, are chosen uniformly at random from the set of tokens for each data point, and these chosen tokens are then replaced with a 'mask token'. If no tokens are initially selected for masking, the protocol is overridden to ensure that at least one token is replaced with a 'mask token'.\n",
    "\n",
    "4. Encoder: This consists of an N-layer Transformer equipped with Pre-Normalization. It serves as the core processing unit, efficiently encoding the tokenized input.\n",
    "\n",
    "5. Post-Encoder: Post-encoding is performed by adding a mask token, positional embedding, and an additional Transformer block to the output of the Encoder.\n",
    "\n",
    "6. Projector: Finally, this layer maps the processed tokens back into the original column feature spaces using linear layers, completing the forward pass through the model.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"422\" alt=\"Screen Shot 2023-09-05 at 10 26 14 PM\" src=\"https://user-images.githubusercontent.com/16665038/266691464-17d952d3-1c93-4809-8161-92f585965811.png\">\n",
    "</p>\n",
    "\n",
    "\n",
    "We use the same model configuration that is outlined in the original paper. Specifically, the Encoder is designed with 6 block and includes FFN dropout. The Decoder, on the other hand, is streamlined and consists of a single block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabRet architecture config\n",
    "enc_transformer_config = {\n",
    "    \"n_blocks\": 6,\n",
    "    \"residual_dropout\": 0.0,\n",
    "    \"ffn_d_hidden\": 512,\n",
    "    \"d_token\": 384,\n",
    "    \"attention_dropout\": 0.1,\n",
    "    \"ffn_dropout\": 0.1,\n",
    "    \"attention_n_heads\": 8,\n",
    "    \"attention_initialization\": \"kaiming\",\n",
    "    \"ffn_activation\": \"ReGLU\",\n",
    "    \"attention_normalization\": \"LayerNorm\",\n",
    "    \"ffn_normalization\": \"LayerNorm\",\n",
    "    \"prenormalization\": True,\n",
    "    \"first_prenormalization\": False,\n",
    "    \"last_layer_query_idx\": None,\n",
    "    \"n_tokens\": None,\n",
    "    \"kv_compression_ratio\": None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "}\n",
    "dec_transformer_config = {\n",
    "    \"n_blocks\": 1,\n",
    "    \"residual_dropout\": 0.0,\n",
    "    \"ffn_d_hidden\": 128,\n",
    "    \"d_token\": 96,\n",
    "    \"attention_dropout\": 0.1,\n",
    "    \"ffn_dropout\": 0.0,\n",
    "    \"attention_n_heads\": 8,\n",
    "    \"attention_initialization\": \"kaiming\",\n",
    "    \"ffn_activation\": \"ReGLU\",\n",
    "    \"attention_normalization\": \"LayerNorm\",\n",
    "    \"ffn_normalization\": \"LayerNorm\",\n",
    "    \"prenormalization\": True,\n",
    "    \"first_prenormalization\": False,\n",
    "    \"last_layer_query_idx\": None,\n",
    "    \"n_tokens\": None,\n",
    "    \"kv_compression_ratio\": None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training\n",
    "\n",
    "In the pretraining phase, a masked autoencoder approach is employed to prepare the model for better generalization and performance in downstream tasks.\n",
    "\n",
    "1. Mask Token Replacement: The first step involves replacing the embeddings of randomly selected columns with a specialized form of embedding known as a \"mask token.\" This is akin to obscuring some of the information, forcing the model to learn to predict or \"fill in the gaps\" during training. This strategy is effective for making the model more robust and capable of handling unseen or missing data.\n",
    "\n",
    "2. Shuffle Augmentation: Alongside the mask token replacement, another strategy called \"Shuffle Augmentation\" is used. This technique involves the column-wise permutation of data within a minibatch, but only for a randomly chosen subset of columns. Essentially, the columns are shuffled to create a new, augmented version of the data. This helps the model become invariant to the ordering of columns, increasing its flexibility and ability to generalize across different kinds of tabular data structures.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"422\" alt=\"Screen Shot 2023-09-05 at 10 26 14 PM\" src=\"https://user-images.githubusercontent.com/16665038/266691464-17d952d3-1c93-4809-8161-92f585965811.png\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Below, the TabRetFTTrans class serves as a high-level interface for pretraining the TabRet model using PyTorch Lightning. This class simplifies the model training and validation process, making it more convenient and straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabRetFTTrans(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        continuous_columns: List[str],\n",
    "        cat_cardinality_dict: Dict[str, int],\n",
    "        enc_transformer_config: Dict[str, Any],\n",
    "        dec_transformer_config: Dict[str, Any],\n",
    "        mask_ratio: float = 0.7,\n",
    "        col_shuffle: Optional[Dict[str, Any]] = None,\n",
    "        epochs: int = 1000,\n",
    "        lr: float = 1e-4,\n",
    "        warmup_epochs: int = 10,\n",
    "        warmup_start_lr: float = 1e-6,\n",
    "        min_lr: float = 1e-6,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = TabRet.make(\n",
    "            continuous_columns=continuous_columns,\n",
    "            cat_cardinality_dict=cat_cardinality_dict,\n",
    "            enc_transformer_config=enc_transformer_config,\n",
    "            dec_transformer_config=dec_transformer_config,\n",
    "        )\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.col_shuffle = col_shuffle\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, inp: Dict[str, Any]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        loss, preds, mask = self.model(\n",
    "            x_num=inp[\"continuous\"],\n",
    "            x_cat=inp[\"categorical\"],\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            col_shuffle=dict(self.col_shuffle),  # type: ignore\n",
    "        )\n",
    "        return loss, preds, mask\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Dict[str, Tensor], batch_idx: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        loss, _, _ = self(batch)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, Tensor], batch_idx: int) -> None:\n",
    "        loss, _, _ = self(batch)\n",
    "        self.log(\"val_loss\", loss, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(\n",
    "        self,\n",
    "    ) -> Dict[str, Union[optim.Optimizer, optim.lr_scheduler._LRScheduler]]:\n",
    "        optimizer = optim.AdamW(self.model.optimization_param_groups(), lr=self.lr)\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer,\n",
    "            warmup_epochs=self.warmup_epochs,\n",
    "            max_epochs=self.epochs,\n",
    "            warmup_start_lr=self.warmup_start_lr,\n",
    "            eta_min=self.min_lr,\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretraining hyperparameters\n",
    "pre_mask_ratio = 0.7\n",
    "pre_epochs = 1000\n",
    "pre_batch_size = 4096\n",
    "pre_eval_batch_size = 4096\n",
    "num_workers = 4\n",
    "pre_lr = 1.5e-5\n",
    "pre_warmup_epochs = 40\n",
    "pre_warmup_start = 1e-6\n",
    "pre_lr_min = 1e-6\n",
    "column_shuffle = {\n",
    "    \"ratio\": 0.1,\n",
    "    \"mode\": \"shuffle\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = TabRetFTTrans(\n",
    "    continuous_columns=pre_continuous_columns,\n",
    "    cat_cardinality_dict=pre_cat_cardinality_dict,\n",
    "    enc_transformer_config=enc_transformer_config,\n",
    "    dec_transformer_config=dec_transformer_config,\n",
    "    mask_ratio=pre_mask_ratio,\n",
    "    col_shuffle=column_shuffle,\n",
    "    epochs=pre_epochs,\n",
    "    lr=pre_lr,\n",
    "    warmup_epochs=pre_warmup_epochs,\n",
    "    warmup_start_lr=pre_warmup_start,\n",
    "    min_lr=pre_lr_min,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_dataloader = DataLoader(\n",
    "    pre_train_dataset,\n",
    "    batch_size=pre_batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "pre_val_dataloader = DataLoader(\n",
    "    pre_val_dataset,\n",
    "    batch_size=pre_eval_batch_size,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    # strategy=DDPStrategy(find_unused_parameters=False),\n",
    "    max_epochs=pre_epochs,\n",
    "    precision=16,\n",
    "    check_val_every_n_epoch=2,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            filename=\"best\",\n",
    "            save_top_k=1,\n",
    "            save_last=True,\n",
    "            verbose=True,\n",
    "            dirpath=join(CHECKPOINT_DIR, \"pretraining\"),\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        RichProgressBar(),\n",
    "        # logger=WandbLogger(\n",
    "        #     project=\"tabret-pretraining\",\n",
    "        #     entity=\"vector-ssl-bootcamp\",\n",
    "        #     save_dir=join(LOG_DIR, \"pretraining\"),\n",
    "        # ),\n",
    "    ],\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trainer.fit(\n",
    "    model=pre_model,\n",
    "    train_dataloaders=pre_train_dataloader,\n",
    "    val_dataloaders=pre_val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retokenizing and Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stroke Dataset\n",
    "\n",
    "Stroke prediction is used as a downstram task. \n",
    "\n",
    "Regarding the data transformation, for features that are common to both datasets, transformation models (like QuantileTransformer for continuous features and OrdinalEncoder for categorical ones) are first fitted using the pre-training dataset and then applied to the downstream dataset. On the other hand, for features unique to the downstream dataset, transformations are directly fitted and applied using that specific dataset. This way, the preprocessing aligns well with the pre-training scheme, allowing for a seamless transition to fine-tuning the model.\n",
    "\n",
    "The dataset is divided into 80% training dataset and 20% test dataset. Of the training data, 100 samples are separated as a fine-tuning dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df = pd.read_csv(join(DATA_DIR, \"stroke\", \"stroke.csv\"))\n",
    "stroke_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy of the original dataframe for later use for the baseline model\n",
    "stroke_df_copy = stroke_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = [\n",
    "    \"avg_glucose_level\",\n",
    "    \"_BMI5\",\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    \"hypertension\",\n",
    "    \"heart_disease\",\n",
    "    \"work_type\",\n",
    "    \"Residence_type\",\n",
    "    \"SEX\",\n",
    "    \"_AGEG5YR\",\n",
    "    \"MARITAL\",\n",
    "    \"SMOKE100\",\n",
    "]\n",
    "\n",
    "common_cont_columns = [\"_BMI5\"]\n",
    "\n",
    "common_cate_columns = [\n",
    "    \"SEX\",\n",
    "    \"_AGEG5YR\",\n",
    "    \"MARITAL\",\n",
    "    \"SMOKE100\",\n",
    "]\n",
    "\n",
    "diff_cont_columns = [\n",
    "    \"avg_glucose_level\",\n",
    "]\n",
    "\n",
    "diff_cate_columns = [\n",
    "    \"hypertension\",\n",
    "    \"heart_disease\",\n",
    "    \"work_type\",\n",
    "    \"Residence_type\",\n",
    "]\n",
    "\n",
    "target_columns = [\"stroke\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if common_cont_columns:\n",
    "    common_cont_enc = QuantileTransformer(output_distribution=\"normal\").fit(brfss_df_copy[common_cont_columns])\n",
    "\n",
    "if common_cate_columns:\n",
    "    common_cate_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1).fit(\n",
    "        brfss_df_copy[common_cate_columns]\n",
    "    )\n",
    "\n",
    "if diff_cont_columns:\n",
    "    diff_cont_enc = QuantileTransformer(output_distribution=\"normal\").fit(stroke_df[diff_cont_columns])\n",
    "\n",
    "if diff_cate_columns:\n",
    "    diff_cate_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1).fit(\n",
    "        stroke_df[diff_cate_columns]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if common_cont_columns:\n",
    "    stroke_df[common_cont_columns] = common_cont_enc.transform(stroke_df[common_cont_columns])\n",
    "\n",
    "if common_cate_columns:\n",
    "    stroke_df[common_cate_columns] = common_cate_enc.transform(stroke_df[common_cate_columns])\n",
    "\n",
    "if diff_cont_columns:\n",
    "    stroke_df[diff_cont_columns] = diff_cont_enc.transform(stroke_df[diff_cont_columns])\n",
    "\n",
    "if diff_cate_columns:\n",
    "    stroke_df[diff_cate_columns] = diff_cate_enc.transform(stroke_df[diff_cate_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cardinality_dict = {}\n",
    "\n",
    "\n",
    "def get_cardinality_dict(enc: OrdinalEncoder) -> Dict[str, int]:\n",
    "    return {key: len(cardinality) for key, cardinality in zip(enc.feature_names_in_, enc.categories_)}\n",
    "\n",
    "\n",
    "if common_cate_columns:\n",
    "    cat_cardinality_dict.update(get_cardinality_dict(common_cate_enc))\n",
    "if diff_cate_columns:\n",
    "    cat_cardinality_dict.update(get_cardinality_dict(diff_cate_enc))\n",
    "\n",
    "cat_cardinality_dict = {key: cat_cardinality_dict[key] for key in categorical_columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(stroke_df, test_size=0.20, random_state=SEED, stratify=stroke_df[\"stroke\"])\n",
    "\n",
    "fine_df, fval_df = train_test_split(\n",
    "    train_df,\n",
    "    train_size=100,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df[\"stroke\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TabularDataset(\n",
    "    data=fine_df,\n",
    "    categorical_columns=categorical_columns,\n",
    "    continuous_columns=continuous_columns,\n",
    "    target=target_columns,\n",
    ")\n",
    "\n",
    "val_dataset = TabularDataset(\n",
    "    data=fval_df,\n",
    "    categorical_columns=categorical_columns,\n",
    "    continuous_columns=continuous_columns,\n",
    "    target=target_columns,\n",
    ")\n",
    "\n",
    "test_dataset = TabularDataset(\n",
    "    data=test_df,\n",
    "    categorical_columns=categorical_columns,\n",
    "    continuous_columns=continuous_columns,\n",
    "    target=target_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retokenizing\n",
    "\n",
    "In the retokenizing step, several actions are taken to adapt the model for columns that were not seen during pre-training. First, new tokenizers are added specifically for these newly appearing columns to convert their data into a form that the model can understand. To ensure compatibility, parts of the decoder, specifically the Positional Embedding and Projector, are initialized to align with the fine-tuning table's requirements.\n",
    "\n",
    "Once the new tokenizers and parts of the decoder are set up, the parameters for all existing components—old tokenizers, the encoder, and the decoder—are frozen. This means that these parts of the model are not updated during the retokenizing process, ensuring that the knowledge gained during pre-training is preserved.\n",
    "\n",
    "The training of the new tokenizers employs the same masked modeling approach used in the initial pre-training. Essentially, the columns that the model hasn't seen before are treated as masked out. During this process, special 'mask' tokens are fed into the Post-Encoder as placeholders for these unseen columns. This allows the model to learn how to handle these new columns without disrupting the learned patterns for the existing ones.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"457\" alt=\"Screen Shot 2023-09-05 at 10 26 26 PM\" src=\"https://user-images.githubusercontent.com/16665038/266691732-ad9339ef-62ec-41f9-a12d-ea3e2f5e80aa.png\">\n",
    "</p>\n",
    "\n",
    "Below, the TabRetokenize class serves as a high-level interface for retokenizing step using PyTorch Lightning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabRetokenize(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pre_continuous_columns: List[str],\n",
    "        pre_cat_cardinality_dict: Dict[str, int],\n",
    "        continuous_columns: List[str],\n",
    "        cat_cardinality_dict: Dict[str, int],\n",
    "        enc_transformer_config: Dict[str, Any],\n",
    "        dec_transformer_config: Dict[str, Any],\n",
    "        model_path: str,\n",
    "        epochs: int = 200,\n",
    "        batch_size: int = 32,\n",
    "        lr: float = 1.5e-3,\n",
    "        warmup_epochs: int = 10,\n",
    "        warmup_start_lr: float = 1e-6,\n",
    "        min_lr: float = 1e-6,\n",
    "        mask_ratio: float = 0.5,\n",
    "        para_freeze: bool = True,\n",
    "        mask_token_freeze: bool = True,\n",
    "        except_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.batch_size = batch_size\n",
    "        self.save_hyperparameters()\n",
    "        self.model = TabRet.make(\n",
    "            continuous_columns=pre_continuous_columns,\n",
    "            cat_cardinality_dict=pre_cat_cardinality_dict,\n",
    "            enc_transformer_config=enc_transformer_config,\n",
    "            dec_transformer_config=dec_transformer_config,\n",
    "        )\n",
    "        state_dict = torch.load(model_path)[\"state_dict\"]\n",
    "        self.model.load_state_dict(state_dict, strict=False)\n",
    "        diff_columns, continuous_columns, cat_cardinality_dict = get_diff_columns(\n",
    "            continuous_columns,\n",
    "            cat_cardinality_dict,\n",
    "            pre_continuous_columns,\n",
    "            pre_cat_cardinality_dict,\n",
    "        )\n",
    "        self.model.add_attribute(\n",
    "            continuous_columns=continuous_columns,\n",
    "            cat_cardinality_dict=cat_cardinality_dict,\n",
    "        )\n",
    "        if para_freeze:\n",
    "            self.model.freeze_parameters_wo_specific_columns(diff_columns)\n",
    "\n",
    "        if not mask_token_freeze:\n",
    "            self.model.unfreeze_mask_token()\n",
    "\n",
    "        if except_decoder:\n",
    "            self.model.unfreeze_decoder()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, inp: Dict[str, Any]) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        loss, preds, mask = self.model(\n",
    "            x_num=inp[\"continuous\"],\n",
    "            x_cat=inp[\"categorical\"],\n",
    "            mask_ratio=self.mask_ratio,\n",
    "        )\n",
    "        return loss, preds, mask\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Dict[str, Tensor], batch_idx: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        loss, _, _ = self(batch)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, Tensor], batch_idx: int) -> None:\n",
    "        loss, _, _ = self(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(\n",
    "        self,\n",
    "    ) -> Dict[str, Union[optim.Optimizer, optim.lr_scheduler._LRScheduler]]:\n",
    "        optimizer = optim.AdamW(self.model.optimization_param_groups(), lr=self.lr, weight_decay=1e-5)\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer,\n",
    "            warmup_epochs=self.warmup_epochs,\n",
    "            max_epochs=self.epochs,\n",
    "            warmup_start_lr=self.warmup_start_lr,\n",
    "            eta_min=self.min_lr,\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retokenize hyperparameters\n",
    "ret_mask_ratio = 0.5\n",
    "ret_epochs = 100\n",
    "ret_batch_size = 32\n",
    "ret_eval_batch_size = 32\n",
    "num_workers = 4\n",
    "ret_lr = 1.5e-3\n",
    "ret_warmup_epochs = 10\n",
    "ret_warmup_start = 1e-7\n",
    "ret_lr_min = 1e-7\n",
    "ret_patience = 50\n",
    "para_freeze = True\n",
    "mask_token_freeze = True\n",
    "except_decoder = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = join(PRETRAINED_DIR, \"pretrained-epoch=1000.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_model = TabRetokenize(\n",
    "    pre_continuous_columns=pre_continuous_columns,\n",
    "    pre_cat_cardinality_dict=pre_cat_cardinality_dict,\n",
    "    continuous_columns=continuous_columns,\n",
    "    cat_cardinality_dict=cat_cardinality_dict,\n",
    "    enc_transformer_config=enc_transformer_config,\n",
    "    dec_transformer_config=dec_transformer_config,\n",
    "    model_path=pretrained_model,\n",
    "    epochs=ret_epochs,\n",
    "    batch_size=ret_batch_size,\n",
    "    lr=ret_lr,\n",
    "    warmup_epochs=ret_warmup_epochs,\n",
    "    warmup_start_lr=ret_warmup_start,\n",
    "    min_lr=ret_lr_min,\n",
    "    mask_ratio=ret_mask_ratio,\n",
    "    para_freeze=para_freeze,\n",
    "    mask_token_freeze=mask_token_freeze,\n",
    "    except_decoder=except_decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=ret_batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "ret_val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=ret_eval_batch_size,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=ret_epochs,\n",
    "    precision=16,\n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            filename=\"best\",\n",
    "            save_top_k=1,\n",
    "            save_last=True,\n",
    "            verbose=True,\n",
    "            dirpath=join(CHECKPOINT_DIR, \"retokenize\"),\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0.00,\n",
    "            patience=ret_patience,\n",
    "            verbose=True,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        RichProgressBar(),\n",
    "        # logger=WandbLogger(\n",
    "        #     project=\"tabret-retokenize\",\n",
    "        #     entity=\"vector-ssl-bootcamp\",\n",
    "        #     save_dir=join(LOG_DIR, \"retokenize\"),\n",
    "        # ),\n",
    "    ],\n",
    "    log_every_n_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_trainer.fit(\n",
    "    model=ret_model,\n",
    "    train_dataloaders=ret_train_dataloader,\n",
    "    val_dataloaders=ret_val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "In the fine-tuning step, the target value 'y' for this task is introduced as a new column in the data and treated as a \"masked\" entry, similar to how new features were handled during the retokenizing phase.\n",
    "\n",
    "Initially, all the parameters that the model learned during the pre-training and retokenizing phases are kept constant, or \"frozen.\" This helps to maintain the general understanding the model has developed thus far.\n",
    "\n",
    "The training then zeroes in on specific components, particularly the Positional Embedding and Projector, but now for the newly added target value column. This is similar to how these components were adapted during the retokenizing phase for newly seen columns. The goal is to enable the model to accurately predict or classify the target values based on its learned knowledge.\n",
    "\n",
    "One of the key advantages of this step is efficiency. Since only specific components are trained, there's a significant reduction in the number of learning parameters. This not only speeds up the training process but also reduces the required sample size of the training dataset for the downstream task, making fine-tuning a more resource-efficient way to adapt the model for specific applications.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"488\" alt=\"Screen Shot 2023-09-05 at 10 26 36 PM\" src=\"https://user-images.githubusercontent.com/16665038/266691879-547996b7-9e3e-41f3-b266-a8865000703e.png\">\n",
    "</p>\n",
    "\n",
    "Below, the TabRetFinetune class serves as a high-level interface for finetuning step using PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabRetFinetune(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pre_continuous_columns: List[str],\n",
    "        pre_cat_cardinality_dict: Dict[str, int],\n",
    "        continuous_columns: List[str],\n",
    "        cat_cardinality_dict: Dict[str, int],\n",
    "        enc_transformer_config: Dict[str, Any],\n",
    "        dec_transformer_config: Dict[str, Any],\n",
    "        model_path: str,\n",
    "        epochs: int = 200,\n",
    "        lr: float = 1.5e-3,\n",
    "        warmup_epochs: int = 10,\n",
    "        warmup_start_lr: float = 1e-6,\n",
    "        min_lr: float = 1e-6,\n",
    "        output_dim: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = TabRet.make(\n",
    "            continuous_columns=pre_continuous_columns,\n",
    "            cat_cardinality_dict=pre_cat_cardinality_dict,\n",
    "            enc_transformer_config=enc_transformer_config,\n",
    "            dec_transformer_config=dec_transformer_config,\n",
    "        )\n",
    "        self.model.add_attribute(\n",
    "            continuous_columns=continuous_columns,\n",
    "            cat_cardinality_dict=cat_cardinality_dict,\n",
    "        )\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        state_dict = torch.load(model_path)[\"state_dict\"]\n",
    "        self.model.load_state_dict(state_dict, strict=False)\n",
    "        self.model.freeze_parameters()\n",
    "        output_dim += 1\n",
    "        self.model = TabRetClassifier(self.model, output_dim)\n",
    "        self.model.show_trainable_parameter()\n",
    "\n",
    "    def forward(self, inp: Dict[str, Any]) -> Tuple[Tensor, Tensor]:\n",
    "        logits = self.model(\n",
    "            x_num=inp[\"continuous\"],\n",
    "            x_cat=inp[\"categorical\"],\n",
    "        )\n",
    "        loss = self.model.loss_fn(logits, inp[\"target\"].squeeze(-1))\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Dict[str, Tensor], batch_idx: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        loss, _ = self(batch)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, Tensor], batch_idx: int) -> None:\n",
    "        loss, _ = self(batch)\n",
    "        self.log(\"val_loss\", loss, sync_dist=True)\n",
    "\n",
    "    def test_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Dict[str, Tensor]:\n",
    "        loss, logits = self(batch)\n",
    "        preds = torch.softmax(logits, dim=-1)[:, 1]\n",
    "        return {\"test_loss\": loss, \"preds\": preds, \"target\": batch[\"target\"]}\n",
    "\n",
    "    def test_epoch_end(self, outputs: List[Any]) -> None:\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        preds = torch.cat([x[\"preds\"] for x in outputs])\n",
    "        labels = torch.cat([x[\"target\"] for x in outputs])\n",
    "        acc = accuracy_score(labels.cpu(), preds.cpu().round())\n",
    "        auc = roc_auc_score(labels.cpu(), preds.cpu())\n",
    "        self.log(\"avg_test_loss\", avg_loss)\n",
    "        self.log(\"test_acc\", acc)\n",
    "        self.log(\"test_auc\", auc)\n",
    "\n",
    "    def configure_optimizers(\n",
    "        self,\n",
    "    ) -> Dict[str, Union[optim.Optimizer, optim.lr_scheduler._LRScheduler]]:\n",
    "        optimizer = optim.AdamW(\n",
    "            self.model.optimization_param_groups(),\n",
    "            lr=self.lr,\n",
    "        )\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer,\n",
    "            warmup_epochs=self.warmup_epochs,\n",
    "            max_epochs=self.epochs,\n",
    "            warmup_start_lr=self.warmup_start_lr,\n",
    "            eta_min=self.min_lr,\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune hyperparameters\n",
    "fine_epochs = 200\n",
    "fine_batch_size = 32\n",
    "fine_eval_batch_size = 32\n",
    "num_workers = 4\n",
    "fine_lr = 5e-4\n",
    "fine_warmup_epochs = 10\n",
    "fine_warmup_start = 1e-7\n",
    "min_lr = 1e-7\n",
    "fine_patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retokenized_model = join(CHECKPOINT_DIR, \"retokenize\", \"last.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_model = TabRetFinetune(\n",
    "    pre_continuous_columns=pre_continuous_columns,\n",
    "    pre_cat_cardinality_dict=pre_cat_cardinality_dict,\n",
    "    continuous_columns=continuous_columns,\n",
    "    cat_cardinality_dict=cat_cardinality_dict,\n",
    "    enc_transformer_config=enc_transformer_config,\n",
    "    dec_transformer_config=dec_transformer_config,\n",
    "    model_path=retokenized_model,\n",
    "    epochs=fine_epochs,\n",
    "    lr=fine_lr,\n",
    "    warmup_epochs=fine_warmup_epochs,\n",
    "    warmup_start_lr=fine_warmup_start,\n",
    "    min_lr=min_lr,\n",
    "    output_dim=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=fine_batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=fine_eval_batch_size,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=len(test_dataset),\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=fine_epochs,\n",
    "    precision=16,\n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            filename=\"best\",\n",
    "            save_top_k=1,\n",
    "            save_last=True,\n",
    "            verbose=True,\n",
    "            dirpath=join(CHECKPOINT_DIR, \"finetune\"),\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0.00,\n",
    "            patience=fine_patience,\n",
    "            verbose=False,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        RichProgressBar(),\n",
    "        # logger=WandbLogger(\n",
    "        #     project=\"tabret-finetune\",\n",
    "        #     entity=\"vector-ssl-bootcamp\",\n",
    "        #     save_dir=join(LOG_DIR, \"finetune\"),\n",
    "        # ),\n",
    "    ],\n",
    "    log_every_n_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_trainer.fit(model=fine_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "fine_trainer.test(dataloaders=test_dataloader, ckpt_path=join(CHECKPOINT_DIR, \"finetune\", \"last.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "We employ XGBoost Classifier as our baseline model, training and testing it on the same data subsets used for finetuning. To optimize hyperparameters, we utilize the Optuna library. Categorical features are converted to numerical format using Ordinal Encoding, after which they are treated as numerical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if common_cate_columns:\n",
    "    base_common_enc = OneHotEncoder(handle_unknown=\"ignore\").fit(brfss_df_copy[common_cate_columns])\n",
    "\n",
    "if diff_cate_columns:\n",
    "    base_diff_enc = OneHotEncoder(handle_unknown=\"ignore\").fit(stroke_df_copy[diff_cate_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(columns: List[str], categories: List[np.ndarray]) -> List[str]:\n",
    "    columns_after = []\n",
    "    for col, cates in zip(columns, categories):\n",
    "        for cate in cates:\n",
    "            columns_after.append(f\"{col}-{cate}\")\n",
    "    return columns_after\n",
    "\n",
    "\n",
    "if common_cate_columns:\n",
    "    new_df = pd.DataFrame(\n",
    "        base_common_enc.transform(stroke_df_copy[common_cate_columns]).toarray().astype(\"int64\"),\n",
    "        columns=get_columns(common_cate_columns, base_common_enc.categories_),\n",
    "    )\n",
    "    stroke_df_copy = pd.concat([stroke_df_copy, new_df], axis=1)\n",
    "    stroke_df_copy = stroke_df_copy.drop(common_cate_columns, axis=1)\n",
    "\n",
    "if diff_cate_columns:\n",
    "    new_df = pd.DataFrame(\n",
    "        base_diff_enc.transform(stroke_df_copy[diff_cate_columns]).toarray().astype(\"int64\"),\n",
    "        columns=get_columns(diff_cate_columns, base_diff_enc.categories_),\n",
    "    )\n",
    "    stroke_df_copy = pd.concat([stroke_df_copy, new_df], axis=1)\n",
    "    stroke_df_copy = stroke_df_copy.drop(diff_cate_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = list(stroke_df_copy.drop(target_columns, axis=1).columns)\n",
    "categorical_columns = []\n",
    "feature_columns = continuous_columns + categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base, test_base = train_test_split(\n",
    "    stroke_df_copy, test_size=0.20, random_state=SEED, stratify=stroke_df_copy[\"stroke\"]\n",
    ")\n",
    "\n",
    "fine_base, fval_base = train_test_split(\n",
    "    train_base,\n",
    "    train_size=100,\n",
    "    random_state=SEED,\n",
    "    stratify=train_base[\"stroke\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 11),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 5900, 200),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 1e2),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 0.7),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 7),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1, 4),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1e2),\n",
    "    }\n",
    "\n",
    "    params.update({\"objective\": \"binary:logistic\", \"eval_metric\": \"auc\"})\n",
    "    model = xgb.XGBClassifier(\n",
    "        **params,\n",
    "        random_state=SEED,\n",
    "        early_stopping_rounds=20,\n",
    "    )\n",
    "    model.fit(\n",
    "        fine_base[feature_columns],\n",
    "        fine_base[target_columns],\n",
    "        eval_set=[(fval_base[feature_columns], fval_base[target_columns])],\n",
    "        verbose=False,\n",
    "    )\n",
    "    pred = model.predict_proba(fval_base[feature_columns])[:, 1]\n",
    "    auc = roc_auc_score(fval_base[target_columns], pred)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    load_if_exists=True,\n",
    ")\n",
    "study.optimize(objective, callbacks=[optuna.study.MaxTrialsCallback(500)])\n",
    "best_params = study.best_params\n",
    "best_params.update({\"objective\": \"binary:logistic\", \"eval_metric\": \"auc\"})\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = xgb.XGBClassifier(\n",
    "    **best_params,\n",
    "    random_state=SEED,\n",
    "    early_stopping_rounds=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.fit(\n",
    "    fine_base[feature_columns],\n",
    "    fine_base[target_columns],\n",
    "    eval_set=[(fval_base[feature_columns], fval_base[target_columns])],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = base_model.predict_proba(test_base[feature_columns])[:, 1]\n",
    "target = test_base[target_columns]\n",
    "label = (prediction > 0.5).astype(np.int64)\n",
    "base_score = {\n",
    "    \"ACC\": accuracy_score(target, label),\n",
    "    \"AUC\": roc_auc_score(target, prediction),\n",
    "}\n",
    "base_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sslenv",
   "language": "python",
   "name": "sslenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
